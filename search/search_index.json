{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Prefect Examples","text":"<p>Progressive Prefect 3 examples -- from hello-world to production patterns.</p>"},{"location":"#what-you-will-learn","title":"What you will learn","text":"<p>This project walks through 113 self-contained flows that cover the full spectrum of Prefect 3 capabilities:</p> <ul> <li>Basics -- flows, tasks, parameters, return values</li> <li>Control flow -- branching, state hooks, parameterisation</li> <li>Composition -- subflows, dynamic task mapping</li> <li>Operational -- polling, retries, error handling, caching</li> <li>Reuse and events -- shared task libraries, custom events</li> <li>Data processing -- file I/O, quality rules, incremental processing</li> <li>Analytics -- correlation, regression, dimensional modeling</li> <li>DHIS2 integration -- custom credentials block, API client, metadata and   analytics pipelines</li> <li>Deployments -- <code>flow.serve()</code>, <code>flow.deploy()</code>, <code>prefect.yaml</code>, work pools</li> <li>Advanced -- concurrency limits, variables, transactions, async patterns,   circuit breakers, and end-to-end pipelines</li> </ul> <p>Every example notes its Airflow equivalent so you can map existing knowledge directly.</p>"},{"location":"#navigate","title":"Navigate","text":"Page Description Getting Started Prerequisites, installation, first run Tutorials Step-by-step walkthroughs for common tasks Core Concepts Prefect fundamentals and Airflow comparison Flow Reference Detailed walkthrough of all 113 flows Patterns Common patterns and best practices Feature Coverage Prefect 3 feature coverage matrix and gaps Infrastructure Docker Compose stack and local dev setup CLI Reference Prefect CLI commands used in this project API Reference Auto-generated docs for <code>Dhis2Client</code>, <code>Dhis2Credentials</code> Testing How to test Prefect flows"},{"location":"#links","title":"Links","text":"<ul> <li>Prefect documentation</li> <li>Prefect GitHub</li> <li>Source repository</li> </ul>"},{"location":"api-reference/","title":"API Reference -- Prefect HTTP API","text":"<p>The Prefect server exposes a REST API that backs every operation in the UI, CLI, and Python SDK. This page documents the most common endpoints and patterns for interacting with it directly.</p>"},{"location":"api-reference/#overview","title":"Overview","text":"Detail Value Base URL <code>http://localhost:4200/api</code> Protocol HTTP / JSON Auth (OSS) None Auth (Cloud) Bearer token via <code>PREFECT_API_KEY</code> <p>Set the base URL with the <code>PREFECT_API_URL</code> environment variable or the <code>prefect config set</code> command:</p> <pre><code>export PREFECT_API_URL=\"http://localhost:4200/api\"\n# or\nprefect config set PREFECT_API_URL=\"http://localhost:4200/api\"\n</code></pre>"},{"location":"api-reference/#authentication","title":"Authentication","text":"<p>Prefect OSS -- no authentication is required. The API is open on the configured host/port.</p> <p>Prefect Cloud -- every request must include an <code>Authorization</code> header:</p> <pre><code>Authorization: Bearer &lt;PREFECT_API_KEY&gt;\n</code></pre> <pre><code>curl -s https://api.prefect.cloud/api/accounts/&lt;ACCOUNT_ID&gt;/workspaces/&lt;WORKSPACE_ID&gt;/flows \\\n  -H \"Authorization: Bearer $PREFECT_API_KEY\"\n</code></pre>"},{"location":"api-reference/#health-check","title":"Health check","text":"<pre><code>curl -s http://localhost:4200/api/health\n# 200 OK  -- returns true when the server is ready\n</code></pre>"},{"location":"api-reference/#endpoint-groups","title":"Endpoint groups","text":"<p>All request/response bodies are JSON. For POST endpoints that filter or list resources, send a JSON body with filter, sort, limit, and offset fields.</p>"},{"location":"api-reference/#flows","title":"Flows","text":"<p>List flows</p> <pre><code>curl -s http://localhost:4200/api/flows/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"limit\": 10}'\n</code></pre> <p>Read a flow</p> <pre><code>curl -s http://localhost:4200/api/flows/&lt;FLOW_ID&gt;\n</code></pre>"},{"location":"api-reference/#deployments","title":"Deployments","text":"<p>List deployments</p> <pre><code>curl -s http://localhost:4200/api/deployments/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"limit\": 10}'\n</code></pre> <p>Create a deployment</p> <pre><code>curl -s -X POST http://localhost:4200/api/deployments/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"my-deployment\",\n    \"flow_id\": \"&lt;FLOW_ID&gt;\",\n    \"work_pool_name\": \"default\"\n  }'\n</code></pre> <p>Read a deployment</p> <pre><code>curl -s http://localhost:4200/api/deployments/&lt;DEPLOYMENT_ID&gt;\n</code></pre> <p>Read a deployment by name</p> <p>Look up a deployment using the flow name and deployment name instead of an ID:</p> <pre><code>curl -s http://localhost:4200/api/deployments/name/&lt;FLOW_NAME&gt;/&lt;DEPLOYMENT_NAME&gt;\n</code></pre> <p>Example:</p> <pre><code>curl -s http://localhost:4200/api/deployments/name/dhis2_org_units/dhis2-org-units\n</code></pre> <p>Trigger a deployment run</p> <pre><code>curl -s -X POST http://localhost:4200/api/deployments/&lt;DEPLOYMENT_ID&gt;/create_flow_run \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre> <p>Trigger a deployment run with parameters</p> <pre><code>curl -s -X POST http://localhost:4200/api/deployments/&lt;DEPLOYMENT_ID&gt;/create_flow_run \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"parameters\": {\"output_dir\": \"/tmp/results\", \"batch_size\": 500}}'\n</code></pre> <p>Trigger by name (two-step)</p> <p>The <code>create_flow_run</code> endpoint requires a deployment ID. Combine the name lookup with the trigger in a single command:</p> <pre><code>DEPLOY_ID=$(curl -s http://localhost:4200/api/deployments/name/dhis2_org_units/dhis2-org-units \\\n  | python3 -c \"import sys,json;print(json.load(sys.stdin)['id'])\")\n\ncurl -s -X POST \"http://localhost:4200/api/deployments/$DEPLOY_ID/create_flow_run\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre> <p>Pause / resume a deployment</p> <pre><code># Pause\ncurl -s -X POST http://localhost:4200/api/deployments/&lt;DEPLOYMENT_ID&gt;/set_schedule_inactive\n\n# Resume\ncurl -s -X POST http://localhost:4200/api/deployments/&lt;DEPLOYMENT_ID&gt;/set_schedule_active\n</code></pre> <p>Set a schedule</p> <pre><code>curl -s -X POST http://localhost:4200/api/deployments/&lt;DEPLOYMENT_ID&gt;/schedule \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"schedule\": {\"cron\": \"0 8 * * *\", \"timezone\": \"UTC\"}\n  }'\n</code></pre>"},{"location":"api-reference/#flow-runs","title":"Flow runs","text":"<p>List flow runs</p> <pre><code>curl -s http://localhost:4200/api/flow_runs/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"limit\": 5, \"sort\": \"EXPECTED_START_TIME_DESC\"}'\n</code></pre> <p>Read a flow run</p> <pre><code>curl -s http://localhost:4200/api/flow_runs/&lt;FLOW_RUN_ID&gt;\n</code></pre> <p>Create (trigger) a flow run</p> <pre><code>curl -s -X POST http://localhost:4200/api/flow_runs/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"flow_id\": \"&lt;FLOW_ID&gt;\",\n    \"deployment_id\": \"&lt;DEPLOYMENT_ID&gt;\",\n    \"state\": {\"type\": \"SCHEDULED\"}\n  }'\n</code></pre> <p>Cancel a flow run</p> <pre><code>curl -s -X POST http://localhost:4200/api/flow_runs/&lt;FLOW_RUN_ID&gt;/set_state \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"state\": {\"type\": \"CANCELLING\"}}'\n</code></pre> <p>Delete a flow run</p> <pre><code>curl -s -X DELETE http://localhost:4200/api/flow_runs/&lt;FLOW_RUN_ID&gt;\n</code></pre>"},{"location":"api-reference/#work-pools","title":"Work pools","text":"<p>List work pools</p> <pre><code>curl -s http://localhost:4200/api/work_pools/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre> <p>Create a work pool</p> <pre><code>curl -s -X POST http://localhost:4200/api/work_pools/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"my-pool\",\n    \"type\": \"process\"\n  }'\n</code></pre> <p>Read a work pool</p> <pre><code>curl -s http://localhost:4200/api/work_pools/&lt;WORK_POOL_NAME&gt;\n</code></pre> <p>Update a work pool</p> <pre><code>curl -s -X PATCH http://localhost:4200/api/work_pools/&lt;WORK_POOL_NAME&gt; \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"is_paused\": true}'\n</code></pre> <p>Delete a work pool</p> <pre><code>curl -s -X DELETE http://localhost:4200/api/work_pools/&lt;WORK_POOL_NAME&gt;\n</code></pre> <p>List workers in a pool</p> <pre><code>curl -s http://localhost:4200/api/work_pools/&lt;WORK_POOL_NAME&gt;/workers/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre>"},{"location":"api-reference/#blocks","title":"Blocks","text":"<p>List block types</p> <pre><code>curl -s http://localhost:4200/api/block_types/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre> <p>List block documents</p> <pre><code>curl -s http://localhost:4200/api/block_documents/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"include_secrets\": false}'\n</code></pre> <p>Read a block document</p> <pre><code>curl -s http://localhost:4200/api/block_documents/&lt;BLOCK_DOCUMENT_ID&gt;\n</code></pre> <p>Create a block document</p> <pre><code>curl -s -X POST http://localhost:4200/api/block_documents/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"my-secret\",\n    \"block_type_id\": \"&lt;BLOCK_TYPE_ID&gt;\",\n    \"data\": {\"value\": \"supersecret\"}\n  }'\n</code></pre> <p>Delete a block document</p> <pre><code>curl -s -X DELETE http://localhost:4200/api/block_documents/&lt;BLOCK_DOCUMENT_ID&gt;\n</code></pre>"},{"location":"api-reference/#automations","title":"Automations","text":"<p>List automations</p> <pre><code>curl -s http://localhost:4200/api/automations/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre> <p>Create an automation</p> <pre><code>curl -s -X POST http://localhost:4200/api/automations/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"cancel-stuck-runs\",\n    \"trigger\": {\n      \"type\": \"event\",\n      \"expect\": [\"prefect.flow-run.Failed\"],\n      \"posture\": \"Reactive\",\n      \"threshold\": 1,\n      \"within\": 0\n    },\n    \"actions\": [\n      {\"type\": \"cancel-flow-run\"}\n    ]\n  }'\n</code></pre> <p>Read an automation</p> <pre><code>curl -s http://localhost:4200/api/automations/&lt;AUTOMATION_ID&gt;\n</code></pre> <p>Update an automation</p> <pre><code>curl -s -X PUT http://localhost:4200/api/automations/&lt;AUTOMATION_ID&gt; \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ ... }'\n</code></pre> <p>Delete an automation</p> <pre><code>curl -s -X DELETE http://localhost:4200/api/automations/&lt;AUTOMATION_ID&gt;\n</code></pre> <p>Pause / resume an automation</p> <pre><code># Pause\ncurl -s -X PATCH http://localhost:4200/api/automations/&lt;AUTOMATION_ID&gt; \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"enabled\": false}'\n\n# Resume\ncurl -s -X PATCH http://localhost:4200/api/automations/&lt;AUTOMATION_ID&gt; \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"enabled\": true}'\n</code></pre>"},{"location":"api-reference/#events","title":"Events","text":"<p>List events</p> <pre><code>curl -s http://localhost:4200/api/events/filter \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"filter\": {\n      \"occurred\": {\n        \"since_\": \"2024-01-01T00:00:00Z\",\n        \"until_\": \"2024-12-31T23:59:59Z\"\n      }\n    },\n    \"limit\": 50\n  }'\n</code></pre>"},{"location":"api-reference/#pagination-and-filtering","title":"Pagination and filtering","text":"<p>List/filter endpoints use a POST body with these common fields:</p> Field Type Description <code>limit</code> int Max items to return (default varies) <code>offset</code> int Number of items to skip <code>sort</code> string Sort key, e.g. <code>CREATED_DESC</code> <code>filter</code> object Nested filter conditions (see below) <p>Filter objects are nested by resource. Example -- flow runs in a <code>COMPLETED</code> state:</p> <pre><code>{\n  \"flow_runs\": {\n    \"state\": {\n      \"type\": {\n        \"any_\": [\"COMPLETED\"]\n      }\n    }\n  },\n  \"limit\": 20,\n  \"offset\": 0\n}\n</code></pre> <p>Page through results by incrementing <code>offset</code> by <code>limit</code> until fewer than <code>limit</code> items are returned.</p>"},{"location":"api-reference/#common-response-codes","title":"Common response codes","text":"Code Meaning Typical cause 200 OK Successful read or filter 201 Created Resource created (deployment, block, etc.) 204 No Content Successful delete or update with no body 404 Not Found ID does not exist 409 Conflict Duplicate name or unique constraint violated 422 Unprocessable Entity Validation error in the request body"},{"location":"api-reference/#python-client","title":"Python client","text":"<p>The <code>prefect</code> SDK and CLI use this REST API under the hood. For scripted access outside of a flow context, <code>httpx</code> works well:</p> <pre><code>import httpx\n\nAPI = \"http://localhost:4200/api\"\n\n# List flows\nresponse = httpx.post(f\"{API}/flows/filter\", json={\"limit\": 5})\nresponse.raise_for_status()\n\nfor flow in response.json():\n    print(flow[\"name\"], flow[\"id\"])\n</code></pre> <p>For Prefect Cloud, add the authorization header:</p> <pre><code>headers = {\"Authorization\": f\"Bearer {api_key}\"}\nresponse = httpx.post(f\"{API}/flows/filter\", json={\"limit\": 5}, headers=headers)\n</code></pre>"},{"location":"api-reference/#airflow-comparison","title":"Airflow comparison","text":"Capability Airflow stable REST API Prefect REST API Auth Basic / Kerberos / session None (OSS) or Bearer token (Cloud) Trigger a DAG / run <code>POST /dags/{id}/dagRuns</code> <code>POST /deployments/{id}/create_flow_run</code> List runs <code>GET /dags/{id}/dagRuns</code> <code>POST /flow_runs/filter</code> Cancel a run <code>PATCH /dags/{id}/dagRuns/{id}</code> <code>POST /flow_runs/{id}/set_state</code> Filter pattern Query-string params JSON body with nested filter objects Pagination <code>limit</code> + <code>offset</code> query params <code>limit</code> + <code>offset</code> in JSON body Schema docs Swagger / OpenAPI at <code>/api/v1/ui</code> OpenAPI schema at <code>/api/docs</code>"},{"location":"api-reference/#official-documentation","title":"Official documentation","text":"<p>Full schema and interactive docs are available at:</p> <ul> <li>Self-hosted: <code>http://localhost:4200/docs</code> (Swagger UI served by the Prefect server)</li> <li>Prefect docs: https://docs.prefect.io/latest/api-ref/rest-api/</li> </ul>"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>Prefect CLI commands used in this project, organised by workflow. All commands assume the virtual environment is active (prefix with <code>uv run</code> if needed).</p>"},{"location":"cli-reference/#server-management","title":"Server management","text":"<p>Start a local Prefect server with the UI at <code>http://localhost:4200</code>:</p> <pre><code>prefect server start\n</code></pre> <p>Useful environment variables:</p> Variable Description Default <code>PREFECT_API_URL</code> API endpoint the client talks to <code>http://127.0.0.1:4200/api</code> <code>PREFECT_SERVER_ANALYTICS_ENABLED</code> Send anonymous usage stats <code>true</code> <code>PREFECT_SERVER_UI_SHOW_PROMOTIONAL_CONTENT</code> Show promotional banners in UI <code>true</code> <code>PREFECT_API_DATABASE_CONNECTION_URL</code> Database connection string SQLite (default) <p>For a full stack with PostgreSQL, see Infrastructure.</p>"},{"location":"cli-reference/#deployments","title":"Deployments","text":""},{"location":"cli-reference/#creating-deployments","title":"Creating deployments","text":"<p>Deploy all flows defined in a <code>prefect.yaml</code>:</p> <pre><code>prefect deploy --all\n</code></pre> <p>Deploy a single named deployment:</p> <pre><code>prefect deploy -n dhis2-ou\n</code></pre>"},{"location":"cli-reference/#listing-and-inspecting","title":"Listing and inspecting","text":"<pre><code>prefect deployment ls                           # list all deployments\nprefect deployment inspect &lt;flow/deployment&gt;    # view deployment details\n</code></pre>"},{"location":"cli-reference/#triggering-runs","title":"Triggering runs","text":"<pre><code># Run with default parameters\nprefect deployment run &lt;flow/deployment&gt;\n\n# Run with parameter overrides\nprefect deployment run &lt;flow/deployment&gt; -p key=value\n</code></pre>"},{"location":"cli-reference/#schedule-management","title":"Schedule management","text":"<p>Prefect supports three schedule types -- cron, interval, and RRule. Schedules can be set at deployment time in <code>prefect.yaml</code> or updated after deployment via the CLI.</p>"},{"location":"cli-reference/#setting-schedules-in-prefectyaml","title":"Setting schedules in <code>prefect.yaml</code>","text":"<p>Schedules are declared per deployment. Multiple schedules per deployment are supported:</p> <pre><code>deployments:\n  - name: dhis2-ou\n    entrypoint: flow.py:dhis2_ou_flow\n    schedules:\n      - cron: \"0 6 * * *\"        # daily at 06:00 UTC\n        timezone: \"UTC\"\n      - cron: \"0 18 * * *\"       # also at 18:00 UTC\n        timezone: \"UTC\"\n    work_pool:\n      name: default\n</code></pre> <p>Cron expressions:</p> Expression Meaning <code>\"0 6 * * *\"</code> Daily at 06:00 <code>\"*/15 * * * *\"</code> Every 15 minutes <code>\"0 8 * * 1-5\"</code> Weekdays at 08:00 <code>\"0 0 1 * *\"</code> First day of each month at midnight <p>Interval and RRule in code:</p> <pre><code># Interval -- every 15 minutes\nmy_flow.serve(name=\"frequent\", interval=900)\n\n# RRule -- weekdays only\nmy_flow.serve(name=\"weekdays\", rrule=\"FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR\")\n</code></pre>"},{"location":"cli-reference/#updating-schedules-after-deployment","title":"Updating schedules after deployment","text":"<p>Change schedules without redeploying:</p> <pre><code># Set a cron schedule\nprefect deployment set-schedule &lt;flow/deployment&gt; --cron \"0 8 * * *\"\n\n# Set an interval schedule (seconds)\nprefect deployment set-schedule &lt;flow/deployment&gt; --interval 3600\n\n# Set an RRule schedule\nprefect deployment set-schedule &lt;flow/deployment&gt; --rrule \"FREQ=WEEKLY;BYDAY=MO,WE,FR\"\n\n# Remove all schedules\nprefect deployment clear-schedule &lt;flow/deployment&gt;\n</code></pre>"},{"location":"cli-reference/#pausing-and-resuming-schedules","title":"Pausing and resuming schedules","text":"<pre><code># Pause scheduling (existing runs are not affected)\nprefect deployment pause &lt;flow/deployment&gt;\n\n# Resume scheduling\nprefect deployment resume &lt;flow/deployment&gt;\n</code></pre>"},{"location":"cli-reference/#viewing-schedules","title":"Viewing schedules","text":"<pre><code># Inspect shows schedule details\nprefect deployment inspect &lt;flow/deployment&gt;\n</code></pre> <p>The Prefect UI (<code>http://localhost:4200</code>) also shows schedules on the deployment detail page, where they can be edited visually.</p>"},{"location":"cli-reference/#passing-parameters-to-deployments","title":"Passing parameters to deployments","text":"<p>Default parameters are set in <code>prefect.yaml</code>:</p> <pre><code>deployments:\n  - name: dhis2-sync\n    entrypoint: flow.py:dhis2_sync_flow\n    parameters:\n      endpoints:\n        - organisationUnits\n        - dataElements\n    schedules:\n      - cron: \"0 6 * * *\"\n        timezone: \"UTC\"\n    work_pool:\n      name: default\n</code></pre> <p>Override at run time:</p> <pre><code>prefect deployment run dhis2_sync/dhis2-sync -p endpoints='[\"indicators\"]'\n</code></pre> <p>Parameters can also be overridden from the Prefect UI when triggering a manual run.</p>"},{"location":"cli-reference/#deployment-lifecycle","title":"Deployment lifecycle","text":"Action Command Create / update <code>prefect deploy --all</code> or <code>prefect deploy -n &lt;name&gt;</code> List <code>prefect deployment ls</code> Inspect <code>prefect deployment inspect &lt;flow/deployment&gt;</code> Trigger run <code>prefect deployment run &lt;flow/deployment&gt;</code> Set schedule <code>prefect deployment set-schedule &lt;name&gt; --cron \"...\"</code> Clear schedule <code>prefect deployment clear-schedule &lt;name&gt;</code> Pause <code>prefect deployment pause &lt;name&gt;</code> Resume <code>prefect deployment resume &lt;name&gt;</code> Delete <code>prefect deployment delete &lt;name&gt;</code>"},{"location":"cli-reference/#work-pools","title":"Work pools","text":""},{"location":"cli-reference/#creating-a-work-pool","title":"Creating a work pool","text":"<pre><code>prefect work-pool create my-pool --type process\n</code></pre> <p>Pool types:</p> Type Description <code>process</code> Runs flows as local subprocesses (development) <code>docker</code> Runs flows in Docker containers (team use) <code>kubernetes</code> Runs flows as K8s jobs (production)"},{"location":"cli-reference/#starting-a-worker","title":"Starting a worker","text":"<pre><code>prefect worker start --pool my-pool\n</code></pre> <p>The worker polls the pool for scheduled runs and executes them. Multiple workers can share the same pool for horizontal scaling.</p>"},{"location":"cli-reference/#managing-work-pools","title":"Managing work pools","text":"<pre><code>prefect work-pool ls                    # list all pools\nprefect work-pool inspect my-pool       # view pool details\nprefect work-pool pause my-pool         # pause the pool\nprefect work-pool resume my-pool        # resume the pool\nprefect work-pool delete my-pool        # delete the pool\n</code></pre>"},{"location":"cli-reference/#blocks","title":"Blocks","text":""},{"location":"cli-reference/#registering-block-types","title":"Registering block types","text":"<pre><code>prefect block register -m prefect_examples.dhis2\n</code></pre> <p>This registers custom block types (like <code>Dhis2Credentials</code>) with the Prefect server so they appear in the UI.</p>"},{"location":"cli-reference/#managing-blocks","title":"Managing blocks","text":"<pre><code>prefect block ls                        # list all saved blocks\nprefect block inspect &lt;type/name&gt;       # view block details\nprefect block delete &lt;type/name&gt;        # delete a block\n</code></pre>"},{"location":"cli-reference/#flow-runs","title":"Flow runs","text":"<pre><code>prefect flow-run ls                     # list recent flow runs\nprefect flow-run inspect &lt;id&gt;           # view run details\nprefect flow-run cancel &lt;id&gt;            # cancel a running flow\nprefect flow-run delete &lt;id&gt;            # delete a flow run\n</code></pre>"},{"location":"cli-reference/#automations","title":"Automations","text":"<pre><code>prefect automation ls                     # list automations\nprefect automation inspect &lt;name&gt;         # view automation details\nprefect automation delete &lt;name&gt;          # remove an automation\nprefect automation pause &lt;name&gt;           # disable an automation\nprefect automation resume &lt;name&gt;          # re-enable an automation\n</code></pre>"},{"location":"cli-reference/#makefile-shortcuts","title":"Makefile shortcuts","text":"<p>The project <code>Makefile</code> wraps common commands:</p> Target Command Description <code>make help</code> -- Show all available targets <code>make sync</code> <code>uv sync</code> Install dependencies <code>make lint</code> <code>uv run ruff check . &amp;&amp; uv run mypy src/ packages/</code> Run linter and type checker <code>make fmt</code> <code>uv run ruff format . &amp;&amp; uv run ruff check --fix .</code> Auto-format code <code>make test</code> <code>uv run pytest</code> Run the test suite <code>make clean</code> <code>rm -rf ...</code> Remove build artifacts <code>make run</code> <code>uv run python flows/basics/basics_hello_world.py</code> Run the hello-world flow <code>make server</code> <code>uv run prefect server start</code> Start a local Prefect server <code>make start</code> <code>docker compose up --build</code> Start the full Docker stack <code>make restart</code> <code>docker compose down -v &amp;&amp; ... up</code> Tear down and rebuild the stack <code>make deploy</code> register-blocks + create-blocks + <code>prefect deploy --all</code> Register blocks, create instances, and deploy all flows <code>make register-blocks</code> <code>prefect block register -m prefect_dhis2</code> Register custom block types with the server <code>make create-blocks</code> <code>uv run python scripts/create_blocks.py</code> Create DHIS2 credentials block instances for all known servers <code>make docs</code> <code>uv run mkdocs serve</code> Serve docs locally <code>make docs-build</code> <code>uv run mkdocs build</code> Build static docs site"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>A quick tour of the Prefect 3 building blocks used throughout these examples.</p>"},{"location":"core-concepts/#flows","title":"Flows","text":"<p>A flow is the main container for orchestrated work. Decorate any Python function with <code>@flow</code> and Prefect tracks its execution, state, and metadata.</p> <pre><code>from prefect import flow\n\n@flow(name=\"my_flow\", log_prints=True)\ndef my_flow():\n    print(\"Running!\")\n</code></pre> <p>Flows can call other flows (subflows), accept typed parameters, and return values.</p>"},{"location":"core-concepts/#tasks","title":"Tasks","text":"<p>A task is a unit of work inside a flow. Decorate a function with <code>@task</code> to gain retries, caching, concurrency controls, and observability.</p> <pre><code>from prefect import task\n\n@task(retries=3, retry_delay_seconds=10)\ndef extract_data() -&gt; dict:\n    ...\n</code></pre> <p>Tasks are called like normal functions. Dependencies are expressed through return-value wiring -- pass the output of one task as input to the next.</p>"},{"location":"core-concepts/#states","title":"States","text":"<p>Every flow run and task run has a state that tracks its lifecycle: <code>Pending</code>, <code>Running</code>, <code>Completed</code>, <code>Failed</code>, <code>Cancelled</code>, etc.</p> <p>You can inspect states programmatically:</p> <pre><code>state = my_flow(return_state=True)\nassert state.is_completed()\n</code></pre>"},{"location":"core-concepts/#results","title":"Results","text":"<p>Tasks and flows return values directly -- no push/pull ceremony required. In Airflow you would use XCom; in Prefect you simply return and pass values:</p> <pre><code>@task\ndef produce() -&gt; dict:\n    return {\"key\": \"value\"}\n\n@task\ndef consume(data: dict) -&gt; None:\n    print(data[\"key\"])\n\n@flow\ndef pipeline():\n    data = produce()\n    consume(data)      # data flows naturally\n</code></pre>"},{"location":"core-concepts/#deployments","title":"Deployments","text":"<p>A deployment packages a flow for remote execution on a schedule or via API triggers. A flow is just a Python function; a deployment is the configuration that says when, where, and with what parameters that function should run.</p>"},{"location":"core-concepts/#why-you-need-a-deployment","title":"Why you need a deployment","text":"<p>Running <code>python my_flow.py</code> executes a flow once. A deployment lets you:</p> <ul> <li>Schedule runs (cron, interval, or RRule)</li> <li>Trigger runs from the UI, API, or CLI</li> <li>Parameterize runs with different inputs</li> <li>Run on remote infrastructure (Docker, Kubernetes, etc.)</li> </ul>"},{"location":"core-concepts/#flowserve-vs-flowdeploy","title":"<code>flow.serve()</code> vs <code>flow.deploy()</code>","text":"<code>flow.serve()</code> <code>flow.deploy()</code> Where it runs Same process Work pool (separate infra) Infra needed None Work pool + worker Scaling Single process Pool-level scaling Best for Dev, simple cron jobs Production, team use <pre><code># flow.serve() -- simplest approach, runs in-process\nmy_flow.serve(name=\"my-flow\", cron=\"0 6 * * *\")\n\n# flow.deploy() -- production, sends runs to a work pool\nmy_flow.deploy(\n    name=\"my-flow\",\n    work_pool_name=\"my-pool\",\n    cron=\"0 6 * * *\",\n)\n</code></pre> <p>Both methods accept <code>parameters=</code> to set default parameter values:</p> <pre><code>my_flow.serve(\n    name=\"dhis2-daily\",\n    cron=\"0 6 * * *\",\n    parameters={\"endpoints\": [\"organisationUnits\", \"dataElements\"]},\n)\n</code></pre>"},{"location":"core-concepts/#prefectyaml-for-declarative-deployments","title":"<code>prefect.yaml</code> for declarative deployments","text":"<p>For production, define deployments in a <code>prefect.yaml</code> file at the project root. This is the Prefect equivalent of Airflow's <code>dags/</code> folder:</p> <pre><code>deployments:\n  - name: dhis2-ou\n    entrypoint: flow.py:dhis2_ou_flow\n    schedules:\n      - cron: \"0 6 * * *\"\n        timezone: \"UTC\"\n    work_pool:\n      name: default\n</code></pre> <p>Deploy with <code>prefect deploy --all</code> or <code>prefect deploy -n dhis2-ou</code>.</p>"},{"location":"core-concepts/#work-pools","title":"Work pools","text":"<p>Work pools define where flow runs execute. Three common types:</p> Pool type What it does When to use <code>process</code> Runs flows as local subprocesses Development, single-machine <code>docker</code> Runs flows in Docker containers Team use, isolation <code>kubernetes</code> Runs flows as K8s jobs Production, auto-scaling <p>Create a pool and start a worker:</p> <pre><code>prefect work-pool create my-pool --type process\nprefect worker start --pool my-pool\n</code></pre>"},{"location":"core-concepts/#deployment-lifecycle","title":"Deployment lifecycle","text":"<ol> <li>Create -- <code>prefect deploy</code> or <code>flow.serve()</code>/<code>flow.deploy()</code></li> <li>Schedule -- runs are created automatically per schedule</li> <li>Run -- trigger manually via <code>prefect deployment run &lt;name&gt;</code></li> <li>Pause -- <code>prefect deployment pause &lt;name&gt;</code> (stops scheduling)</li> <li>Resume -- <code>prefect deployment resume &lt;name&gt;</code></li> <li>Update -- re-run <code>prefect deploy</code> or change schedule via CLI</li> <li>Delete -- <code>prefect deployment delete &lt;name&gt;</code></li> </ol>"},{"location":"core-concepts/#prefect-cli-for-deployments","title":"Prefect CLI for deployments","text":"<pre><code># List and inspect\nprefect deployment ls                          # list all deployments\nprefect deployment inspect &lt;flow/deployment&gt;   # view deployment details\n\n# Trigger runs\nprefect deployment run &lt;flow/deployment&gt;                    # run with defaults\nprefect deployment run &lt;flow/deployment&gt; -p key=value       # run with params\n\n# Schedule management\nprefect deployment set-schedule &lt;name&gt; --cron \"0 8 * * *\"   # change schedule\nprefect deployment set-schedule &lt;name&gt; --interval 3600      # every hour\nprefect deployment clear-schedule &lt;name&gt;                    # remove schedule\nprefect deployment pause &lt;name&gt;                             # pause scheduling\nprefect deployment resume &lt;name&gt;                            # resume scheduling\n\n# Cleanup\nprefect deployment delete &lt;name&gt;\n</code></pre>"},{"location":"core-concepts/#artifacts","title":"Artifacts","text":"<p>Artifacts publish rich content (markdown, tables, links) to the Prefect UI. Use them for reports, dashboards, and reference links.</p> <pre><code>from prefect.artifacts import create_markdown_artifact, create_table_artifact\n\ncreate_markdown_artifact(key=\"report\", markdown=\"# Report\\n...\")\ncreate_table_artifact(key=\"data\", table=[{\"col\": \"value\"}])\n</code></pre> <p>Without a Prefect server, artifact functions silently no-op.</p> <p>See: Markdown Artifacts, Table and Link Artifacts</p>"},{"location":"core-concepts/#tags","title":"Tags","text":"<p>Tags label flows and tasks for filtering, searching, and grouping in the Prefect UI. Apply them at definition time or dynamically at runtime:</p> <pre><code>from prefect import flow, task, tags\n\n@task(tags=[\"etl\", \"extract\"])\ndef extract_data() -&gt; dict:\n    ...\n\n@flow(name=\"my_flow\", tags=[\"examples\"])\ndef my_flow() -&gt; None:\n    extract_data()\n\n    # Dynamic tags applied at runtime\n    with tags(\"ad-hoc\", \"debug\"):\n        debug_task()\n</code></pre> <p>Tags are additive -- tasks inherit tags from their parent flow, plus any set via the <code>tags()</code> context manager.</p> <p>See: Tags</p>"},{"location":"core-concepts/#events","title":"Events","text":"<p>Events are custom signals emitted from flows and tasks. Use them to trigger automations or track domain-specific occurrences:</p> <pre><code>from prefect.events import emit_event\n\nemit_event(\n    event=\"data.quality.check\",\n    resource={\"prefect.resource.id\": \"quality-monitor\"},\n    payload={\"score\": 0.95, \"status\": \"green\"},\n)\n</code></pre> <p>Events are sent to the Prefect event system and can trigger automations configured in the UI. Without a server, <code>emit_event()</code> silently no-ops.</p> <p>See: Events</p>"},{"location":"core-concepts/#automations","title":"Automations","text":"<p>Automations are event-driven rules that pair a trigger with an action. When the trigger condition is met (e.g. a flow run fails, an event is emitted, or a work queue becomes unhealthy), Prefect executes the configured action automatically. Automations can be created in the UI, via the CLI, or with the Python SDK.</p>"},{"location":"core-concepts/#python-sdk","title":"Python SDK","text":"<p>Use the <code>Automation</code> and <code>EventTrigger</code> classes from <code>prefect.automations</code>:</p> <pre><code>from prefect.automations import Automation\nfrom prefect.events.schemas.automations import EventTrigger\n\n# Send a notification when any flow run fails\nAutomation(\n    name=\"notify-on-failure\",\n    trigger=EventTrigger(\n        expect={\"prefect.flow-run.Failed\"},\n        match={\"prefect.resource.id\": \"prefect.flow-run.*\"},\n    ),\n    actions=[{\"type\": \"send-notification\", \"block_document_id\": \"&lt;notification-block-id&gt;\"}],\n).create()\n</code></pre> <pre><code># Circuit breaker: pause a deployment after 3 failures in 10 minutes\nfrom datetime import timedelta\n\nAutomation(\n    name=\"circuit-breaker\",\n    trigger=EventTrigger(\n        expect={\"prefect.flow-run.Failed\"},\n        match={\"prefect.resource.id\": \"prefect.flow-run.*\"},\n        for_each={\"prefect.resource.id\"},\n        threshold=3,\n        within=timedelta(minutes=10),\n    ),\n    actions=[{\"type\": \"pause-deployment\"}],\n).create()\n</code></pre>"},{"location":"core-concepts/#common-triggeraction-patterns","title":"Common trigger/action patterns","text":"Trigger Action Use case Flow run enters <code>Failed</code> state Send notification Alert on-call engineer N failures within time window Pause deployment Circuit breaker Work queue becomes unhealthy Send notification Infrastructure monitoring Custom event emitted Run deployment Event-driven orchestration Flow run duration exceeds threshold Cancel flow run Timeout guard"},{"location":"core-concepts/#cli","title":"CLI","text":"<pre><code>prefect automation ls                     # list automations\nprefect automation inspect &lt;name&gt;         # view automation details\nprefect automation pause &lt;name&gt;           # disable an automation\nprefect automation resume &lt;name&gt;          # re-enable an automation\nprefect automation delete &lt;name&gt;          # remove an automation\n</code></pre>"},{"location":"core-concepts/#airflow-comparison","title":"Airflow comparison","text":"Airflow Prefect SLA callbacks (<code>sla_miss_callback</code>) Automation with duration trigger PagerDuty / Slack operators in <code>on_failure_callback</code> Notification action on failure trigger Sensors polling for external events <code>EventTrigger</code> reacting to emitted events No built-in circuit breaker Threshold trigger + pause action <p>See: Prefect Automations docs</p>"},{"location":"core-concepts/#webhooks","title":"Webhooks","text":"<p>Prefect has two webhook concepts:</p>"},{"location":"core-concepts/#webhook-block-outbound","title":"Webhook block (outbound)","text":"<p>The <code>Webhook</code> block from <code>prefect.blocks.webhook</code> is a reusable, server-persisted block for making outbound HTTP calls. It stores URL, method, headers, and auth credentials securely (URL as <code>SecretStr</code>, headers as <code>SecretDict</code>):</p> <pre><code>from pydantic import SecretStr\nfrom prefect.blocks.webhook import Webhook\nfrom prefect.types import SecretDict\n\nwebhook = Webhook(\n    method=\"POST\",\n    url=SecretStr(\"https://api.example.com/events\"),\n    headers=SecretDict({\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": \"Bearer my-token\",\n    }),\n)\n\n# Call the webhook (async)\nresponse = await webhook.call({\"event\": \"pipeline.completed\"})\n\n# Save to server for reuse across flows\nwebhook.save(\"my-webhook\", overwrite=True)\nloaded = Webhook.load(\"my-webhook\")\n</code></pre> <p>The <code>Webhook</code> block also powers the <code>CallWebhook</code> automation action -- when an automation fires, Prefect calls the saved webhook block automatically.</p>"},{"location":"core-concepts/#inbound-webhooks-cloud","title":"Inbound webhooks (Cloud)","text":"<p>Prefect Cloud provides inbound webhook endpoints where external systems (GitHub, Slack, CI tools) POST events. A Jinja2 template transforms each incoming payload into a Prefect event:</p> <pre><code>{\n  \"event\": \"{{ body.action }}\",\n  \"resource\": {\n    \"prefect.resource.id\": \"github.repo.{{ body.repository.full_name }}\",\n    \"ref\": \"{{ body.ref }}\",\n    \"sender\": \"{{ body.sender.login }}\"\n  }\n}\n</code></pre> <p>This template would transform a GitHub push event into a Prefect event that can trigger automations. Inbound webhooks are managed via the CLI:</p> <pre><code>prefect webhook create my-github-hook \\\n    --template '{\"event\":\"{{ body.action }}\"}'\nprefect webhook ls\nprefect webhook get my-github-hook\nprefect webhook delete my-github-hook\n</code></pre> <p>See: Webhook Block flow, Prefect Webhooks docs</p>"},{"location":"core-concepts/#custom-run-names","title":"Custom Run Names","text":"<p>Customise flow run and task run names for easier identification in the UI and logs. Names support template strings and callable generators:</p> <pre><code>@task(task_run_name=\"fetch-{source}-page-{page}\")\ndef fetch_page(source: str, page: int) -&gt; dict:\n    ...\n\n@flow(flow_run_name=\"report-{env}-{date_str}\")\ndef report_flow(env: str, date_str: str) -&gt; None:\n    ...\n</code></pre> <p>For dynamic names, pass a callable:</p> <pre><code>def generate_name():\n    return f\"run-{datetime.now():%Y%m%d-%H%M}\"\n\n@flow(flow_run_name=generate_name)\ndef my_flow() -&gt; None:\n    ...\n</code></pre> <p>See: Task Run Names, Flow Run Names</p>"},{"location":"core-concepts/#blocks","title":"Blocks","text":"<p>Blocks are typed, reusable configuration objects. Built-in blocks include <code>Secret</code>, <code>JSON</code>, and others. Custom blocks subclass <code>Block</code>:</p> <pre><code>from prefect.blocks.core import Block\n\nclass DatabaseConfig(Block):\n    host: str = \"localhost\"\n    port: int = 5432\n\n# Use directly or save/load from server\nconfig = DatabaseConfig(host=\"db.prod.com\")\n</code></pre> <p>The <code>Secret</code> block handles encrypted credentials:</p> <pre><code>from prefect.blocks.system import Secret\napi_key = Secret.load(\"my-key\").get()\n</code></pre>"},{"location":"core-concepts/#block-lifecycle","title":"Block lifecycle","text":"<ol> <li>Define -- subclass <code>Block</code> with typed fields</li> <li>Instantiate -- create with values: <code>MyBlock(field=\"value\")</code></li> <li>Save -- persist to the Prefect server: <code>block.save(\"my-block\")</code></li> <li>Load -- retrieve in any flow: <code>MyBlock.load(\"my-block\")</code></li> </ol> <p>Blocks saved to a Prefect server have their <code>SecretStr</code> fields encrypted at rest.  Without a server, blocks work as plain Python objects with inline defaults.</p>"},{"location":"core-concepts/#when-to-use-block-vs-secret-vs-json","title":"When to use Block vs Secret vs JSON","text":"Type Use for Example Custom <code>Block</code> Typed connection config with methods <code>Dhis2Credentials</code> <code>Secret</code> Single credential value API key, token <code>JSON</code> Unstructured config Feature flags, thresholds"},{"location":"core-concepts/#secretstr-for-credentials","title":"SecretStr for credentials","text":"<p>Use Pydantic <code>SecretStr</code> for password and token fields on blocks. SecretStr prevents accidental exposure in logs, repr, and serialization:</p> <pre><code>from pydantic import Field, SecretStr\n\nclass MyConnection(Block):\n    username: str = \"admin\"\n    password: SecretStr = Field(default=SecretStr(\"changeme\"))\n\nconn = MyConnection()\nconn.password                       # SecretStr('**********')\nconn.password.get_secret_value()    # 'changeme'\n</code></pre>"},{"location":"core-concepts/#adding-methods-to-blocks-the-integration-pattern","title":"Adding methods to blocks (the integration pattern)","text":"<p>Official Prefect integrations (prefect-aws, prefect-gcp, prefect-slack) put a <code>get_client()</code> method on credentials blocks that returns an authenticated SDK client. API methods live on the client class, not the block:</p> <pre><code>class Dhis2Client:\n    \"\"\"Authenticated DHIS2 API client.\"\"\"\n\n    def __init__(self, base_url: str, username: str, password: str) -&gt; None:\n        self._http = httpx.Client(\n            base_url=f\"{base_url}/api\",\n            auth=(username, password),\n            timeout=60,\n        )\n\n    def fetch_metadata(self, endpoint: str) -&gt; list[dict]:\n        resp = self._http.get(f\"/{endpoint}\", params={\"paging\": \"false\"})\n        resp.raise_for_status()\n        return resp.json()[endpoint]\n\nclass Dhis2Credentials(Block):\n    base_url: str = \"https://play.im.dhis2.org/dev\"\n    username: str = \"admin\"\n    password: SecretStr = Field(default=SecretStr(\"district\"))\n\n    def get_client(self) -&gt; Dhis2Client:\n        return Dhis2Client(\n            self.base_url,\n            self.username,\n            self.password.get_secret_value(),\n        )\n</code></pre> <p>This pattern separates credential storage (Block) from API logic (Client), following the same convention as <code>GcpCredentials.get_client()</code> in prefect-gcp.</p>"},{"location":"core-concepts/#airflow-connections-vs-prefect-blocks","title":"Airflow Connections vs Prefect Blocks","text":"Airflow Prefect <code>Connection</code> (host, login, password, extras) Custom <code>Block</code> with typed fields <code>BaseHook.get_connection(\"conn_id\")</code> <code>MyBlock.load(\"block-name\")</code> Fernet-encrypted in metadata DB Encrypted at rest in Prefect server Password as plain string <code>SecretStr</code> prevents accidental logging Hook classes with methods Block classes with methods"},{"location":"core-concepts/#pydantic-models","title":"Pydantic Models","text":"<p>Prefect works natively with Pydantic models as task parameters and return types. This gives automatic validation, serialisation, and type safety:</p> <pre><code>from pydantic import BaseModel, field_validator\n\nclass WeatherReading(BaseModel):\n    station_id: str\n    temperature: float\n    humidity: float\n\n    @field_validator(\"temperature\")\n    @classmethod\n    def temperature_in_range(cls, v: float) -&gt; float:\n        if v &lt; -100 or v &gt; 60:\n            raise ValueError(f\"Temperature {v} out of range\")\n        return v\n</code></pre> <p>Pydantic replaces the manual serialisation required by Airflow's XCom. Models flow between tasks naturally, with validation happening automatically.</p>"},{"location":"core-concepts/#transactions","title":"Transactions","text":"<p>Transactions group tasks atomically. If any task in the group fails, the entire transaction is treated as a unit:</p> <pre><code>from prefect.transactions import transaction\n\n@flow\ndef atomic_pipeline():\n    with transaction():\n        step_a()\n        step_b()\n        step_c()\n</code></pre> <p>Transactions are a Prefect-specific feature with no direct Airflow equivalent.</p>"},{"location":"core-concepts/#async-flows","title":"Async Flows","text":"<p>Prefect natively supports <code>async def</code> tasks and flows. Use <code>asyncio.gather()</code> for concurrent I/O-bound work:</p> <pre><code>@task\nasync def fetch(url: str) -&gt; dict:\n    await asyncio.sleep(0.5)\n    return {\"url\": url}\n\n@flow\nasync def pipeline() -&gt; None:\n    results = await asyncio.gather(fetch(\"a\"), fetch(\"b\"), fetch(\"c\"))\n</code></pre> <p>Sync and async tasks can be mixed in an async flow. Async flows use <code>asyncio.run()</code> in <code>__main__</code>.</p>"},{"location":"core-concepts/#deployment-and-scheduling","title":"Deployment and Scheduling","text":"<p>Prefect supports three schedule types:</p> <ul> <li>CronSchedule -- standard cron expressions (<code>\"0 6 * * *\"</code>)</li> <li>IntervalSchedule -- fixed intervals (<code>interval=900</code> seconds)</li> <li>RRuleSchedule -- RFC 5545 recurrence rules (<code>\"FREQ=WEEKLY;BYDAY=MO,WE,FR\"</code>)</li> </ul> <p>Schedules are passed to <code>flow.serve()</code> or <code>flow.deploy()</code>:</p> <pre><code>my_flow.serve(name=\"daily\", cron=\"0 6 * * *\")\nmy_flow.serve(name=\"every-15m\", interval=900)\nmy_flow.serve(name=\"weekdays\", rrule=\"FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR\")\n</code></pre> <p>In <code>prefect.yaml</code>, schedules are declared per deployment:</p> <pre><code>schedules:\n  - cron: \"0 6 * * *\"\n    timezone: \"UTC\"\n</code></pre>"},{"location":"core-concepts/#passing-parameters-to-deployments","title":"Passing parameters to deployments","text":"<p>Deployments can override a flow's default parameters. Parameters are passed at deployment creation and can be overridden per run:</p> <pre><code># At deployment time (default parameters for all runs)\nmy_flow.serve(\n    name=\"dhis2-sync\",\n    parameters={\"endpoints\": [\"organisationUnits\"]},\n)\n\n# Override at run time via CLI\n# prefect deployment run my-flow/dhis2-sync -p endpoints='[\"dataElements\"]'\n</code></pre>"},{"location":"core-concepts/#deployment-aware-flows-with-prefectruntime","title":"Deployment-aware flows with <code>prefect.runtime</code>","text":"<p>Use <code>prefect.runtime</code> to access deployment context inside a running flow:</p> <pre><code>from prefect.runtime import deployment, flow_run\n\ndeployment_name = deployment.name            # None for local runs\nflow_run_name = flow_run.name                # auto-generated name\nscheduled_start = flow_run.scheduled_start_time\n</code></pre> <p>This lets flows adapt their behaviour depending on whether they are running locally or inside a deployment.</p>"},{"location":"core-concepts/#changing-schedules-after-deployment","title":"Changing schedules after deployment","text":"<p>Use the CLI to update schedules without redeploying:</p> <pre><code>prefect deployment set-schedule &lt;name&gt; --cron \"0 8 * * *\"\nprefect deployment set-schedule &lt;name&gt; --interval 1800\nprefect deployment set-schedule &lt;name&gt; --rrule \"FREQ=DAILY;BYDAY=MO,WE,FR\"\nprefect deployment clear-schedule &lt;name&gt;\n</code></pre> <p>Or re-run <code>prefect deploy</code> after updating <code>prefect.yaml</code>.</p>"},{"location":"core-concepts/#file-io","title":"File I/O","text":"<p>Prefect flows handle file I/O with stdlib modules (<code>csv</code>, <code>json</code>, <code>pathlib</code>, <code>tempfile</code>). Each file operation is a <code>@task</code> for observability:</p> <pre><code>@task\ndef read_csv(path: Path) -&gt; list[dict]:\n    with open(path, newline=\"\") as f:\n        return list(csv.DictReader(f))\n</code></pre> <p>Use <code>tempfile.mkdtemp()</code> for isolated working directories in flows, and <code>tmp_path</code> in tests. For mixed file types, dispatch on path suffix. Track processed files in a JSON manifest for incremental processing.</p>"},{"location":"core-concepts/#data-quality","title":"Data Quality","text":"<p>Define quality rules as configuration and execute them against data:</p> <pre><code>@task\ndef execute_rule(data: list[dict], rule: QualityRule) -&gt; RuleResult:\n    if rule.rule_type == \"not_null\":\n        return run_not_null_check.fn(data, rule.column)\n</code></pre> <p>Score rules individually, then compute an overall quality score with traffic-light classification (green/amber/red). For cross-dataset validation, check referential integrity between related datasets.</p> <p>Statistical profiling uses the stdlib <code>statistics</code> module (mean, stdev, median) to profile columns by inferred type (numeric vs string).</p>"},{"location":"core-concepts/#analytics-and-modeling","title":"Analytics and Modeling","text":"<p>Phase 5 introduces statistical analysis and modeling patterns:</p> <ul> <li>Pearson correlation -- manual implementation using <code>math</code> and <code>statistics</code>   modules (no numpy/scipy required)</li> <li>Linear regression -- ordinary least squares with R-squared computation</li> <li>Star schema -- dimensional modeling with fact and dimension tables,   surrogate keys, and composite index ranking</li> <li>Log returns -- financial time series analysis with rolling volatility</li> <li>Hypothesis testing -- educational null hypothesis validation pattern</li> </ul> <p>These patterns demonstrate that analytics pipelines can be built with stdlib modules alone, making flows lightweight and dependency-free.</p>"},{"location":"core-concepts/#blocks-and-connections","title":"Blocks and Connections","text":"<p>In Airflow, external system credentials are stored as Connections and accessed via <code>BaseHook.get_connection(\"conn_id\")</code>. In Prefect, the equivalent is a custom Block -- a typed, serializable configuration object whose <code>get_client()</code> method returns an authenticated API client:</p> <pre><code>from prefect.blocks.core import Block\nfrom pydantic import Field, SecretStr\n\nclass Dhis2Credentials(Block):\n    base_url: str = \"https://play.im.dhis2.org/dev\"\n    username: str = \"admin\"\n    password: SecretStr = Field(default=SecretStr(\"district\"))\n\n    def get_client(self) -&gt; Dhis2Client:\n        return Dhis2Client(\n            self.base_url,\n            self.username,\n            self.password.get_secret_value(),\n        )\n\n# Register once:\nDhis2Credentials(base_url=\"https://dhis2.example.org\").save(\"dhis2\")\n\n# Load in any flow:\ncreds = Dhis2Credentials.load(\"dhis2\")\nclient = creds.get_client()\nunits = client.fetch_metadata(\"organisationUnits\")\n</code></pre> <p>Password is stored as <code>SecretStr</code> directly on the block. When saved to a Prefect server, <code>SecretStr</code> fields are encrypted at rest.</p> <p>Multiple configuration strategies can coexist -- inline defaults for development, saved blocks for production, environment variables for CI:</p> Strategy Best for Example Inline <code>Block()</code> Development, testing <code>Dhis2Credentials()</code> <code>Block.load()</code> Production with Prefect server <code>Dhis2Credentials.load(\"dhis2\")</code> <code>SecretStr</code> on Block Credentials with config <code>password: SecretStr</code> <code>Secret.load()</code> Standalone passwords, API keys <code>Secret.load(\"dhis2-password\")</code> <code>os.environ</code> CI/CD, containers <code>os.environ[\"DHIS2_PASSWORD\"]</code> <code>JSON.load()</code> Structured config <code>JSON.load(\"dhis2-config\")</code>"},{"location":"core-concepts/#airflow-to-prefect-comparison","title":"Airflow to Prefect comparison","text":"Airflow concept Prefect equivalent Example DAG <code>@flow</code> 001 PythonOperator <code>@task</code> 002 <code>&gt;&gt;</code> / <code>set_downstream</code> Return-value wiring, <code>.submit()</code> 003 TaskFlow API (<code>@task</code>) Native -- Prefect is taskflow-first 004 XCom push/pull Return values 005 BranchPythonOperator Python <code>if/elif/else</code> 006 <code>on_failure_callback</code> / trigger_rule State hooks, <code>allow_failure</code> 007 Jinja2 templating / params Typed function parameters 008 TaskGroup / SubDagOperator Subflows (<code>@flow</code> calling <code>@flow</code>) 009 <code>expand()</code> (dynamic task mapping) <code>.map()</code> 010 Sensor (poke/reschedule) While-loop polling 011 <code>retries</code> + callbacks <code>retries</code>, <code>retry_delay_seconds</code>, hooks 012 Custom operators / shared utils Python imports 013 Custom XCom + trigger rules <code>emit_event()</code> 014 TriggerDagRunOperator Subflow calls, <code>run_deployment()</code> 015 Pool slots <code>concurrency()</code> context manager 016 Variables + params <code>Variable.get()</code>/<code>set()</code> 017 ShortCircuitOperator Python <code>return</code> 018 <code>@setup</code> / <code>@teardown</code> Context managers, <code>try/finally</code> 019 Complex DAG Subflows + <code>.map()</code> + hooks 020 Custom caching / Redis <code>cache_policy</code>, <code>cache_key_fn</code> 021 <code>execution_timeout</code> <code>timeout_seconds</code> 022 Custom <code>task_id</code> <code>task_run_name</code> 023 <code>exponential_backoff</code> <code>retry_delay_seconds</code> list, <code>retry_jitter_factor</code> 024 Task instance logger <code>get_run_logger()</code> 025 DAG/task tags <code>tags=</code>, <code>tags()</code> context manager 026 Custom <code>run_id</code> <code>flow_run_name</code> 027 XCom backend config <code>persist_result</code>, <code>result_storage_key</code> 028 Custom HTML / reports <code>create_markdown_artifact()</code> 029 UI plugins <code>create_table_artifact()</code>, <code>create_link_artifact()</code> 030 Connections (encrypted) <code>Secret</code> block 031 Custom connection types Custom <code>Block</code> subclass 032 Deferrable operators <code>async def</code> tasks and flows 033 Parallel deferrable ops <code>asyncio.gather()</code> 034 Mixed operator types Sync + async tasks in async flow 035 Dynamic mapping + async <code>.map()</code> / <code>.submit()</code> with async tasks 036 DAG in <code>dags/</code> folder <code>flow.serve()</code> 037 <code>schedule_interval</code> <code>CronSchedule</code>, <code>IntervalSchedule</code>, <code>RRuleSchedule</code> 038 Executors (Celery, K8s) Work pools + workers 039 Production DAG Caching + retries + artifacts + tags 040 XCom + complex types Pydantic <code>BaseModel</code> params/returns 041 BashOperator <code>subprocess.run()</code> in a <code>@task</code> 042 HttpOperator <code>httpx</code> in a <code>@task</code> 043 Custom operators Task factory functions 044 <code>expand_kwargs</code> Multi-arg <code>.map()</code> 045 Error handling patterns Quarantine pattern with Pydantic 046 Schema validation Pydantic <code>field_validator</code> 047 SLA miss detection <code>time.monotonic()</code> + threshold checks 048 Webhook callbacks <code>httpx.post()</code> + flow hooks 049 Progressive retry <code>retries</code> + <code>on_failure</code> hooks 050 Thin DAG wiring Pure functions + thin <code>@task</code> wrappers 051 Custom hooks/sensors Python decorators wrapping <code>@task</code> 052 Trigger rules <code>allow_failure</code>, state inspection 053 TaskGroups Nested subflows (<code>@flow</code> calling <code>@flow</code>) 054 Backfill / <code>logical_date</code> Flow parameters for date ranges 055 Jinja <code>{{ ds }}</code> macros <code>prefect.runtime</code> context 056 No equivalent <code>transaction()</code> for atomic groups 057 Human-in-the-loop ops <code>pause_flow_run()</code> / approval pattern 058 Executors Task runners (<code>ThreadPoolTaskRunner</code>) 059 Full ETL SCD pipeline Capstone: all Phase 3 features 060 CSV landing zone stdlib <code>csv</code> in <code>@task</code> 061 JSON event ingestion Recursive flatten, NDJSON output 062 Multi-file batch File-type dispatch, hash dedup 063 Incremental file processing JSON manifest tracking 064 Freshness/completeness checks Config-driven quality rules 065 Referential integrity FK checks between datasets 066 Quality dashboard Statistical profiling (<code>statistics</code>) 067 Pipeline health check Meta-monitoring / watchdog 068 Multi-city forecast Chained <code>.map()</code> calls 069 Paginated API fetch Offset/limit simulation, chunked <code>.map()</code> 070 Cross-API enrichment Multi-source join, partial fallback 071 Cached API comparison Application-level cache with TTL 072 API-triggered config Config-driven stage dispatch 073 Asset producer/consumer File-based data contracts 074 No equivalent Circuit breaker state machine 075 Multi-API dashboard Pydantic discriminated unions 076 GeoJSON / OData pivot Windowed batch, anomaly detection 077 No equivalent Hash-based idempotency registry 078 No equivalent Checkpoint-based stage recovery 079 Quality framework + dashboard Capstone: all Phase 4 features 080 WHO threshold classification Threshold-based AQI classification 081 Weighted risk scoring Multi-source composite risk index 082 Seasonal analysis Latitude-daylight correlation 083 Parquet aggregation Fan-out grouped aggregation 084 Nested JSON normalization Pydantic model flattening 085 Multi-indicator correlation Pearson correlation matrix 086 Currency volatility analysis Log returns, rolling volatility 087 Cross-domain hypothesis test Null hypothesis validation 088 Log-linear regression Manual OLS regression 089 Dimensional modeling Star schema, composite index 090 SQL-based ETL layers Simulated staging/production/summary 091 Generic data transfer Category computation, checksum verification 092 Org unit hierarchy Tree flattening, path-based depth 093 Expression parsing Regex complexity scoring 094 GeoJSON construction Spatial feature collection 095 Combined parallel export Fan-in multi-endpoint summary 096 No equivalent Data lineage tracking (hashlib) 097 No equivalent Pipeline template factory 098 No equivalent Multi-pipeline orchestrator 099 Full analytics pipeline Grand capstone: all Phase 5 patterns 100 <code>BaseHook.get_connection()</code> Custom <code>Block</code> with methods + <code>SecretStr</code> 101 DHIS2 org unit fetch Block auth + Pydantic flattening 102 DHIS2 data element fetch Block auth + categorization 103 DHIS2 indicator fetch Block auth + regex expression parsing 104 DHIS2 geometry export Block auth + GeoJSON construction 105 DHIS2 combined export Parallel <code>.submit()</code> + shared block 106 DHIS2 analytics query Dimension query + headers/rows parsing 107 Full DHIS2 pipeline Multi-stage pipeline + quality + dashboard 108 Connection/Variable config Multiple config strategies (Block, Secret, env) 109 Authenticated API pattern Pluggable auth block (api_key, bearer, basic) 110 Scheduled DAG + Connections <code>flow.deploy()</code> with blocks + artifacts <code>deployments/dhis2_ou</code>"},{"location":"feature-coverage/","title":"Feature Coverage","text":"<p>Prefect 3 features covered by this example repository, and gaps still to fill.</p>"},{"location":"feature-coverage/#coverage-matrix","title":"Coverage matrix","text":"Feature Covered Example flows Notes <code>@flow</code> / <code>@task</code> Yes Basics 1--20 Core building blocks Subflows Yes Subflows, Flow of Flows, Complex Pipeline Nested <code>@flow</code> calls <code>.map()</code> / <code>.submit()</code> Yes Dynamic Tasks, Advanced Map Patterns Fan-out and parallel execution Retries / retry delays Yes Retries and Hooks, Advanced Retries Lists, jitter, condition functions Task caching Yes Task Caching <code>INPUTS</code>, <code>TASK_SOURCE</code>, <code>cache_key_fn</code> Timeouts Yes Task Timeouts <code>timeout_seconds</code> on tasks and flows State hooks Yes State Handlers, Failure Escalation <code>on_failure</code>, <code>on_completion</code> <code>allow_failure</code> Yes Advanced State Handling Downstream continues after failure Parameters Yes Parameterized Flows Typed defaults, runtime overrides Results / persistence Yes Task Results, Result Persistence <code>persist_result</code>, <code>result_storage_key</code> Remote result storage Yes Remote Result Storage <code>result_storage=S3Bucket(...)</code> on tasks/flows Artifacts Yes Markdown Artifacts, Table and Link Artifacts, Progress Artifacts Markdown, tables, links, progress tracking in UI Tags Yes Tags Decorator and context-manager Custom run names Yes Task Run Names, Flow Run Names Templates and callables Structured logging Yes Structured Logging <code>get_run_logger()</code>, <code>log_prints</code> Events Yes Events <code>emit_event()</code> Blocks / Secret Yes Secret Block, Custom Blocks, DHIS2 flows <code>Block</code> subclass, <code>SecretStr</code> Variables Yes Variables and Params <code>Variable.get()</code> / <code>set()</code> Concurrency limits (local) Yes Concurrency Limits <code>concurrency()</code> context manager Async tasks/flows Yes Async Tasks, Concurrent Async, Async Flow Patterns <code>asyncio.gather()</code> <code>flow.serve()</code> Yes Flow Serve In-process scheduling <code>flow.deploy()</code> Yes Work Pools Work pool + worker <code>prefect.yaml</code> Yes Deployments directory Declarative deployments Schedules (cron/interval/rrule) Yes Schedules All three schedule types <code>prefect.runtime</code> Yes Runtime Context Deployment-aware flows Transactions Yes Transactions <code>transaction()</code> for atomic groups Interactive flows Yes Interactive Flows Mock approval (real <code>pause_flow_run()</code> requires server) Task runners (ThreadPool) Yes Task Runners <code>ThreadPoolTaskRunner</code> Pydantic models Yes Pydantic Models, Pydantic Validation Type-safe data passing S3 storage (prefect-aws) Yes S3 Parquet Export, DHIS2 GeoParquet Export <code>S3Bucket</code>, <code>MinIOCredentials</code> Notification blocks Yes Notification Blocks Built-in blocks (Slack, Teams, Custom Webhook) with <code>notify()</code> interface Webhooks Yes Webhook Block <code>Webhook</code> block for outbound HTTP; inbound webhooks documented Basic auth Yes Docker Compose stack <code>PREFECT_SERVER_API_AUTH_STRING</code> / <code>PREFECT_API_AUTH_STRING</code>"},{"location":"feature-coverage/#gaps-features-not-yet-covered","title":"Gaps -- features not yet covered","text":""},{"location":"feature-coverage/#high-priority-core-prefect-3-differentiators","title":"High priority (core Prefect 3 differentiators)","text":"Feature What it does Complexity Why it matters Automations and triggers Event-driven flow execution, deployment triggers, state-change reactions Advanced Core Prefect 3 differentiator for reactive orchestration Global concurrency limits Cross-flow resource throttling via API (vs local <code>concurrency()</code>) Intermediate Important for shared-resource coordination across deployments Rate limits <code>prefect.concurrency</code> API-level rate limiting for tasks and flows Intermediate Prevents overwhelming external APIs in production"},{"location":"feature-coverage/#medium-priority-commonly-used-integrations","title":"Medium priority (commonly used integrations)","text":"Feature What it does Complexity Why it matters prefect-sqlalchemy Database connections, parameterized SQL queries Intermediate Most common integration for data engineering prefect-dbt dbt Cloud/Core orchestration from Prefect flows Advanced Popular for analytics engineering teams Docker/K8s work pools Push-based execution to containers Advanced Production infrastructure patterns GitHub storage deployments Pull flow code from GitHub repos at runtime Intermediate GitOps deployment workflow DaskTaskRunner / RayTaskRunner Distributed task execution for CPU-heavy workloads Advanced Scaling beyond single-machine parallelism <code>pause_flow_run()</code> (real) Server-side flow pause with human-in-the-loop approval Intermediate Interactive flows example uses mock; real pause needs server"},{"location":"feature-coverage/#lower-priority-nice-to-have","title":"Lower priority (nice to have)","text":"Feature What it does Complexity Managed execution Prefect Cloud managed infrastructure Intermediate Custom event triggers Complex event-driven patterns with compound triggers Advanced Work pool priority Priority-based flow run scheduling across pools Intermediate Flow run infrastructure overrides Per-run infrastructure customization Intermediate Reverse proxy auth (SSO/OIDC) oauth2-proxy + nginx/Caddy/Traefik for multi-user auth Advanced RBAC Role-based access control (Prefect Cloud only) N/A Incident management Prefect Cloud incident creation and response N/A"},{"location":"flow-reference/","title":"Flow Reference","text":"<p>Detailed walkthrough of all 113 example flows, organised by category.</p>"},{"location":"flow-reference/#basics","title":"Basics","text":""},{"location":"flow-reference/#hello-world","title":"Hello World","text":"<p>What it demonstrates: The simplest possible Prefect flow -- two tasks executed sequentially.</p> <p>Airflow equivalent: BashOperator tasks with <code>&gt;&gt;</code> dependency.</p> <pre><code>@task\ndef say_hello() -&gt; str:\n    msg = \"Hello from Prefect!\"\n    print(msg)\n    return msg\n\n@flow(name=\"basics_hello_world\", log_prints=True)\ndef hello_world() -&gt; None:\n    say_hello()\n    print_date()\n</code></pre> <p>Tasks are plain Python functions. Call them inside a <code>@flow</code> and Prefect tracks everything automatically.</p>"},{"location":"flow-reference/#python-tasks","title":"Python Tasks","text":"<p>What it demonstrates: Tasks with typed parameters and return values.</p> <p>Airflow equivalent: PythonOperator with <code>python_callable</code>.</p> <pre><code>@task\ndef greet(name: str, greeting: str = \"Hello\") -&gt; str:\n    msg = f\"{greeting}, {name}!\"\n    print(msg)\n    return msg\n\n@task\ndef compute_sum(a: int, b: int) -&gt; int:\n    result = a + b\n    print(f\"{a} + {b} = {result}\")\n    return result\n</code></pre> <p>Any Python function becomes a task with <code>@task</code> -- type hints, defaults, and docstrings all work as expected.</p>"},{"location":"flow-reference/#task-dependencies","title":"Task Dependencies","text":"<p>What it demonstrates: Parallel fan-out with <code>.submit()</code> and a join step.</p> <p>Airflow equivalent: <code>&gt;&gt;</code> operator / <code>set_downstream</code>.</p> <pre><code>@flow(name=\"basics_task_dependencies\", log_prints=True)\ndef task_dependencies_flow() -&gt; None:\n    initial = start()\n\n    future_a = task_a.submit(initial)\n    future_b = task_b.submit(initial)\n    future_c = task_c.submit(initial)\n\n    join([future_a.result(), future_b.result(), future_c.result()])\n</code></pre> <p><code>.submit()</code> launches tasks concurrently. Call <code>.result()</code> to wait for their outputs before passing them downstream.</p>"},{"location":"flow-reference/#taskflow-etl","title":"Taskflow ETL","text":"<p>What it demonstrates: Classic extract-transform-load wired through return values.</p> <p>Airflow equivalent: TaskFlow API (<code>@task</code>).</p> <pre><code>@flow(name=\"basics_taskflow_etl\", log_prints=True)\ndef taskflow_etl_flow() -&gt; None:\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n</code></pre> <p>Prefect is natively taskflow-first. Each task returns data and the next task receives it -- no XCom needed.</p>"},{"location":"flow-reference/#task-results","title":"Task Results","text":"<p>What it demonstrates: Passing structured data (dicts, lists) between tasks.</p> <p>Airflow equivalent: XCom push/pull.</p> <pre><code>@task\ndef produce_metrics() -&gt; dict[str, Any]:\n    return {\"total\": 150, \"average\": 37.5, \"items\": [\"alpha\", \"beta\", \"gamma\", \"delta\"]}\n\n@task\ndef consume_metrics(metrics: dict[str, Any]) -&gt; str:\n    summary = f\"Total: {metrics['total']}, Average: {metrics['average']}\"\n    return summary\n</code></pre> <p>Return values replace XCom entirely. Pass dicts, lists, or any serialisable object between tasks.</p>"},{"location":"flow-reference/#control-flow","title":"Control Flow","text":""},{"location":"flow-reference/#conditional-logic","title":"Conditional Logic","text":"<p>What it demonstrates: Branching with plain Python <code>if/elif/else</code>.</p> <p>Airflow equivalent: BranchPythonOperator.</p> <pre><code>@flow(name=\"basics_conditional_logic\", log_prints=True)\ndef conditional_logic_flow() -&gt; None:\n    branch = check_condition()\n\n    if branch == \"a\":\n        path_a()\n    elif branch == \"b\":\n        path_b()\n    else:\n        default_path()\n</code></pre> <p>No special operators needed. Python control flow works directly inside flows.</p>"},{"location":"flow-reference/#state-handlers","title":"State Handlers","text":"<p>What it demonstrates: Reacting to task/flow state changes with hook functions, and continuing past failures with <code>allow_failure</code>.</p> <p>Airflow equivalent: <code>on_failure_callback</code> / trigger_rule.</p> <pre><code>@task(on_failure=[on_task_failure])\ndef fail_task():\n    raise ValueError(\"Intentional failure for demonstration\")\n\n@flow(name=\"basics_state_handlers\", log_prints=True, on_completion=[on_flow_completion])\ndef state_handlers_flow() -&gt; None:\n    succeed_task()\n    failing_future = fail_task.submit()\n    always_run_task(wait_for=[allow_failure(failing_future)])\n</code></pre> <p>Hooks are plain functions (not tasks) that receive <code>task</code>, <code>task_run</code>, and <code>state</code>. <code>allow_failure</code> lets downstream tasks run even when upstream tasks fail.</p>"},{"location":"flow-reference/#parameterized-flows","title":"Parameterized Flows","text":"<p>What it demonstrates: Runtime parameters with typed defaults.</p> <p>Airflow equivalent: Jinja2 templating / params dict.</p> <pre><code>@flow(name=\"basics_parameterized_flows\", log_prints=True)\ndef parameterized_flow(\n    name: str = \"World\",\n    date_str: str | None = None,\n    template: str = \"Greetings, {name}! Today is {date}.\",\n) -&gt; None:\n    if date_str is None:\n        date_str = datetime.date.today().isoformat()\n    build_greeting(name, date_str, template)\n</code></pre> <p>Flow parameters are regular Python function arguments. Type hints and defaults are preserved in the Prefect UI when the flow is deployed.</p>"},{"location":"flow-reference/#composition","title":"Composition","text":""},{"location":"flow-reference/#subflows","title":"Subflows","text":"<p>What it demonstrates: Composing larger pipelines from smaller, reusable flows.</p> <p>Airflow equivalent: TaskGroup / SubDagOperator.</p> <pre><code>@flow(name=\"basics_subflows\", log_prints=True)\ndef pipeline_flow() -&gt; None:\n    raw = extract_flow()\n    transformed = transform_flow(raw)\n    load_flow(transformed)\n</code></pre> <p>A <code>@flow</code> can call other <code>@flow</code> functions. Each subflow appears as a nested flow run in the Prefect UI with its own state tracking.</p>"},{"location":"flow-reference/#dynamic-tasks","title":"Dynamic Tasks","text":"<p>What it demonstrates: Dynamic fan-out over a list of items with <code>.map()</code>.</p> <p>Airflow equivalent: Dynamic task mapping (<code>expand()</code>).</p> <pre><code>@flow(name=\"basics_dynamic_tasks\", log_prints=True)\ndef dynamic_tasks_flow() -&gt; None:\n    items = generate_items()\n    processed = process_item.map(items)\n    summarize(processed)\n</code></pre> <p><code>.map()</code> creates one task run per item. The number of items can vary at runtime -- no DAG rewrite required.</p>"},{"location":"flow-reference/#operational","title":"Operational","text":""},{"location":"flow-reference/#polling-tasks","title":"Polling Tasks","text":"<p>What it demonstrates: Waiting for an external condition with a polling loop.</p> <p>Airflow equivalent: Sensor (poke/reschedule).</p> <pre><code>@task\ndef poll_condition(name: str, interval: float = 1.0, timeout: float = 10.0,\n                   succeed_after: float = 3.0) -&gt; str:\n    start_time = time.monotonic()\n    while True:\n        elapsed = time.monotonic() - start_time\n        if elapsed &gt;= succeed_after:\n            return f\"[{name}] Condition met after {elapsed:.1f}s\"\n        if elapsed &gt;= timeout:\n            raise TimeoutError(f\"[{name}] Timed out after {elapsed:.1f}s\")\n        time.sleep(interval)\n</code></pre> <p>No special sensor class needed. A <code>while</code> loop with <code>time.sleep()</code> inside a task accomplishes the same thing.</p>"},{"location":"flow-reference/#retries-and-hooks","title":"Retries and Hooks","text":"<p>What it demonstrates: Automatic retries and lifecycle hooks on tasks and flows.</p> <p>Airflow equivalent: <code>retries</code> + <code>on_failure_callback</code>.</p> <pre><code>@task(retries=3, retry_delay_seconds=1, on_failure=[my_task_failure_hook])\ndef flaky_task(fail_count: int = 2) -&gt; str:\n    key = \"flaky_task\"\n    _attempt_counter[key] = _attempt_counter.get(key, 0) + 1\n    attempt = _attempt_counter[key]\n    if attempt &lt;= fail_count:\n        raise ValueError(f\"Attempt {attempt}/{fail_count} \u2014 simulated failure\")\n    return f\"flaky_task succeeded on attempt {attempt}\"\n</code></pre> <p><code>retries</code> and <code>retry_delay_seconds</code> are set on the decorator. Hooks fire on state transitions for logging or alerting.</p>"},{"location":"flow-reference/#reuse-and-events","title":"Reuse and Events","text":""},{"location":"flow-reference/#reusable-tasks","title":"Reusable Tasks","text":"<p>What it demonstrates: Importing shared tasks from a project task library.</p> <p>Airflow equivalent: Custom operators / shared utils.</p> <pre><code>from prefect_examples.tasks import print_message, square_number\n\n@flow(name=\"basics_reusable_tasks\", log_prints=True)\ndef reusable_tasks_flow() -&gt; None:\n    print_message(\"Hello from reusable tasks!\")\n    result = square_number(7)\n</code></pre> <p>Tasks are just Python functions. Import them from a shared module and call them in any flow. The shared library lives in <code>src/prefect_examples/tasks.py</code>.</p>"},{"location":"flow-reference/#events","title":"Events","text":"<p>What it demonstrates: Emitting custom Prefect events for observability and automation triggers.</p> <p>Airflow equivalent: Custom XCom + trigger rules.</p> <pre><code>@task\ndef emit_completion_event(result: str) -&gt; None:\n    emit_event(\n        event=\"flow.data.produced\",\n        resource={\"prefect.resource.id\": \"prefect_examples.014\"},\n        payload={\"result\": result},\n    )\n</code></pre> <p><code>emit_event()</code> sends custom events to the Prefect event system. These can trigger automations, dashboards, or downstream workflows.</p>"},{"location":"flow-reference/#advanced","title":"Advanced","text":""},{"location":"flow-reference/#flow-of-flows","title":"Flow of Flows","text":"<p>What it demonstrates: Orchestrating multiple flows from a parent flow.</p> <p>Airflow equivalent: TriggerDagRunOperator.</p> <pre><code>@flow(name=\"basics_flow_of_flows\", log_prints=True)\ndef orchestrator() -&gt; None:\n    raw = ingest_flow()\n    processed = transform_flow(raw)\n    summary = report_flow(processed)\n    print(f\"Pipeline complete: {summary}\")\n</code></pre> <p>The orchestrator calls subflows (ingest, transform, report) in sequence. Each subflow is independently testable and reusable. For deployed flows, use <code>run_deployment()</code> to trigger remote execution.</p>"},{"location":"flow-reference/#concurrency-limits","title":"Concurrency Limits","text":"<p>What it demonstrates: Throttling parallel task execution with named limits.</p> <p>Airflow equivalent: Pool slots.</p> <pre><code>@task\ndef limited_task(item: str) -&gt; str:\n    with concurrency(\"demo-limit\", occupy=1):\n        print(f\"Processing {item!r} ...\")\n        time.sleep(0.5)\n    return f\"processed:{item}\"\n</code></pre> <p>The <code>concurrency()</code> context manager from <code>prefect.concurrency.sync</code> limits how many tasks can enter a critical section simultaneously. The limit name (<code>\"demo-limit\"</code>) is shared across all task runs.</p>"},{"location":"flow-reference/#variables-and-params","title":"Variables and Params","text":"<p>What it demonstrates: Storing and retrieving runtime configuration.</p> <p>Airflow equivalent: Variables + params.</p> <pre><code>@task\ndef read_config() -&gt; dict:\n    Variable.set(\"example_config\", '{\"debug\": true, \"batch_size\": 100}', overwrite=True)\n    raw = Variable.get(\"example_config\", default=\"{}\")\n    config = json.loads(raw)\n    return config\n</code></pre> <p><code>Variable.get()</code> and <code>Variable.set()</code> store key-value pairs in the Prefect backend. Combine with typed flow parameters for full runtime configuration.</p>"},{"location":"flow-reference/#early-return","title":"Early Return","text":"<p>What it demonstrates: Short-circuiting a flow with a plain <code>return</code>.</p> <p>Airflow equivalent: ShortCircuitOperator.</p> <pre><code>@flow(name=\"basics_early_return\", log_prints=True)\ndef early_return_flow(skip: bool = False) -&gt; None:\n    if skip:\n        print(\"Skip flag is set \u2014 returning early\")\n        return\n\n    proceed = should_continue()\n    if not proceed:\n        return\n\n    do_work()\n    do_more_work()\n</code></pre> <p>No special operator. A Python <code>return</code> statement exits the flow early and marks it as <code>Completed</code>.</p>"},{"location":"flow-reference/#context-managers","title":"Context Managers","text":"<p>What it demonstrates: Resource setup and teardown with <code>try/finally</code>.</p> <p>Airflow equivalent: <code>@setup</code> / <code>@teardown</code> decorators.</p> <pre><code>@flow(name=\"basics_context_managers\", log_prints=True)\ndef context_managers_flow() -&gt; None:\n    resource = setup_resource()\n    try:\n        use_resource(resource)\n    finally:\n        cleanup_resource(resource)\n</code></pre> <p>Standard Python resource management patterns (context managers, <code>try/finally</code>) work inside flows and guarantee teardown even on failure.</p>"},{"location":"flow-reference/#complex-pipeline","title":"Complex Pipeline","text":"<p>What it demonstrates: End-to-end pipeline combining subflows, mapped tasks, and notifications.</p> <p>Airflow equivalent: Complex DAG with branching, sensors, callbacks.</p> <pre><code>@flow(name=\"basics_complex_pipeline\", log_prints=True)\ndef complex_pipeline() -&gt; None:\n    raw = extract_stage()\n    transformed = transform_stage(raw)\n    summary = load_stage(transformed)\n    notify(summary)\n</code></pre> <p>The transform stage uses chained <code>.map()</code> calls:</p> <pre><code>@flow(name=\"basics_transform\", log_prints=True)\ndef transform_stage(raw: list[dict]) -&gt; list[dict]:\n    validated = validate_record.map(raw)\n    enriched = enrich_record.map(validated)\n    return [future.result() for future in enriched]\n</code></pre> <p>This is the capstone flow for Phase 1, demonstrating how subflows, mapped tasks, result passing, and post-pipeline notifications compose into a realistic data pipeline.</p>"},{"location":"flow-reference/#task-level-configuration-021-024","title":"Task-Level Configuration (021--024)","text":""},{"location":"flow-reference/#task-caching","title":"Task Caching","text":"<p>What it demonstrates: Task-level caching to avoid redundant computation.</p> <p>Airflow equivalent: Custom caching logic or external cache (Redis, etc.).</p> <pre><code>from prefect.cache_policies import INPUTS, TASK_SOURCE\n\n@task(cache_policy=INPUTS, cache_expiration=300)\ndef expensive_computation(x: int, y: int) -&gt; int:\n    return x * y\n\n@task(cache_policy=TASK_SOURCE + INPUTS)\ndef compound_cache_task(data: str) -&gt; str:\n    return data.upper()\n\n@task(cache_key_fn=_category_cache_key, cache_expiration=600)\ndef cached_lookup(category: str, item_id: int) -&gt; dict:\n    return {\"category\": category, \"item_id\": item_id}\n</code></pre> <p>Three caching strategies: <code>INPUTS</code> (cache by arguments), <code>TASK_SOURCE + INPUTS</code> (invalidate when code or args change), and <code>cache_key_fn</code> for custom cache keys. Cache hits are only visible in Prefect runtime.</p>"},{"location":"flow-reference/#task-timeouts","title":"Task Timeouts","text":"<p>What it demonstrates: Task-level and flow-level timeout configuration.</p> <p>Airflow equivalent: <code>execution_timeout</code> on operators.</p> <pre><code>@task(timeout_seconds=3)\ndef quick_task() -&gt; str:\n    return \"completed in time\"\n\n@task(timeout_seconds=2)\ndef slow_task() -&gt; str:\n    time.sleep(10)  # Will be interrupted by timeout\n    return \"completed\"\n\n@flow(name=\"core_task_timeouts\", log_prints=True, timeout_seconds=30)\ndef task_timeouts_flow() -&gt; None:\n    quick_task()\n    try:\n        slow_task()\n    except Exception:\n        cleanup_task(timed_out=True)\n</code></pre> <p><code>timeout_seconds</code> on <code>@task</code> or <code>@flow</code> kills execution that exceeds the limit. The flow catches the timeout and runs cleanup. Note: <code>.fn()</code> bypasses timeouts.</p>"},{"location":"flow-reference/#task-run-names","title":"Task Run Names","text":"<p>What it demonstrates: Custom task run naming using templates and callables.</p> <p>Airflow equivalent: <code>task_id</code> / custom logging for operator identification.</p> <pre><code>@task(task_run_name=\"fetch-{source}-page-{page}\")\ndef fetch_data(source: str, page: int) -&gt; dict:\n    return {\"source\": source, \"page\": page, \"records\": page * 10}\n\ndef generate_task_name() -&gt; str:\n    params = task_run.parameters\n    return f\"process-{params['region']}-batch-{params['batch_id']}\"\n\n@task(task_run_name=generate_task_name)\ndef process_batch(region: str, batch_id: int) -&gt; str:\n    return f\"Processed batch {batch_id} for region {region}\"\n</code></pre> <p>Template strings use parameter names in braces. Callables access <code>prefect.runtime.task_run.parameters</code> for dynamic naming.</p>"},{"location":"flow-reference/#advanced-retries","title":"Advanced Retries","text":"<p>What it demonstrates: Advanced retry configuration: backoff, jitter, and conditional retry logic.</p> <p>Airflow equivalent: Custom retry logic, <code>exponential_backoff</code>.</p> <pre><code>@task(retries=3, retry_delay_seconds=[1, 2, 4])\ndef backoff_task(fail_count: int = 2) -&gt; str: ...\n\n@task(retries=2, retry_delay_seconds=1, retry_jitter_factor=0.5)\ndef jittery_task(fail_count: int = 1) -&gt; str: ...\n\ndef retry_on_value_error(task, task_run, state) -&gt; bool:\n    return isinstance(state.result(raise_on_failure=False), ValueError)\n\n@task(retries=2, retry_condition_fn=retry_on_value_error)\ndef conditional_retry_task(error_type: str) -&gt; str: ...\n</code></pre> <p><code>retry_delay_seconds</code> accepts a list for escalating delays. <code>retry_jitter_factor</code> adds randomness to prevent thundering herd. <code>retry_condition_fn</code> controls which errors trigger retries.</p>"},{"location":"flow-reference/#flow-level-configuration-025-028","title":"Flow-Level Configuration (025--028)","text":""},{"location":"flow-reference/#structured-logging","title":"Structured Logging","text":"<p>What it demonstrates: Prefect's structured logging with <code>get_run_logger()</code>, print capture, and extra context fields.</p> <p>Airflow equivalent: Python logging in operators, task instance logger.</p> <pre><code>from prefect import get_run_logger\n\n@task\ndef task_with_logger(item: str) -&gt; str:\n    logger = get_run_logger()\n    logger.info(\"Processing %s\", item)\n    return f\"processed:{item}\"\n\n@task\ndef task_with_extra_context(user: str, action: str) -&gt; str:\n    logger = get_run_logger()\n    logger.info(\"Action performed\", extra={\"user\": user, \"action\": action})\n    return f\"User {user} performed {action}\"\n</code></pre> <p><code>get_run_logger()</code> returns a logger bound to the current run. Outside Prefect runtime it falls back to stdlib logging. With <code>log_prints=True</code>, <code>print()</code> output is captured as INFO-level log entries.</p>"},{"location":"flow-reference/#tags","title":"Tags","text":"<p>What it demonstrates: Tagging tasks and flows for organisation and filtering.</p> <p>Airflow equivalent: DAG/task tags for filtering in the UI.</p> <pre><code>from prefect import flow, tags, task\n\n@task(tags=[\"etl\", \"extract\"])\ndef extract_sales() -&gt; list[dict]: ...\n\n@flow(name=\"core_tags\", log_prints=True, tags=[\"examples\", \"phase-2\"])\ndef tags_flow() -&gt; None:\n    extract_sales()\n    with tags(\"ad-hoc\", \"debug\"):\n        generic_task(\"debug-data\")\n</code></pre> <p>Static tags are set on decorators. The <code>tags()</code> context manager adds runtime tags to all tasks within its scope. Tags are visible in the Prefect UI and can be used for filtering and automation rules.</p>"},{"location":"flow-reference/#flow-run-names","title":"Flow Run Names","text":"<p>What it demonstrates: Custom flow run naming using templates and callables.</p> <p>Airflow equivalent: Custom DAG <code>run_id</code> / <code>dag_run</code> naming.</p> <pre><code>@flow(flow_run_name=\"report-{env}-{date_str}\", log_prints=True)\ndef template_named_flow(env: str, date_str: str) -&gt; str: ...\n\ndef generate_flow_name() -&gt; str:\n    ts = datetime.datetime.now(datetime.UTC).strftime(\"%Y%m%d-%H%M%S\")\n    return f\"dynamic-{ts}\"\n\n@flow(flow_run_name=generate_flow_name, log_prints=True)\ndef callable_named_flow() -&gt; str: ...\n</code></pre> <p>Works like task run names but on <code>@flow</code>. Template strings and callables are both supported.</p>"},{"location":"flow-reference/#result-persistence","title":"Result Persistence","text":"<p>What it demonstrates: Persisting task and flow results for durability.</p> <p>Airflow equivalent: XCom backend configuration, custom result backends.</p> <pre><code>@task(persist_result=True)\ndef compute_metrics(data: list[int]) -&gt; dict:\n    return {\"total\": sum(data), \"mean\": statistics.mean(data)}\n\n@task(persist_result=True, result_storage_key=\"latest-summary-{parameters[label]}\")\ndef build_summary(metrics: dict, label: str) -&gt; str:\n    return f\"[{label}] Total: {metrics['total']}\"\n</code></pre> <p><code>persist_result=True</code> stores results beyond the flow run lifetime. <code>result_storage_key</code> provides a stable key for retrieval. Persistence behaviour requires a Prefect server; tests verify logic only.</p>"},{"location":"flow-reference/#artifacts-and-blocks","title":"Artifacts and Blocks","text":""},{"location":"flow-reference/#markdown-artifacts","title":"Markdown Artifacts","text":"<p>What it demonstrates: Creating markdown artifacts for rich reporting.</p> <p>Airflow equivalent: Custom HTML in XCom or external reporting tools.</p> <pre><code>from prefect.artifacts import create_markdown_artifact\n\n@task\ndef publish_report(results: list[dict]) -&gt; str:\n    markdown = \"# Report\\n| Name | Score |\\n|---|---|\\n\"\n    markdown += \"\\n\".join(f\"| {r['name']} | {r['score']} |\" for r in results)\n    create_markdown_artifact(key=\"report\", markdown=markdown, description=\"Weekly report\")\n    return markdown\n</code></pre> <p><code>create_markdown_artifact()</code> publishes formatted content visible in the Prefect UI. Without a server, it silently no-ops \u2014 tests pass locally.</p>"},{"location":"flow-reference/#table-and-link-artifacts","title":"Table and Link Artifacts","text":"<p>What it demonstrates: Table and link artifacts for structured data display.</p> <p>Airflow equivalent: Custom UI plugins, external dashboards.</p> <pre><code>from prefect.artifacts import create_link_artifact, create_table_artifact\n\n@task\ndef publish_table(inventory: list[dict]) -&gt; None:\n    create_table_artifact(key=\"inventory\", table=inventory, description=\"Inventory levels\")\n\n@task\ndef publish_links() -&gt; None:\n    create_link_artifact(key=\"dashboard\", link=\"https://example.com/dashboard\",\n                         description=\"Live dashboard\")\n</code></pre> <p>Table artifacts render as formatted tables. Link artifacts provide quick access to related resources from the flow run page.</p>"},{"location":"flow-reference/#secret-block","title":"Secret Block","text":"<p>What it demonstrates: Secure credential management with Prefect's Secret block.</p> <p>Airflow equivalent: Connections / Variables with <code>is_encrypted</code>.</p> <pre><code>from prefect.blocks.system import Secret\n\n@task\ndef get_api_key() -&gt; str:\n    try:\n        secret = Secret.load(\"example-api-key\")\n        return secret.get()\n    except ValueError:\n        return \"dev-fallback-key-12345\"\n</code></pre> <p><code>Secret.load()</code> retrieves encrypted values from the Prefect server. The fallback pattern ensures local development works without a configured server.</p>"},{"location":"flow-reference/#custom-blocks","title":"Custom Blocks","text":"<p>What it demonstrates: Defining custom Block classes for typed configuration.</p> <p>Airflow equivalent: Custom connection types, configuration classes.</p> <pre><code>from prefect.blocks.core import Block\n\nclass DatabaseConfig(Block):\n    host: str = \"localhost\"\n    port: int = 5432\n    database: str = \"mydb\"\n    username: str = \"admin\"\n\n@task\ndef connect_database(config: DatabaseConfig) -&gt; str:\n    return f\"Connected to {config.host}:{config.port}/{config.database}\"\n</code></pre> <p>Custom blocks provide typed, validated configuration. In production, save with <code>block.save(\"name\")</code> and load with <code>Block.load(\"name\")</code>. Here blocks are constructed directly for local testability.</p>"},{"location":"flow-reference/#async-patterns","title":"Async Patterns","text":""},{"location":"flow-reference/#async-tasks","title":"Async Tasks","text":"<p>What it demonstrates: Async task and flow definitions with sequential awaiting.</p> <p>Airflow equivalent: Deferrable operators (async sensor pattern).</p> <pre><code>@task\nasync def async_fetch(url: str) -&gt; dict:\n    await asyncio.sleep(0.1)\n    return {\"url\": url, \"status\": 200}\n\n@flow(name=\"core_async_tasks\", log_prints=True)\nasync def async_tasks_flow() -&gt; None:\n    response = await async_fetch(\"https://api.example.com/users\")\n    await async_process(response)\n</code></pre> <p>Async tasks and flows are defined with <code>async def</code> and awaited. The <code>__main__</code> block uses <code>asyncio.run()</code>.</p>"},{"location":"flow-reference/#concurrent-async","title":"Concurrent Async","text":"<p>What it demonstrates: Concurrent task execution with <code>asyncio.gather()</code>.</p> <p>Airflow equivalent: Multiple deferrable operators running in parallel.</p> <pre><code>@flow(name=\"core_concurrent_async\", log_prints=True)\nasync def concurrent_async_flow() -&gt; None:\n    results = await asyncio.gather(\n        fetch_endpoint(\"users\", delay=0.3),\n        fetch_endpoint(\"orders\", delay=0.5),\n        fetch_endpoint(\"products\", delay=0.2),\n    )\n    await aggregate_results(list(results))\n</code></pre> <p><code>asyncio.gather()</code> runs all fetches concurrently. Total wall-clock time is approximately <code>max(delays)</code>, not <code>sum(delays)</code>.</p>"},{"location":"flow-reference/#async-flow-patterns","title":"Async Flow Patterns","text":"<p>What it demonstrates: Mixing sync and async tasks in an async flow.</p> <p>Airflow equivalent: Mix of standard and deferrable operators.</p> <pre><code>@flow(name=\"core_async_flow_patterns\", log_prints=True)\nasync def async_flow_patterns_flow() -&gt; None:\n    raw = sync_extract()              # sync task\n    enriched = await enrich_subflow(raw)  # async subflow with gather\n    sync_load(enriched)               # sync task\n</code></pre> <p>Sync tasks are called normally inside async flows. Async subflows use <code>asyncio.gather()</code> for concurrent fan-out over records.</p>"},{"location":"flow-reference/#async-map-and-submit","title":"Async Map and Submit","text":"<p>What it demonstrates: <code>.map()</code> and <code>.submit()</code> with async tasks.</p> <p>Airflow equivalent: Dynamic task mapping with deferrable operators.</p> <pre><code>@flow(name=\"core_async_map_and_submit\", log_prints=True)\nasync def async_map_and_submit_flow() -&gt; None:\n    transform_futures = async_transform.map(items)\n    transformed = [f.result() for f in transform_futures]\n\n    validate_futures = [async_validate.submit(item) for item in transformed]\n    validations = [f.result() for f in validate_futures]\n</code></pre> <p><code>.map()</code> and <code>.submit()</code> work with async tasks for parallel fan-out within an async flow.</p>"},{"location":"flow-reference/#deployment-and-scheduling","title":"Deployment and Scheduling","text":""},{"location":"flow-reference/#flow-serve","title":"Flow Serve","text":"<p>What it demonstrates: The simplest deployment method: <code>flow.serve()</code>.</p> <p>Airflow equivalent: DAG placed in <code>dags/</code> folder, picked up by scheduler.</p> <pre><code>@flow(name=\"core_flow_serve\", log_prints=True)\ndef flow_serve_flow() -&gt; None:\n    raw = extract_data()\n    transformed = transform_data(raw)\n    load_data(transformed)\n\n# Deploy with: flow_serve_flow.serve(name=\"037-flow-serve\", cron=\"*/5 * * * *\")\n</code></pre> <p><code>flow.serve()</code> creates a lightweight deployment that runs locally. Pass <code>cron=</code> or <code>interval=</code> for scheduling. For production infrastructure isolation, use <code>flow.deploy()</code> with work pools.</p>"},{"location":"flow-reference/#schedules","title":"Schedules","text":"<p>What it demonstrates: Schedule types for Prefect deployments.</p> <p>Airflow equivalent: DAG <code>schedule_interval</code> (cron, timedelta, timetable).</p> <pre><code># CronSchedule: daily_report_flow.serve(name=\"daily\", cron=\"0 6 * * *\")\n# IntervalSchedule: interval_check_flow.serve(name=\"interval\", interval=900)\n# RRuleSchedule: custom_flow.serve(name=\"custom\", rrule=\"FREQ=WEEKLY;BYDAY=MO,WE,FR\")\n</code></pre> <p>Three schedule types: <code>CronSchedule</code> for cron expressions, <code>IntervalSchedule</code> for fixed intervals, and <code>RRuleSchedule</code> for complex recurrence rules. All can be passed to <code>flow.serve(schedule=...)</code> or <code>flow.deploy(schedule=...)</code>.</p>"},{"location":"flow-reference/#work-pools","title":"Work Pools","text":"<p>What it demonstrates: Work pool concepts for production deployments.</p> <p>Airflow equivalent: Executors (Local, Celery, Kubernetes).</p> <pre><code># Deploy to a work pool:\n# work_pools_flow.deploy(name=\"039-work-pool\", work_pool_name=\"my-pool\")\n# Start a worker:\n# prefect worker start --pool \"my-pool\"\n</code></pre> <p>Work pools define WHERE work runs. Types include <code>process</code>, <code>docker</code>, and <code>kubernetes</code>. <code>flow.deploy()</code> targets a named pool. Workers are long-running processes that poll a work pool for scheduled runs.</p>"},{"location":"flow-reference/#production-pipeline","title":"Production Pipeline","text":"<p>What it demonstrates: Capstone flow combining all Phase 2 concepts.</p> <p>Airflow equivalent: Production DAG with sensors, retries, SLAs, callbacks.</p> <pre><code>@flow(name=\"core_production_pipeline\", log_prints=True)\ndef production_pipeline() -&gt; None:\n    with tags(\"production\", \"phase-2\"):\n        raw = extract_stage()           # tagged subflow\n        transformed = transform_stage(raw)  # retries + caching\n        summary = load_stage(transformed)   # persist_result\n        notify(summary)                     # markdown artifact\n</code></pre> <p>This is the capstone flow for Phase 2, combining task caching (<code>INPUTS</code> policy), retries, markdown artifacts, tags, result persistence, and structured logging into a production-ready pipeline.</p>"},{"location":"flow-reference/#pydantic-and-data-patterns","title":"Pydantic and Data Patterns","text":""},{"location":"flow-reference/#pydantic-models","title":"Pydantic Models","text":"<p>What it demonstrates: Using Pydantic <code>BaseModel</code> as task parameters and return types for automatic validation and type safety.</p> <p>Airflow equivalent: XCom push/pull with complex types (JSON/pickle serialisation).</p> <pre><code>from pydantic import BaseModel\n\nclass UserRecord(BaseModel):\n    name: str\n    email: str\n    age: int\n\n@task\ndef extract_users(config: PipelineConfig) -&gt; list[UserRecord]:\n    raw = [{\"name\": \"Alice\", \"email\": \"alice@example.com\", \"age\": 30}]\n    return [UserRecord(**r) for r in raw[:config.batch_size]]\n</code></pre> <p>Pydantic models flow naturally between tasks -- no XCom serialisation pain. Validation happens automatically on construction.</p>"},{"location":"flow-reference/#shell-tasks","title":"Shell Tasks","text":"<p>What it demonstrates: Running shell commands and scripts from Prefect tasks using <code>subprocess</code>.</p> <p>Airflow equivalent: BashOperator.</p> <pre><code>@task\ndef run_command(cmd: str) -&gt; str:\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n    return result.stdout.strip()\n</code></pre> <p>Prefect has no BashOperator. <code>subprocess.run()</code> inside a <code>@task</code> is the direct equivalent.</p>"},{"location":"flow-reference/#http-tasks","title":"HTTP Tasks","text":"<p>What it demonstrates: Making HTTP requests from tasks using <code>httpx</code>.</p> <p>Airflow equivalent: HttpOperator, HttpSensor.</p> <pre><code>@task\ndef http_get(url: str) -&gt; dict:\n    response = httpx.get(url, timeout=10.0)\n    response.raise_for_status()\n    return response.json()\n</code></pre> <p>No special operator needed. <code>httpx</code> (a Prefect transitive dependency) in a <code>@task</code> replaces HttpOperator entirely.</p>"},{"location":"flow-reference/#task-factories","title":"Task Factories","text":"<p>What it demonstrates: Creating reusable tasks dynamically with factory functions.</p> <p>Airflow equivalent: Custom operators, <code>@task.bash</code> decorator variants.</p> <pre><code>def make_extractor(source: str):\n    @task(name=f\"extract_{source}\")\n    def extract() -&gt; dict:\n        return {\"source\": source, \"records\": [...]}\n    return extract\n\nextract_api = make_extractor(\"api\")\nextract_database = make_extractor(\"database\")\n</code></pre> <p>Factory functions generate <code>@task</code>-decorated callables for consistent behaviour across different data sources.</p>"},{"location":"flow-reference/#advanced-mapping-and-error-handling","title":"Advanced Mapping and Error Handling","text":""},{"location":"flow-reference/#advanced-map-patterns","title":"Advanced Map Patterns","text":"<p>What it demonstrates: Multi-argument <code>.map()</code>, chained maps, and result collection.</p> <p>Airflow equivalent: <code>expand_kwargs()</code>, <code>partial().expand()</code>.</p> <pre><code>station_futures = process_station.map(\n    [s[\"station_id\"] for s in stations],\n    [s[\"lat\"] for s in stations],\n    [s[\"lon\"] for s in stations],\n)\nstation_results = [f.result() for f in station_futures]\n</code></pre> <p>Unpack list-of-dicts into parallel <code>.map()</code> calls by passing separate lists for each parameter.</p>"},{"location":"flow-reference/#error-handling-etl","title":"Error Handling ETL","text":"<p>What it demonstrates: The quarantine pattern -- good rows pass through, bad rows are captured with error reasons.</p> <p>Airflow equivalent: Error handling with quarantine pattern.</p> <pre><code>class QuarantineResult(BaseModel):\n    good_records: list[dict]\n    bad_records: list[dict]\n    errors: list[str]\n\n@task\ndef process_with_quarantine(records: list[dict]) -&gt; QuarantineResult:\n    good, bad, errors = [], [], []\n    for record in records:\n        try:\n            validate(record)\n            good.append(record)\n        except ValueError as e:\n            bad.append(record)\n            errors.append(str(e))\n    return QuarantineResult(good_records=good, bad_records=bad, errors=errors)\n</code></pre> <p>Pydantic models make quarantine results structured and type-safe.</p>"},{"location":"flow-reference/#pydantic-validation","title":"Pydantic Validation","text":"<p>What it demonstrates: Using Pydantic <code>field_validator</code> for data quality enforcement.</p> <p>Airflow equivalent: Schema validation pipeline.</p> <pre><code>class WeatherReading(BaseModel):\n    station_id: str\n    temperature: float\n    humidity: float\n\n    @field_validator(\"temperature\")\n    @classmethod\n    def temperature_in_range(cls, v: float) -&gt; float:\n        if v &lt; -100 or v &gt; 60:\n            raise ValueError(f\"Temperature {v} out of range\")\n        return v\n</code></pre> <p><code>field_validator</code> replaces manual schema checking code. Invalid data raises a <code>ValidationError</code> automatically.</p>"},{"location":"flow-reference/#sla-monitoring","title":"SLA Monitoring","text":"<p>What it demonstrates: Tracking task durations and comparing against SLA thresholds.</p> <p>Airflow equivalent: SLA miss detection, <code>execution_timeout</code>.</p> <pre><code>@task\ndef sla_report(results: list[dict], thresholds: dict | None = None) -&gt; str:\n    for result in results:\n        duration = result[\"duration\"]\n        limit = thresholds.get(result[\"task\"], 1.0)\n        status = \"OK\" if duration &lt;= limit else \"BREACH\"\n</code></pre> <p>Use <code>time.monotonic()</code> for accurate timing and compare against configurable thresholds.</p>"},{"location":"flow-reference/#notifications-and-observability","title":"Notifications and Observability","text":""},{"location":"flow-reference/#webhook-notifications","title":"Webhook Notifications","text":"<p>What it demonstrates: Sending webhook notifications on pipeline events.</p> <p>Airflow equivalent: Webhook alerts on pipeline events.</p> <pre><code>@flow(\n    name=\"patterns_webhook_notifications\",\n    on_completion=[on_flow_completion],\n    on_failure=[on_flow_failure],\n)\ndef webhook_notifications_flow() -&gt; None:\n    send_notification(\"pipeline.started\", {\"source\": \"demo\"})\n    result = process_data()\n    send_notification(\"pipeline.completed\", result)\n</code></pre> <p>Flow hooks (<code>on_completion</code>, <code>on_failure</code>) trigger automatically. In production, <code>send_notification</code> would POST to Slack, PagerDuty, etc.</p>"},{"location":"flow-reference/#notification-blocks","title":"Notification Blocks","text":"<p>What it demonstrates: Using Prefect's built-in <code>SlackWebhook</code> and <code>CustomWebhookNotificationBlock</code> for pipeline alerting with a unified <code>notify(body, subject)</code> interface.</p> <p>Airflow equivalent: Slack/email/PagerDuty callbacks via operators.</p> <pre><code>@task\ndef configure_notification_blocks() -&gt; dict[str, Any]:\n    slack = SlackWebhook(url=SecretStr(\"https://hooks.slack.com/services/T00/B00/xxxx\"))\n\n    custom = CustomWebhookNotificationBlock(\n        name=\"ops-webhook\",\n        url=\"https://monitoring.example.com/alerts\",\n        method=\"POST\",\n        json_data={\"text\": \"{{subject}}: {{body}}\"},\n        secrets={\"api_token\": \"placeholder-token\"},\n    )\n    return {\"slack\": type(slack).__name__, \"custom\": type(custom).__name__}\n\n@task\ndef demonstrate_template_resolution() -&gt; dict[str, Any]:\n    block = CustomWebhookNotificationBlock(\n        name=\"template-demo\",\n        url=\"https://api.example.com/notify?token={{api_token}}\",\n        method=\"POST\",\n        json_data={\n            \"title\": \"{{subject}}\",\n            \"message\": \"{{body}}\",\n            \"source\": \"{{name}}\",\n            \"auth\": \"Bearer {{api_token}}\",\n        },\n        secrets={\"api_token\": \"secret-xyz-789\"},\n    )\n    return block._build_request_args(\n        body=\"Pipeline completed: 150 records processed\",\n        subject=\"Pipeline Alert\",\n    )\n\ndef on_completion_notify(flow, flow_run, state):\n    # SlackWebhook.load(\"prod-slack\").notify(\n    #     body=f\"Flow {flow_run.name!r} completed.\", subject=\"Flow Completed\")\n    print(f\"HOOK  Flow {flow_run.name!r} completed -- would notify via SlackWebhook\")\n\n@flow(\n    name=\"patterns_notification_blocks\",\n    log_prints=True,\n    on_completion=[on_completion_notify],\n    on_failure=[on_failure_notify],\n)\ndef notification_blocks_flow() -&gt; None:\n    channels = configure_notification_blocks()\n    for source in [\"api\", \"database\", \"file\"]:\n        process_data(source)\n    demonstrate_template_resolution()\n</code></pre> <p>All notification blocks share the same <code>notify(body, subject)</code> method. <code>SlackWebhook</code> sends to a Slack webhook URL; <code>CustomWebhookNotificationBlock</code> sends to any HTTP endpoint with template resolution for <code>{{subject}}</code>, <code>{{body}}</code>, <code>{{name}}</code>, and custom <code>secrets</code> keys. Flow hooks wire these blocks to lifecycle events for automatic alerting in production.</p>"},{"location":"flow-reference/#webhook-block","title":"Webhook Block","text":"<p>What it demonstrates: Using the built-in <code>Webhook</code> block for configurable outbound HTTP calls with stored credentials.</p> <p>Airflow equivalent: <code>SimpleHttpOperator</code> with connection credentials.</p> <pre><code>@task\ndef create_post_webhook() -&gt; dict[str, Any]:\n    webhook = Webhook(\n        method=\"POST\",\n        url=SecretStr(\"https://api.example.com/events\"),\n        headers=SecretDict({\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": \"Bearer placeholder-token\",\n        }),\n    )\n    summary = {\n        \"method\": webhook.method,\n        \"url_host\": \"api.example.com\",\n        \"header_keys\": list(webhook.headers.get_secret_value().keys()),\n    }\n    print(f\"POST webhook configured: {summary}\")\n    return summary\n\n@task\ndef simulate_webhook_call(\n    method: str, url_host: str, payload: dict[str, Any] | None = None\n) -&gt; dict[str, Any]:\n    result = {\n        \"method\": method,\n        \"url_host\": url_host,\n        \"payload\": payload,\n        \"simulated\": True,\n    }\n    print(f\"Simulated {method} to {url_host} -- payload: {payload}\")\n    return result\n\n@flow(name=\"core_webhook_block\", log_prints=True)\ndef webhook_block_flow() -&gt; dict[str, Any]:\n    post_summary = create_post_webhook()\n    post_result = simulate_webhook_call(\n        method=post_summary[\"method\"],\n        url_host=post_summary[\"url_host\"],\n        payload={\"event\": \"pipeline.completed\", \"records\": 150},\n    )\n    pattern = demonstrate_save_load_pattern()\n    return {\"post_result\": post_result, \"persistence_pattern\": pattern}\n</code></pre> <p>The <code>Webhook</code> block stores URL, method, headers, and auth in a reusable, server-persisted block. It powers the <code>CallWebhook</code> automation action and can replace ad-hoc <code>httpx.post()</code> calls with a managed, auditable configuration.</p>"},{"location":"flow-reference/#failure-escalation","title":"Failure Escalation","text":"<p>What it demonstrates: Progressive retry with escalation hooks at each failure.</p> <p>Airflow equivalent: Progressive retry with escalating callbacks.</p> <pre><code>@task(retries=3, retry_delay_seconds=0, on_failure=[on_task_failure])\ndef flaky_task(fail_count: int = 2) -&gt; str:\n    ...\n</code></pre> <p>The <code>on_failure</code> hook fires on each retry failure, allowing escalation logging. After all retries exhaust, the flow-level <code>on_completion</code> hook reports the final outcome.</p>"},{"location":"flow-reference/#testable-flow-patterns","title":"Testable Flow Patterns","text":"<p>What it demonstrates: Separating business logic from Prefect wiring for maximum testability.</p> <p>Airflow equivalent: Thin DAG wiring with logic in external modules.</p> <pre><code># Pure function (no Prefect imports)\ndef _validate_record(record: dict) -&gt; dict:\n    if not record.get(\"name\"):\n        raise ValueError(\"missing name\")\n    return {**record, \"valid\": True}\n\n# Thin task wrapper\n@task\ndef validate(record: dict) -&gt; dict:\n    return _validate_record(record)\n</code></pre> <p>Test pure functions directly (fast, no Prefect overhead) and task wrappers via <code>.fn()</code>.</p>"},{"location":"flow-reference/#reusable-utilities","title":"Reusable Utilities","text":"<p>What it demonstrates: Custom task utility decorators for consistent behaviour.</p> <p>Airflow equivalent: Custom hooks and sensors.</p> <pre><code>def timed_task(fn):\n    @task(name=fn.__name__)\n    @functools.wraps(fn)\n    def wrapper(*args, **kwargs):\n        start = time.monotonic()\n        result = fn(*args, **kwargs)\n        result[\"_duration\"] = round(time.monotonic() - start, 4)\n        return result\n    return wrapper\n\n@timed_task\ndef compute_metric(name: str, value: float) -&gt; dict:\n    return {\"name\": name, \"value\": value * 1.1, \"unit\": \"ops/sec\"}\n</code></pre> <p>Build a task utility library for timing, validation, and other cross-cutting concerns.</p>"},{"location":"flow-reference/#composition-and-scheduling","title":"Composition and Scheduling","text":""},{"location":"flow-reference/#advanced-state-handling","title":"Advanced State Handling","text":"<p>What it demonstrates: Using <code>allow_failure</code> and state inspection for mixed-outcome workflows.</p> <p>Airflow equivalent: Trigger rules (<code>all_success</code>, <code>all_done</code>, etc.).</p> <pre><code>from prefect import allow_failure\n\nfail_future = fail_task.submit()\nskip_task(wait_for=[allow_failure(fail_future)])\n</code></pre> <p><code>allow_failure</code> lets downstream tasks run even when upstream tasks fail. Combine with state inspection for conditional logic.</p>"},{"location":"flow-reference/#nested-subflows","title":"Nested Subflows","text":"<p>What it demonstrates: Organising complex pipelines with hierarchical subflow groups.</p> <p>Airflow equivalent: TaskGroups and nested groups.</p> <pre><code>@flow(name=\"patterns_nested_subflows\", log_prints=True)\ndef nested_subflows_flow() -&gt; None:\n    raw = extract_group()       # subflow with multiple tasks\n    transformed = transform_group(raw)  # subflow with clean + enrich\n    load_group(transformed)     # subflow with write + verify\n</code></pre> <p>Each subflow appears as a nested flow run in the Prefect UI with independent state tracking -- the equivalent of Airflow TaskGroups.</p>"},{"location":"flow-reference/#backfill-patterns","title":"Backfill Patterns","text":"<p>What it demonstrates: Parameterised pipelines for date-range processing with gap detection.</p> <p>Airflow equivalent: Backfill awareness, parameterised pipelines.</p> <pre><code>@flow(name=\"patterns_backfill_patterns\", log_prints=True)\ndef backfill_patterns_flow(start_date: str = \"2024-01-01\", end_date: str = \"2024-01-05\"):\n    initial_results = process_date.map(initial_dates)\n    gaps = detect_gaps(initial_dates, start_date, end_date)\n    backfill_results = process_date.map(gaps)\n</code></pre> <p>Flow parameters replace Airflow's <code>logical_date</code>. Gap detection identifies missing dates for incremental backfill.</p>"},{"location":"flow-reference/#runtime-context","title":"Runtime Context","text":"<p>What it demonstrates: Accessing flow and task run metadata at runtime.</p> <p>Airflow equivalent: Jinja templating (<code>{{ ds }}</code>), macros, runtime info.</p> <pre><code>from prefect.runtime import flow_run, task_run\n\n@task\ndef get_flow_info() -&gt; dict:\n    return {\n        \"flow_run_name\": flow_run.name,\n        \"flow_name\": flow_run.flow_name,\n    }\n</code></pre> <p><code>prefect.runtime</code> provides access to flow run ID, name, parameters, and tags -- replacing Airflow's Jinja template variables.</p>"},{"location":"flow-reference/#advanced-features-and-capstone","title":"Advanced Features and Capstone","text":""},{"location":"flow-reference/#transactions","title":"Transactions","text":"<p>What it demonstrates: Atomic task groups with rollback on failure using Prefect transactions.</p> <p>Airflow equivalent: No direct equivalent -- Prefect-specific feature.</p> <pre><code>from prefect.transactions import transaction\n\n@flow(name=\"patterns_transactions\", log_prints=True)\ndef transactions_flow() -&gt; None:\n    with transaction():\n        a = step_a()\n        b = step_b()\n        c = step_c()\n    summarize_transaction([a, b, c])\n</code></pre> <p>The <code>transaction()</code> context manager groups tasks atomically. This is a unique Prefect advantage with no Airflow equivalent.</p>"},{"location":"flow-reference/#interactive-flows","title":"Interactive Flows","text":"<p>What it demonstrates: Human-in-the-loop approval patterns.</p> <p>Airflow equivalent: Human-in-the-loop operators.</p> <pre><code>@flow(name=\"patterns_interactive_flows\", log_prints=True)\ndef interactive_flows_flow() -&gt; None:\n    data = prepare_data()\n    approved = mock_approval(data)  # In production: pause_flow_run()\n    if approved:\n        publish(data)\n    else:\n        archive(data)\n</code></pre> <p>In production, use <code>pause_flow_run()</code> to pause and wait for human input via the Prefect UI. The mock approval pattern enables local testing.</p>"},{"location":"flow-reference/#task-runners","title":"Task Runners","text":"<p>What it demonstrates: Comparing thread pool and default task runners for different workloads.</p> <p>Airflow equivalent: Executors (Local, Celery, Kubernetes).</p> <pre><code>from prefect.task_runners import ThreadPoolTaskRunner\n\n@flow(task_runner=ThreadPoolTaskRunner(max_workers=3))\ndef threaded_io_flow() -&gt; str:\n    futures = io_bound_task.map(items)\n    return summarize_runner([f.result() for f in futures], \"ThreadPool\")\n</code></pre> <p><code>ThreadPoolTaskRunner</code> provides concurrent execution for I/O-bound tasks. The default runner handles CPU-bound work.</p>"},{"location":"flow-reference/#production-pipeline-v2","title":"Production Pipeline v2","text":"<p>What it demonstrates: Capstone flow combining all Phase 3 features into a production-ready pipeline.</p> <p>Airflow equivalent: Full ETL SCD capstone.</p> <pre><code>@flow(name=\"patterns_production_pipeline_v2\", log_prints=True, on_completion=[on_pipeline_completion])\ndef production_pipeline_v2_flow() -&gt; None:\n    with tags(\"production\", \"phase-3\"):\n        source_records = extract_stage()\n        with transaction():\n            validated = validate_stage(source_records)\n        transformed = transform_stage(validated)\n        metrics = compute_metrics(transformed)\n        publish_summary(metrics)\n</code></pre> <p>This capstone combines Pydantic models with field validators, transactions, retries, markdown artifacts, tags, state hooks, and <code>.map()</code> into a realistic production pipeline.</p>"},{"location":"flow-reference/#file-io-patterns","title":"File I/O Patterns","text":""},{"location":"flow-reference/#csv-file-processing","title":"CSV File Processing","text":"<p>What it demonstrates: File-based ETL pipeline using the stdlib <code>csv</code> module with generate, read, validate, transform, write, and archive steps.</p> <p>Airflow equivalent: CSV landing zone pipeline (DAG 063).</p> <pre><code>@task\ndef validate_csv_row(row: dict, row_number: int, required_columns: list[str]) -&gt; CsvRecord:\n    errors = []\n    for col in required_columns:\n        if col not in row or not row[col].strip():\n            errors.append(f\"Missing or empty required column: {col}\")\n    return CsvRecord(row_number=row_number, data=row, valid=len(errors) == 0, errors=errors)\n</code></pre> <p><code>csv.DictReader</code> and <code>csv.DictWriter</code> replace external CSV libraries. <code>tempfile.mkdtemp()</code> provides isolated working directories.</p>"},{"location":"flow-reference/#json-event-ingestion","title":"JSON Event Ingestion","text":"<p>What it demonstrates: Recursive nested JSON flattening into dot-separated keys with NDJSON (newline-delimited JSON) output.</p> <p>Airflow equivalent: JSON event stream to Parquet (DAG 064).</p> <pre><code>@task\ndef flatten_dict(data: dict, prefix: str = \"\", separator: str = \".\") -&gt; dict:\n    items = {}\n    for key, value in data.items():\n        new_key = f\"{prefix}{separator}{key}\" if prefix else key\n        if isinstance(value, dict):\n            items.update(flatten_dict.fn(value, new_key, separator))\n        else:\n            items[new_key] = value\n    return items\n</code></pre> <p>Recursive flattening handles arbitrarily nested structures. NDJSON output writes one JSON object per line for streaming consumption.</p>"},{"location":"flow-reference/#multi-file-batch-processing","title":"Multi-File Batch Processing","text":"<p>What it demonstrates: Mixed CSV+JSON batch processing with file-type dispatch, column harmonisation, and hash-based deduplication.</p> <p>Airflow equivalent: Mixed CSV+JSON batch processing (DAG 065).</p> <pre><code>@task\ndef read_file(path: Path) -&gt; list[dict]:\n    suffix = path.suffix.lower()\n    if suffix == \".csv\":\n        with open(path, newline=\"\") as f:\n            return list(csv.DictReader(f))\n    elif suffix == \".json\":\n        return json.loads(path.read_text())\n</code></pre> <p>File suffix determines the reader. Column harmonisation maps different schemas to a unified format. Hash dedup uses <code>hashlib.sha256</code> on key fields.</p>"},{"location":"flow-reference/#incremental-processing","title":"Incremental Processing","text":"<p>What it demonstrates: Manifest-based incremental file processing. A JSON manifest tracks which files have been processed; re-runs skip them.</p> <p>Airflow equivalent: Manifest-based incremental file processing (DAG 067).</p> <pre><code>@task\ndef identify_new_files(all_files: list[Path], manifest: ProcessingManifest) -&gt; list[Path]:\n    return [f for f in all_files if f.name not in manifest.processed_files]\n</code></pre> <p>Run the flow twice: the second run processes zero files because the manifest already records them. This is the foundation for idempotent file pipelines.</p>"},{"location":"flow-reference/#data-quality-framework","title":"Data Quality Framework","text":""},{"location":"flow-reference/#quality-rules-engine","title":"Quality Rules Engine","text":"<p>What it demonstrates: Configuration-driven data quality rules with a registry pattern and traffic-light scoring (green/amber/red).</p> <p>Airflow equivalent: Freshness and completeness checks (DAG 070).</p> <pre><code>@task\ndef execute_rule(data: list[dict], rule: QualityRule) -&gt; RuleResult:\n    if rule.rule_type == \"not_null\":\n        return run_not_null_check.fn(data, rule.column)\n    elif rule.rule_type == \"range\":\n        return run_range_check.fn(data, rule.column, ...)\n</code></pre> <p>Rules are defined as config dicts, parsed into Pydantic models, and dispatched to check functions. The overall score determines the traffic light.</p>"},{"location":"flow-reference/#cross-dataset-validation","title":"Cross-Dataset Validation","text":"<p>What it demonstrates: Referential integrity checks between related datasets (orders, customers, products) with orphan detection.</p> <p>Airflow equivalent: Referential integrity checks (DAG 071).</p> <pre><code>@task\ndef check_referential_integrity(child_data, parent_data, child_key, parent_key, check_name):\n    parent_values = {row[parent_key] for row in parent_data}\n    orphan_keys = [row[child_key] for row in child_data if row[child_key] not in parent_values]\n    return IntegrityResult(orphan_count=len(orphan_keys), passed=len(orphan_keys) == 0, ...)\n</code></pre> <p>Foreign key validation is a pure Python set operation. The test data deliberately includes orphan records to demonstrate detection.</p>"},{"location":"flow-reference/#data-profiling","title":"Data Profiling","text":"<p>What it demonstrates: Statistical data profiling using the stdlib <code>statistics</code> module (mean, stdev, median) with column-level type inference.</p> <p>Airflow equivalent: Consolidated quality dashboard (DAG 072).</p> <pre><code>@task\ndef profile_numeric_column(name: str, values: list) -&gt; ColumnProfile:\n    non_null = [float(v) for v in values if v is not None]\n    return ColumnProfile(\n        name=name, dtype=\"numeric\",\n        mean=round(statistics.mean(non_null), 4),\n        stdev=round(statistics.stdev(non_null), 4),\n        median=round(statistics.median(non_null), 4), ...\n    )\n</code></pre> <p>Column type is inferred from values. Numeric columns get statistical profiles; string columns get length and uniqueness counts.</p>"},{"location":"flow-reference/#pipeline-health-monitor","title":"Pipeline Health Monitor","text":"<p>What it demonstrates: Meta-monitoring / watchdog pattern. A flow checks the health of other pipelines' outputs via file existence, freshness, row counts, and value range checks.</p> <p>Airflow equivalent: Pipeline health check (DAG 076).</p> <pre><code>@task\ndef aggregate_health(pipeline_name: str, results: list[HealthCheckResult]) -&gt; PipelineHealthReport:\n    if any(r.status == \"critical\" for r in results):\n        overall = \"critical\"\n    elif any(r.status == \"degraded\" for r in results):\n        overall = \"degraded\"\n    else:\n        overall = \"healthy\"\n</code></pre> <p>Worst-status-wins aggregation ensures a single failing check flags the entire pipeline.</p>"},{"location":"flow-reference/#api-orchestration-patterns","title":"API Orchestration Patterns","text":""},{"location":"flow-reference/#multi-source-forecast","title":"Multi-Source Forecast","text":"<p>What it demonstrates: Chained <code>.map()</code> calls: geocode cities, then fetch forecasts using the resulting coordinates.</p> <p>Airflow equivalent: Multi-city forecast, geocoding (DAGs 081, 086).</p> <pre><code>coord_futures = geocode_city.map(cities)\ncoords = [f.result() for f in coord_futures]\nforecast_futures = fetch_forecast.map(coords)\nforecasts = [f.result() for f in forecast_futures]\n</code></pre> <p>The output of one <code>.map()</code> feeds into the next. All API calls are deterministic simulations for offline testing.</p>"},{"location":"flow-reference/#api-pagination","title":"API Pagination","text":"<p>What it demonstrates: Paginated API consumption with chunked parallel processing using <code>.map()</code>.</p> <p>Airflow equivalent: Chunked API fetching (DAG 094).</p> <pre><code>@task\ndef simulate_api_page(page: int, page_size: int, total_records: int) -&gt; PageResponse:\n    total_pages = (total_records + page_size - 1) // page_size\n    start = (page - 1) * page_size\n    end = min(start + page_size, total_records)\n    records = [{\"id\": i + 1, \"value\": ...} for i in range(start, end)]\n    return PageResponse(page=page, records=records, has_next=page &lt; total_pages, ...)\n</code></pre> <p>Pages are fetched sequentially (next page depends on <code>has_next</code>), then records are chunked and processed in parallel via <code>.map()</code>.</p>"},{"location":"flow-reference/#cross-source-enrichment","title":"Cross-Source Enrichment","text":"<p>What it demonstrates: Joining data from three simulated API sources with graceful degradation on partial enrichment failure.</p> <p>Airflow equivalent: Cross-API enrichment (DAGs 090, 092).</p> <pre><code>@task\ndef merge_enrichments(base, demo, fin, geo) -&gt; EnrichedRecord:\n    sources_available = sum(1 for s in [demo, fin, geo] if s is not None)\n    completeness = sources_available / 3.0\n    return EnrichedRecord(..., enrichment_completeness=round(completeness, 2))\n</code></pre> <p>When an enrichment source returns <code>None</code>, the record continues with partial data. Completeness is tracked per-record and summarised in the report.</p>"},{"location":"flow-reference/#response-caching","title":"Response Caching","text":"<p>What it demonstrates: Application-level response cache with TTL expiry, hashlib-based keys, and hit/miss tracking.</p> <p>Airflow equivalent: Forecast accuracy / cached vs fresh comparison (DAG 082).</p> <pre><code>@task\ndef fetch_with_cache(endpoint, params, cache, ttl_seconds=300):\n    key = make_cache_key.fn(endpoint, params)\n    entry = check_cache.fn(cache, key, ttl_seconds)\n    if entry is not None:\n        return entry.value, True  # cache hit\n    data = simulate_api_call.fn(endpoint, params)\n    cache[key] = {\"value\": data, \"cached_at\": time.time()}\n    return data, False  # cache miss\n</code></pre> <p>Duplicate requests hit the cache. TTL-based expiry prevents stale data.</p>"},{"location":"flow-reference/#configuration-and-orchestration-patterns","title":"Configuration and Orchestration Patterns","text":""},{"location":"flow-reference/#config-driven-pipeline","title":"Config-Driven Pipeline","text":"<p>What it demonstrates: Pipeline behaviour controlled entirely by a typed <code>PipelineConfig</code> parameter: stage selection, parameter overrides, conditional execution. The flow accepts a Pydantic model directly so that Prefect can auto-generate a rich parameter schema for the UI.</p> <p>Airflow equivalent: API-triggered scheduling with config payload (DAG 109).</p> <pre><code>@flow(name=\"data_engineering_config_driven_pipeline\", log_prints=True)\ndef config_driven_pipeline_flow(config: PipelineConfig | None = None) -&gt; PipelineResult:\n    ...\n</code></pre> <p>Different <code>PipelineConfig</code> values produce different pipeline runs through the same flow. Disabled stages are skipped automatically.</p>"},{"location":"flow-reference/#producer-consumer","title":"Producer-Consumer","text":"<p>What it demonstrates: Cross-flow communication via file-based data contracts. Separate producer and consumer flows connected through data packages.</p> <p>Airflow equivalent: Asset + XCom producer/consumer (DAG 112).</p> <pre><code>@flow(name=\"data_engineering_producer_consumer\", log_prints=True)\ndef producer_consumer_flow(work_dir=None):\n    producer_flow(data_dir, producer_id=\"alpha\", records=8)\n    producer_flow(data_dir, producer_id=\"beta\", records=12)\n    results = consumer_flow(data_dir, consumer_id=\"main_consumer\")\n</code></pre> <p>Producers write JSON data + metadata files. Consumers discover and process them. Each is independently testable.</p>"},{"location":"flow-reference/#circuit-breaker","title":"Circuit Breaker","text":"<p>What it demonstrates: Circuit breaker state machine (closed -&gt; open -&gt; half_open -&gt; closed). After N consecutive failures, the circuit opens.</p> <p>Airflow equivalent: None (Prefect-native resilience pattern).</p> <pre><code>@task\ndef call_with_circuit(circuit: CircuitState, should_succeed: bool):\n    if circuit.state == \"open\":\n        circuit = circuit.model_copy(update={\"state\": \"half_open\"})\n    # ... execute call, track failures, trip if threshold reached\n</code></pre> <p>Outcomes are a deterministic list of booleans, making the simulation fully testable and reproducible.</p>"},{"location":"flow-reference/#discriminated-unions","title":"Discriminated Unions","text":"<p>What it demonstrates: Pydantic discriminated unions for type-safe polymorphic event dispatch. The flow accepts a typed <code>list[Event]</code> directly, giving Prefect a rich parameter schema instead of freeform JSON.</p> <p>Airflow equivalent: Multi-API dashboard with heterogeneous sources (DAG 098).</p> <pre><code>Event = Annotated[Union[EmailEvent, WebhookEvent, ScheduleEvent], Field(discriminator=\"event_type\")]\n\n@flow(name=\"data_engineering_discriminated_unions\", log_prints=True)\ndef discriminated_unions_flow(events: list[Event] | None = None) -&gt; ProcessingSummary:\n    ...\n</code></pre> <p>The <code>event_type</code> literal field acts as a discriminator. Passing typed model instances directly avoids manual dict parsing inside the flow.</p>"},{"location":"flow-reference/#production-patterns-and-capstone","title":"Production Patterns and Capstone","text":""},{"location":"flow-reference/#streaming-batch-processor","title":"Streaming Batch Processor","text":"<p>What it demonstrates: Windowed batch processing with anomaly detection (values &gt; 3 stdev from window mean) and trend analysis between windows.</p> <p>Airflow equivalent: GeoJSON parsing, OData pivoting (DAGs 091, 095).</p> <pre><code>@task\ndef process_window(data: list[dict], window: BatchWindow) -&gt; WindowResult:\n    values = [r[\"value\"] for r in data[window.start_index:window.end_index]]\n    mean = statistics.mean(values)\n    stdev = statistics.stdev(values)\n    anomalies = [r for r in records if abs(r[\"value\"] - mean) &gt; 3 * stdev]\n</code></pre> <p>Windows are processed in parallel via <code>.map()</code>. Seeded random ensures reproducible test data.</p>"},{"location":"flow-reference/#idempotent-operations","title":"Idempotent Operations","text":"<p>What it demonstrates: Hash-based idempotency registry. Operations check the registry before executing, making them safe to re-run.</p> <p>Airflow equivalent: None (production resilience pattern).</p> <pre><code>@task\ndef idempotent_execute(registry, name, inputs):\n    op_id = compute_operation_id.fn(name, inputs)\n    existing = check_registry.fn(registry, op_id)\n    if existing is not None:\n        return registry, existing.result, True  # skipped\n    result = execute_operation.fn(name, inputs)\n    registry = register_operation.fn(registry, op_id, name, op_id, result)\n    return registry, result, False  # executed\n</code></pre> <p>Duplicate operations are detected by hashing name + inputs. The registry prevents re-execution.</p>"},{"location":"flow-reference/#error-recovery","title":"Error Recovery","text":"<p>What it demonstrates: Checkpoint-based stage recovery. The flow saves progress after each stage; re-runs skip completed stages.</p> <p>Airflow equivalent: None (production resilience combining manifest and checkpoint ideas).</p> <pre><code>@task\ndef run_with_checkpoints(stages, store_path, fail_on=None):\n    store = load_checkpoints.fn(store_path)\n    for stage in stages:\n        if not should_run_stage.fn(store, stage):\n            recovered += 1\n            continue\n        result = execute_stage.fn(stage, context, fail_on)\n        store = save_checkpoint.fn(store, stage, \"completed\", result, store_path)\n</code></pre> <p>Fail at stage X, re-run, and stages before X are automatically skipped.</p>"},{"location":"flow-reference/#production-pipeline-v3","title":"Production Pipeline v3","text":"<p>What it demonstrates: Phase 4 capstone combining file I/O, data profiling, quality rules, enrichment with caching, deduplication, and checkpointing.</p> <p>Airflow equivalent: Quality framework + dashboard capstone (DAGs 099, 098).</p> <pre><code>@flow(name=\"data_engineering_production_pipeline_v3\", log_prints=True)\ndef production_pipeline_v3_flow(work_dir=None):\n    records = ingest_csv(input_path)\n    profile = profile_data(records)\n    quality = run_quality_checks(records, rules)\n    enriched, cache_stats = enrich_records(records, cache)\n    deduped = deduplicate_records(enriched, [\"id\", \"name\"])\n    write_output(deduped, output_path)\n    build_dashboard(result)\n</code></pre> <p>This capstone combines all Phase 4 patterns: CSV file I/O, statistical profiling, quality rule checks with traffic-light scoring, application-level caching, hash-based deduplication, checkpoint saving, and a markdown dashboard artifact.</p>"},{"location":"flow-reference/#environmental-and-risk-analysis","title":"Environmental and Risk Analysis","text":""},{"location":"flow-reference/#air-quality-index","title":"Air Quality Index","text":"<p>What it demonstrates: Threshold-based AQI classification against WHO air quality standards, health advisory generation, and severity ordering.</p> <p>Airflow equivalent: Air quality monitoring with WHO thresholds (DAG 083).</p> <pre><code>AQI_THRESHOLDS: list[tuple[float, str, str]] = [\n    (50.0, \"Good\", \"green\"),\n    (100.0, \"Moderate\", \"yellow\"),\n    (150.0, \"Unhealthy for Sensitive Groups\", \"orange\"),\n    (200.0, \"Unhealthy\", \"red\"),\n    (300.0, \"Very Unhealthy\", \"purple\"),\n    (float(\"inf\"), \"Hazardous\", \"maroon\"),\n]\n\n@task\ndef classify_aqi(readings: list[PollutantReading]) -&gt; list[AqiClassification]:\n    classifications: list[AqiClassification] = []\n    for reading in readings:\n        mean_val = sum(reading.hourly_values) / len(reading.hourly_values)\n        for threshold, cat, col in AQI_THRESHOLDS:\n            if mean_val &lt;= threshold:\n                category = cat\n                color = col\n                break\n        ...\n</code></pre> <p>Readings are classified by walking an ordered list of (threshold, category, color) tuples. A severity ordering list determines the worst city. WHO limits are checked separately for exceedance counting.</p>"},{"location":"flow-reference/#composite-risk-assessment","title":"Composite Risk Assessment","text":"<p>What it demonstrates: Multi-source weighted risk scoring combining marine and flood data into a composite index.</p> <p>Airflow equivalent: Marine forecast + flood discharge composite risk (DAG 084).</p> <pre><code>@task\ndef compute_composite(\n    location: str,\n    marine_factors: list[RiskFactor],\n    flood_factors: list[RiskFactor],\n    marine_weight: float,\n    flood_weight: float,\n) -&gt; CompositeRisk:\n    marine_avg = sum(f.normalized_score for f in marine_factors) / len(marine_factors)\n    flood_avg = sum(f.normalized_score for f in flood_factors) / len(flood_factors)\n    weighted = marine_avg * marine_weight + flood_avg * flood_weight\n    category = _classify_risk(weighted)\n    ...\n</code></pre> <p>Raw risk factors are normalized to a 0-100 scale, then combined with configurable weights (default 60/40 marine/flood). The composite score is classified into low/moderate/high/critical categories.</p>"},{"location":"flow-reference/#daylight-analysis","title":"Daylight Analysis","text":"<p>What it demonstrates: Datetime arithmetic for seasonal daylight profiles and Pearson correlation between latitude and daylight amplitude.</p> <p>Airflow equivalent: Sunrise-sunset daylight analysis across latitudes (DAG 085).</p> <pre><code>@task\ndef correlate_latitude_amplitude(profiles: list[SeasonalProfile]) -&gt; float:\n    x = [abs(p.latitude) for p in profiles]\n    y = [p.amplitude for p in profiles]\n    mean_x = statistics.mean(x)\n    mean_y = statistics.mean(y)\n    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y, strict=True))\n    denom_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n    denom_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n    return numerator / (denom_x * denom_y)\n</code></pre> <p>Day length is computed using a sinusoidal formula with amplitude proportional to latitude. The manual Pearson correlation confirms the expected strong positive relationship between absolute latitude and seasonal daylight variation.</p>"},{"location":"flow-reference/#statistical-aggregation","title":"Statistical Aggregation","text":"<p>What it demonstrates: Fan-out aggregation: one dataset, three independent aggregations (by station, by date, cross-tabulation) running in parallel.</p> <p>Airflow equivalent: Parquet-style aggregation with groupby and cross-tab (DAG 057).</p> <pre><code># Fan-out: 3 independent aggregations\nstation_future = aggregate_by_group.submit(records, \"station\", \"temperature\")\ndate_future = aggregate_by_group.submit(records, \"day\", \"temperature\")\ncross_tab_future = build_cross_tab.submit(records, \"station\", \"day\", \"temperature\")\n\nstation_agg = station_future.result()\ndate_agg = date_future.result()\ncross_tab = cross_tab_future.result()\n</code></pre> <p><code>.submit()</code> launches the three aggregations concurrently. The cross-tabulation builds a station-by-day matrix of mean temperatures using <code>statistics.mean()</code>.</p>"},{"location":"flow-reference/#economic-and-demographic-analysis","title":"Economic and Demographic Analysis","text":""},{"location":"flow-reference/#demographic-analysis","title":"Demographic Analysis","text":"<p>What it demonstrates: Nested JSON normalization into relational tables, bridge tables for multi-valued fields, and border graph edge construction.</p> <p>Airflow equivalent: Country demographics with nested JSON normalization (DAG 087).</p> <pre><code>@task\ndef build_bridge_table(countries: list[Country], field: str) -&gt; list[BridgeRecord]:\n    records: list[BridgeRecord] = []\n    for c in countries:\n        mapping = getattr(c, field)\n        for key, value in mapping.items():\n            records.append(BridgeRecord(country=c.name, key=key, value=value))\n    return records\n</code></pre> <p>Nested JSON fields (languages, currencies) are exploded into bridge tables linking country to key-value pairs. Border relationships become directed graph edges. Countries are ranked by population and density.</p>"},{"location":"flow-reference/#multi-indicator-correlation","title":"Multi-Indicator Correlation","text":"<p>What it demonstrates: Multi-indicator join on (country, year), forward-fill for missing values, and pairwise Pearson correlation matrix.</p> <p>Airflow equivalent: World Bank multi-indicator analysis with correlation (DAG 088).</p> <pre><code>def _pearson(x: list[float], y: list[float]) -&gt; float:\n    mean_x = statistics.mean(x)\n    mean_y = statistics.mean(y)\n    num = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y, strict=True))\n    den_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n    den_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n    return num / (den_x * den_y)\n</code></pre> <p>Three indicator series (GDP, CO2, Renewable) are joined on country+year, forward-filled within each country, and then tested for pairwise correlation. Year-over-year growth rates are also computed.</p>"},{"location":"flow-reference/#financial-time-series","title":"Financial Time Series","text":"<p>What it demonstrates: Log returns, rolling window volatility (annualized), cross-currency correlation matrix, and anomaly detection via z-score.</p> <p>Airflow equivalent: Currency analysis with log returns and volatility (DAG 089).</p> <pre><code>@task\ndef compute_log_returns(records: list[RateRecord]) -&gt; dict[str, list[LogReturn]]:\n    ...\n    for i in range(1, len(sorted_rates)):\n        log_ret = math.log(sorted_rates[i].rate / sorted_rates[i - 1].rate)\n        currency_returns.append(LogReturn(day=sorted_rates[i].day, return_value=round(log_ret, 8)))\n    ...\n</code></pre> <p>Log returns are the natural logarithm of consecutive price ratios. Rolling volatility is the standard deviation of returns over a window, annualized by multiplying by sqrt(252). Anomalies are returns exceeding 2 standard deviations.</p>"},{"location":"flow-reference/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>What it demonstrates: Educational null-hypothesis pattern: align seismic and weather datasets, test for correlation, and interpret the (expected near-zero) result.</p> <p>Airflow equivalent: Earthquake-weather correlation / null hypothesis (DAG 093).</p> <pre><code>@task\ndef check_correlation(observations: list[DailyObservation], hypothesis: str) -&gt; HypothesisResult:\n    x = [o.metric_a for o in observations]\n    y = [o.metric_b for o in observations]\n    r_val = _pearson(x, y)\n    interpretation = _interpret_result(r_val, len(observations))\n    is_significant = abs(r_val) &gt; 0.3\n    ...\n</code></pre> <p>Two hypotheses are tested (earthquakes vs temperature, earthquakes vs pressure). Both produce near-zero correlations, confirming the null hypothesis. The absence of correlation is itself a valid finding.</p>"},{"location":"flow-reference/#advanced-analytics","title":"Advanced Analytics","text":""},{"location":"flow-reference/#regression-analysis","title":"Regression Analysis","text":"<p>What it demonstrates: Manual OLS linear regression with log transformation, R-squared computation, and residual-based efficiency ranking.</p> <p>Airflow equivalent: Health expenditure log-linear regression (DAG 096).</p> <pre><code>@task\ndef linear_regression(x: list[float], y: list[float]) -&gt; RegressionResult:\n    mean_x = statistics.mean(x)\n    mean_y = statistics.mean(y)\n    cov_xy = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y, strict=True)) / n\n    var_x = sum((xi - mean_x) ** 2 for xi in x) / n\n    slope = cov_xy / var_x if var_x &gt; 0 else 0.0\n    intercept = mean_y - slope * mean_x\n    ss_res = sum((yi - (slope * xi + intercept)) ** 2 for xi, yi in zip(x, y, strict=True))\n    ss_tot = sum((yi - mean_y) ** 2 for yi in y)\n    r_squared = 1.0 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0.0\n    ...\n</code></pre> <p>Health spending is log-transformed before regression against mortality. Countries are ranked by residual: negative residual means lower mortality than predicted (more efficient). No numpy or scipy required.</p>"},{"location":"flow-reference/#star-schema","title":"Star Schema","text":"<p>What it demonstrates: Dimensional modeling with fact and dimension tables, surrogate keys, min-max normalization, and weighted composite index ranking.</p> <p>Airflow equivalent: Health profile dimensional model (DAG 097).</p> <pre><code>@task\ndef build_country_dimension(data: list[dict]) -&gt; list[DimCountry]:\n    dims: list[DimCountry] = []\n    for i, d in enumerate(data, 1):\n        dims.append(DimCountry(key=i, name=d[\"name\"], region=d[\"region\"], population=d[\"population\"]))\n    return dims\n</code></pre> <p>Three dimension tables (country, time, indicator) and a fact table form a star schema. Indicators are min-max normalized per the <code>higher_is_better</code> flag, then weighted to produce a composite country ranking.</p>"},{"location":"flow-reference/#staged-etl-pipeline","title":"Staged ETL Pipeline","text":"<p>What it demonstrates: Three-layer ETL pipeline: staging (raw load with timestamp), production (validated + transformed), and summary (grouped stats).</p> <p>Airflow equivalent: SQL-based three-layer ETL (DAGs 035-036).</p> <pre><code>@task\ndef validate_and_transform(staging: list[StagingRecord]) -&gt; list[ProductionRecord]:\n    for s in staging:\n        is_valid = True\n        try:\n            value = float(s.value_raw)\n            if value &lt; 0 or value &gt; 1000:\n                is_valid = False\n        except (ValueError, TypeError):\n            is_valid = False\n        ...\n</code></pre> <p>Each record carries an <code>is_valid</code> flag through the production layer. Invalid records are retained but flagged. The summary layer computes grouped statistics (avg, min, max, count) over valid records only.</p>"},{"location":"flow-reference/#data-transfer","title":"Data Transfer","text":"<p>What it demonstrates: Cross-system data synchronization with computed categorical columns and transfer verification via row count and checksum.</p> <p>Airflow equivalent: Generic table-to-table transfer with transformation (DAG 037).</p> <pre><code>@task\ndef verify_transfer(sources: list[SourceRecord], destinations: list[DestRecord]) -&gt; TransferVerification:\n    count_match = len(sources) == len(destinations)\n    source_hash = hashlib.sha256(\n        \"|\".join(f\"{s.city}:{s.population}\" for s in sorted(sources, key=lambda x: x.city)).encode()\n    ).hexdigest()[:16]\n    dest_hash = hashlib.sha256(\n        \"|\".join(f\"{d.city}:{d.population}\" for d in sorted(destinations, key=lambda x: x.city)).encode()\n    ).hexdigest()[:16]\n    return TransferVerification(count_match=count_match, checksum_match=source_hash == dest_hash)\n</code></pre> <p>Source records are enriched with a <code>size_category</code> during transfer. Verification checks both row counts and SHA-256 checksums to ensure data integrity.</p>"},{"location":"flow-reference/#domain-api-processing","title":"Domain API Processing","text":""},{"location":"flow-reference/#hierarchical-data-processing","title":"Hierarchical Data Processing","text":"<p>What it demonstrates: Tree-structured org unit hierarchy with path-based depth computation, parent field flattening, and root/leaf identification.</p> <p>Airflow equivalent: DHIS2 org unit hierarchy flattening (DAG 058).</p> <pre><code>@task\ndef flatten_hierarchy(raw: list[RawOrgUnit]) -&gt; list[OrgUnit]:\n    units: list[OrgUnit] = []\n    for r in raw:\n        parent_id = r.parent.id if r.parent else \"\"\n        parent_name = r.parent.name if r.parent else \"\"\n        depth = len([s for s in r.path.split(\"/\") if s])\n        units.append(OrgUnit(id=r.id, name=r.name, level=r.level,\n                             parent_id=parent_id, parent_name=parent_name,\n                             path=r.path, hierarchy_depth=depth, ...))\n    ...\n</code></pre> <p>Nested parent references are flattened to <code>parent_id</code> and <code>parent_name</code> columns. Hierarchy depth is derived from the path string. Root and leaf nodes are identified by comparing parent and child ID sets.</p>"},{"location":"flow-reference/#expression-complexity-scoring","title":"Expression Complexity Scoring","text":"<p>What it demonstrates: Regex-based expression parsing for operand counting, operator counting, complexity scoring, and binning.</p> <p>Airflow equivalent: DHIS2 indicator expression parsing (DAGs 059-060).</p> <pre><code>OPERAND_PATTERN = re.compile(r\"#\\{[^}]+\\}\")\n\n@task\ndef score_complexity(expressions: list[Expression]) -&gt; list[ComplexityScore]:\n    for expr in expressions:\n        num_operands = parse_operands.fn(expr.numerator) + parse_operands.fn(expr.denominator)\n        num_operators = count_operators.fn(expr.numerator) + count_operators.fn(expr.denominator)\n        total = num_operands + num_operators\n        if total &lt;= 2:\n            bin_label = \"trivial\"\n        elif total &lt;= 4:\n            bin_label = \"simple\"\n        elif total &lt;= 8:\n            bin_label = \"moderate\"\n        else:\n            bin_label = \"complex\"\n        ...\n</code></pre> <p>Expressions use <code>#{...}</code> operand syntax. The regex counts operands while a character scan counts operators. The combined score determines a complexity bin (trivial/simple/moderate/complex).</p>"},{"location":"flow-reference/#spatial-data-construction","title":"Spatial Data Construction","text":"<p>What it demonstrates: Manual GeoJSON-like feature collection construction, bounding box computation from point geometries, and geometry type filtering.</p> <p>Airflow equivalent: DHIS2 org unit geometry / GeoJSON construction (DAG 061).</p> <pre><code>@task\ndef compute_bounding_box(features: list[Feature]) -&gt; list[float]:\n    lons: list[float] = []\n    lats: list[float] = []\n    for f in features:\n        if f.geometry_type == \"Point\":\n            lons.append(f.coordinates[0])\n            lats.append(f.coordinates[1])\n    return [min(lons), min(lats), max(lons), max(lats)]\n</code></pre> <p>Features with Point and Polygon geometry types are assembled into a collection. The bounding box is computed from point coordinates as [min_lon, min_lat, max_lon, max_lat].</p>"},{"location":"flow-reference/#parallel-multi-endpoint-export","title":"Parallel Multi-Endpoint Export","text":"<p>What it demonstrates: Parallel independent endpoint processing with heterogeneous output formats (CSV + JSON) and fan-in summary.</p> <p>Airflow equivalent: DHIS2 combined parallel export (DAG 062).</p> <pre><code># Fan-out: 3 parallel endpoint fetches\nfuture_a = fetch_endpoint_a.submit(output_dir)\nfuture_b = fetch_endpoint_b.submit(output_dir)\nfuture_c = fetch_endpoint_c.submit(output_dir)\n\nresult_a = future_a.result()\nresult_b = future_b.result()\nresult_c = future_c.result()\n\nsummary = combine_results([result_a, result_b, result_c], duration)\n</code></pre> <p>Three endpoints are fetched in parallel via <code>.submit()</code>, writing CSV and JSON files. Results are combined into a summary with total record count and format distribution.</p>"},{"location":"flow-reference/#advanced-patterns-and-grand-capstone","title":"Advanced Patterns and Grand Capstone","text":""},{"location":"flow-reference/#data-lineage-tracking","title":"Data Lineage Tracking","text":"<p>What it demonstrates: Hash-based provenance tracking through pipeline stages, building a lineage graph from ingest through filter, enrich, and dedup.</p> <p>Airflow equivalent: None (Prefect-native pattern).</p> <pre><code>@task\ndef compute_data_hash(records: list[DataRecord]) -&gt; str:\n    raw = str(sorted(str(r.model_dump()) for r in records))\n    return hashlib.sha256(raw.encode()).hexdigest()[:16]\n\n@task\ndef transform_filter(records, min_value):\n    input_hash = compute_data_hash.fn(records)\n    filtered = [r for r in records if r.value &gt;= min_value]\n    output_hash = compute_data_hash.fn(filtered)\n    entry = record_lineage.fn(\"transform\", \"filter_by_value\", input_hash, output_hash, len(filtered))\n    return filtered, entry\n</code></pre> <p>Every transformation records its input and output hashes, creating a chain of lineage entries. The lineage graph tracks how data changes through each stage and counts data-modifying operations (where input_hash differs from output_hash).</p>"},{"location":"flow-reference/#pipeline-template-factory","title":"Pipeline Template Factory","text":"<p>What it demonstrates: Reusable pipeline templates with ordered stage slots, instantiated with different configurations via the factory pattern.</p> <p>Airflow equivalent: None (Prefect-native pattern).</p> <pre><code>@task\ndef execute_stage(stage: StageTemplate, overrides: dict) -&gt; StageResult:\n    merged = {**stage.default_params, **overrides}\n    if stage.stage_type == \"extract\":\n        count = int(merged.get(\"batch_size\", 100))\n    elif stage.stage_type == \"validate\":\n        count = int(int(merged.get(\"batch_size\", 100)) * float(merged.get(\"pass_rate\", 0.9)))\n    ...\n</code></pre> <p>Templates define ordered stages with default parameters. Instantiation merges overrides into defaults. The same template produces different pipelines depending on the configuration, demonstrating code reuse without duplication.</p>"},{"location":"flow-reference/#multi-pipeline-orchestrator","title":"Multi-Pipeline Orchestrator","text":"<p>What it demonstrates: Orchestration of multiple independent mini-pipelines with status collection and overall health rollup.</p> <p>Airflow equivalent: None (Prefect-native pattern).</p> <pre><code>@task\ndef aggregate_status(statuses: list[PipelineStatus]) -&gt; OrchestratorResult:\n    successful = sum(1 for s in statuses if s.status == \"success\")\n    failed = len(statuses) - successful\n    if failed == 0:\n        overall = \"healthy\"\n    elif failed &lt; len(statuses) / 2:\n        overall = \"degraded\"\n    else:\n        overall = \"critical\"\n    ...\n</code></pre> <p>Four independent pipelines (ingest, transform, export, quality) run and report status. The orchestrator aggregates results into healthy/degraded/critical based on majority voting and produces a markdown report.</p>"},{"location":"flow-reference/#grand-capstone","title":"Grand Capstone","text":"<p>What it demonstrates: End-to-end analytics pipeline combining patterns from all 5 phases: CSV I/O, profiling, quality checks, enrichment, deduplication, regression, dimensional modeling, lineage tracking, and a dashboard artifact.</p> <p>Airflow equivalent: None (combines patterns from all 5 phases).</p> <pre><code>@flow(name=\"analytics_grand_capstone\", log_prints=True)\ndef grand_capstone_flow(work_dir: str | None = None) -&gt; CapstoneResult:\n    records = ingest_data(input_path)\n    profile_data(records)\n    quality = run_quality_checks(records)\n    cleaned = enrich_and_deduplicate(records)\n    regression = run_regression(cleaned, \"value\", \"score\")\n    dimensions = build_dimensions(cleaned)\n    lineage = track_lineage(stages)\n    build_capstone_dashboard(result)\n    ...\n</code></pre> <p>This final flow ties together every major concept from Phase 1 through Phase 5: file ingestion, statistical profiling, quality rule checking, hash-based deduplication, OLS regression, dimensional modeling with composite ranking, lineage tracking, and a rich markdown dashboard artifact.</p>"},{"location":"flow-reference/#dhis2-integration-101-108","title":"DHIS2 Integration (101--108)","text":""},{"location":"flow-reference/#dhis2-connection-block","title":"DHIS2 Connection Block","text":"<p>What it demonstrates: Custom Prefect block with methods for DHIS2 API operations, <code>SecretStr</code> for password management, connection verification.</p> <p>Airflow equivalent: <code>BaseHook.get_connection(\"dhis2_default\")</code> (DAG 110).</p> <pre><code>class Dhis2Credentials(Block):\n    base_url: str = \"https://play.im.dhis2.org/dev\"\n    username: str = \"admin\"\n    password: SecretStr = Field(default=SecretStr(\"district\"))\n\n    def get_client(self) -&gt; Dhis2Client: ...\n\nclass Dhis2Client:\n    def get_server_info(self) -&gt; dict: ...\n    def fetch_metadata(self, endpoint: str) -&gt; list[dict]: ...\n\ncreds = get_dhis2_credentials()       # Block.load() with fallback\nclient = creds.get_client()\ninfo = get_connection_info(creds)     # SecretStr handles masking\nverify_connection(client, creds.base_url)  # uses client.get_server_info()\n</code></pre> <p>The Airflow pattern <code>BaseHook.get_connection()</code> maps to <code>Block.load()</code> in Prefect. Password is stored as <code>SecretStr</code> directly on the block (encrypted at rest when saved to the server). <code>get_client()</code> returns a <code>Dhis2Client</code> that handles API calls -- callers never touch the password directly.</p>"},{"location":"flow-reference/#dhis2-block-connection","title":"DHIS2 Block Connection","text":"<p>What it demonstrates: Named credentials block loading so different DHIS2 instances (play, staging, production) can be targeted at runtime via a single <code>instance</code> parameter.</p> <p>Airflow equivalent: Multiple connections by conn_id.</p> <pre><code>@flow(name=\"dhis2_block_connection\", log_prints=True)\ndef dhis2_block_connection_flow(instance: str = \"dhis2\") -&gt; ConnectionInfo:\n    creds = get_dhis2_credentials(instance)  # loads named block\n    client = creds.get_client()\n    info = get_connection_info(creds, instance)\n    verify_connection(client, creds.base_url)\n    count = fetch_org_unit_count(client)\n    display_connection(info, count)\n</code></pre> <p>Save multiple <code>Dhis2Credentials</code> blocks (e.g. <code>\"dhis2\"</code>, <code>\"dhis2-staging\"</code>, <code>\"dhis2-prod\"</code>) and pass the block name as the <code>instance</code> parameter. The flow loads whichever block you specify, falling back to default play-server credentials when no saved block exists.</p>"},{"location":"flow-reference/#dhis2-org-units-api","title":"DHIS2 Org Units API","text":"<p>What it demonstrates: Block-authenticated metadata fetch, nested JSON flattening with Pydantic, derived columns (hierarchy depth, translation count).</p> <p>Airflow equivalent: DHIS2 org unit fetch (DAG 058).</p> <pre><code>@task\ndef flatten_org_units(raw: list[RawOrgUnit]) -&gt; list[FlatOrgUnit]:\n    for r in raw:\n        parent_id = r.parent[\"id\"] if r.parent else \"\"\n        depth = len([s for s in r.path.split(\"/\") if s])\n        flat.append(FlatOrgUnit(id=r.id, level=r.level,\n                                parent_id=parent_id, hierarchy_depth=depth, ...))\n</code></pre> <p>Nested DHIS2 JSON (parent.id, createdBy.username) is extracted into flat columns. Hierarchy depth is computed from path segment count.</p>"},{"location":"flow-reference/#dhis2-data-elements-api","title":"DHIS2 Data Elements API","text":"<p>What it demonstrates: Metadata categorization with block auth, boolean derived columns, valueType/aggregationType grouping.</p> <p>Airflow equivalent: DHIS2 data element fetch (DAG 059).</p> <pre><code>@task\ndef flatten_data_elements(raw: list[RawDataElement]) -&gt; list[FlatDataElement]:\n    cc_id = r.categoryCombo[\"id\"] if r.categoryCombo else \"\"\n    flat.append(FlatDataElement(\n        category_combo_id=cc_id,\n        has_code=r.code is not None,\n        name_length=len(r.name), ...))\n</code></pre> <p>Each data element is categorized by value type and aggregation type. Boolean <code>has_code</code> and numeric <code>name_length</code> are derived columns.</p>"},{"location":"flow-reference/#dhis2-indicators-api","title":"DHIS2 Indicators API","text":"<p>What it demonstrates: Expression parsing with regex, operand counting, complexity scoring and binning.</p> <p>Airflow equivalent: DHIS2 indicator fetch (DAG 060).</p> <pre><code>OPERAND_PATTERN = re.compile(r\"#\\{[^}]+\\}\")\n\nnum_ops = len(OPERAND_PATTERN.findall(expression))\nnum_operators = sum(1 for c in expression if c in \"+-*/\")\ncomplexity = num_ops + den_ops + num_operators\ncomplexity_bin = \"trivial\" if complexity &lt;= 1 else ...\n</code></pre> <p>DHIS2 indicator expressions use <code>#{uid.uid}</code> operands. The regex counts them, operators are counted separately, and a combined score is binned into trivial/simple/moderate/complex.</p>"},{"location":"flow-reference/#dhis2-org-unit-geometry","title":"DHIS2 Org Unit Geometry","text":"<p>What it demonstrates: GeoJSON construction with block auth, geometry filtering, bounding box computation.</p> <p>Airflow equivalent: DHIS2 org unit geometry export (DAG 061).</p> <pre><code>@task\ndef build_collection(features: list[GeoFeature]) -&gt; GeoCollection:\n    all_points = [_extract_coords(f.geometry) for f in features]\n    bbox = [min(lons), min(lats), max(lons), max(lats)]\n    return GeoCollection(features=features, bbox=bbox)\n</code></pre> <p>Org units with geometry are converted to GeoJSON Features. A FeatureCollection with bounding box is built and written to <code>.geojson</code>.</p>"},{"location":"flow-reference/#dhis2-combined-export","title":"DHIS2 Combined Export","text":"<p>What it demonstrates: Parallel multi-endpoint fetch with shared block, fan-in summary across heterogeneous outputs.</p> <p>Airflow equivalent: DHIS2 combined parallel export (DAG 062).</p> <pre><code>client = get_dhis2_credentials().get_client()\nfuture_ou = export_org_units.submit(client, output_dir)\nfuture_de = export_data_elements.submit(client, output_dir)\nfuture_ind = export_indicators.submit(client, output_dir)\n\nreport = combined_report([future_ou.result(), future_de.result(),\n                          future_ind.result()])\n</code></pre> <p>A shared <code>Dhis2Client</code> is created once from the credentials block, then passed to three parallel <code>.submit()</code> calls. The combined report tracks total records and format distribution (CSV vs JSON).</p>"},{"location":"flow-reference/#dhis2-analytics-query","title":"DHIS2 Analytics Query","text":"<p>What it demonstrates: Analytics API with dimension parameters, headers+rows response parsing, query parameter construction.</p> <p>Airflow equivalent: DHIS2 data values / analytics (DAG 111).</p> <pre><code>@task\ndef parse_analytics(response: RawAnalyticsResponse) -&gt; list[AnalyticsRow]:\n    header_names = [h[\"name\"] for h in response.headers]\n    for row_data in response.rows:\n        record = dict(zip(header_names, row_data, strict=True))\n        rows.append(AnalyticsRow(dx=record[\"dx\"], ou=record[\"ou\"], ...))\n</code></pre> <p>The DHIS2 analytics API returns headers and rows separately. This flow builds dimension queries, fetches results, and parses the tabular response into typed records.</p>"},{"location":"flow-reference/#dhis2-full-pipeline","title":"DHIS2 Full Pipeline","text":"<p>What it demonstrates: End-to-end DHIS2 pipeline with block config, quality checks, timing, and markdown dashboard.</p> <p>Airflow equivalent: None (capstone combining all DHIS2 patterns).</p> <pre><code>@flow(name=\"dhis2_pipeline\", log_prints=True)\ndef dhis2_pipeline_flow() -&gt; Dhis2PipelineResult:\n    creds = get_dhis2_credentials()\n    client = creds.get_client()\n    connect_and_verify(client, creds.base_url)\n    metadata = fetch_all_metadata(client)\n    quality = validate_metadata(metadata)\n    build_dashboard(result)\n</code></pre> <p>The credentials block provides a <code>Dhis2Client</code> used throughout the pipeline. A three-stage pipeline (connect, fetch, validate) with per-stage timing, quality scoring, and a markdown dashboard artifact. This is the DHIS2 capstone.</p>"},{"location":"flow-reference/#dhis2-world-bank-population-import","title":"DHIS2 World Bank Population Import","text":"<p>What it demonstrates: First write flow -- fetches World Bank population data (SP.POP.TOTL) and imports it into a DHIS2 data element via <code>POST /api/dataValueSets</code>.</p> <p>Airflow equivalent: PythonOperator chain with World Bank fetch + DHIS2 POST.</p> <pre><code>@flow(name=\"dhis2_worldbank_population_import\", log_prints=True)\ndef dhis2_worldbank_population_import_flow(query: ImportQuery | None = None) -&gt; ImportResult:\n    client = get_dhis2_credentials().get_client()\n    populations = fetch_population_data(query.iso3_codes, query.start_year, query.end_year)\n    org_unit_map = resolve_org_units(client, query.iso3_codes)\n    data_values = build_data_values(populations, org_unit_map, query.data_element_uid)\n    result = import_data_values(client, data_values)\n    create_markdown_artifact(key=\"dhis2-worldbank-import\", markdown=result.markdown)\n</code></pre> <p>Four tasks form a pipeline: (1) batch-fetch population from the World Bank API, (2) resolve ISO3 codes to DHIS2 org unit UIDs, (3) build data values matching the DHIS2 <code>dataValueSets</code> schema, (4) POST and parse the import summary. Unresolved ISO3 codes are logged as warnings and skipped. Retry-enabled tasks handle transient API failures on both the World Bank and DHIS2 sides.</p>"},{"location":"flow-reference/#dhis2-world-bank-health-indicators-import","title":"DHIS2 World Bank Health Indicators Import","text":"<p>What it demonstrates: Fetches 10 World Bank health indicators and imports them into DHIS2 as data values across 10 data elements within a single data set.</p> <p>Airflow equivalent: PythonOperator chain with World Bank fetch + DHIS2 POST.</p> <pre><code>@flow(name=\"dhis2_worldbank_health_import\", log_prints=True)\ndef dhis2_worldbank_health_import_flow(query: HealthQuery | None = None) -&gt; ImportResult:\n    client = get_dhis2_credentials().get_client()\n    org_unit = ensure_dhis2_metadata(client)\n    for indicator in INDICATORS:\n        all_values.extend(fetch_wb_data(indicator, ...))\n    data_values = build_data_values(org_unit, all_values)\n    result = import_to_dhis2(client, dhis2_url, org_unit, data_values)\n    create_markdown_artifact(key=\"dhis2-worldbank-health-import\", markdown=result.markdown)\n</code></pre> <p>Four tasks form a pipeline: (1) ensure 10 data elements and 1 data set exist in DHIS2, (2) fetch each indicator from the World Bank API in a loop, (3) build data values mapping indicator results to DHIS2 data element UIDs, (4) POST all values in a single import. Covers indicators including under-5 mortality, life expectancy, health expenditure, TB incidence, measles immunization, stunting, and more.</p>"},{"location":"flow-reference/#dhis2-worldpop-population-import","title":"DHIS2 WorldPop Population Import","text":"<p>What it demonstrates: Reads org unit polygon boundaries from DHIS2, queries the WorldPop age-sex API for gridded population estimates, and writes sex-disaggregated results back using DHIS2 category combinations.</p> <p>Airflow equivalent: PythonOperator chain with geometry fetch + WorldPop query + DHIS2 POST.</p> <pre><code>@flow(name=\"dhis2_worldpop_population_import\", log_prints=True)\ndef dhis2_worldpop_population_import_flow(query: ImportQuery | None = None) -&gt; ImportResult:\n    client = get_dhis2_credentials().get_client()\n    org_units, coc_mapping = ensure_dhis2_metadata(client)\n    for ou in org_units:\n        results.append(fetch_worldpop_population(ou, query.year))\n    data_values = build_data_values(results, query.year, coc_mapping)\n    result = import_to_dhis2(client, dhis2_url, org_units, data_values)\n    create_markdown_artifact(key=\"dhis2-worldpop-population-import\", markdown=result.markdown)\n</code></pre> <p>Four tasks form a pipeline: (1) ensure category options, category, category combo, data element, and data set exist in DHIS2 -- then resolve auto-generated categoryOptionCombo UIDs, (2) query the WorldPop <code>wpgpas</code> API for each org unit polygon, summing M_0..M_16 for male and F_0..F_16 for female totals, (3) build data values with categoryOptionCombo disaggregation (2 per org unit), (4) POST all values in a single import. Org units with Point geometry are skipped.</p>"},{"location":"flow-reference/#connection-patterns","title":"Connection Patterns","text":""},{"location":"flow-reference/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>What it demonstrates: Four configuration strategies: inline blocks, Secret blocks, environment variables, JSON config. Compares all approaches in a single flow.</p> <p>Airflow equivalent: None (Prefect-native pattern).</p> <pre><code>sources.append(from_inline_block())    # Dhis2Credentials()\nsources.append(from_secret_block())    # Secret.load()\nsources.append(from_env_var(\"KEY\"))    # os.environ.get()\nsources.append(from_json_config())     # JSON.load()\nreport = compare_strategies(sources)\n</code></pre> <p>Each strategy is loaded with graceful fallback. The comparison report shows which strategies are available in the current environment.</p>"},{"location":"flow-reference/#authenticated-api-pipeline","title":"Authenticated API Pipeline","text":"<p>What it demonstrates: Reusable pattern for any authenticated API: API key, bearer token, and basic auth. Generic API client block with pluggable auth.</p> <p>Airflow equivalent: None (general pattern).</p> <pre><code>class ApiAuthConfig(Block):\n    auth_type: str  # \"api_key\", \"bearer\", \"basic\"\n    base_url: str\n\n@task\ndef build_auth_header(config: ApiAuthConfig, credentials: str) -&gt; AuthHeader:\n    if config.auth_type == \"api_key\":\n        return AuthHeader(header_name=\"X-API-Key\", ...)\n    elif config.auth_type == \"bearer\":\n        return AuthHeader(header_name=\"Authorization\", header_value=f\"Bearer ...\")\n    elif config.auth_type == \"basic\":\n        encoded = base64.b64encode(credentials.encode()).decode()\n        return AuthHeader(header_name=\"Authorization\", header_value=f\"Basic ...\")\n</code></pre> <p>Three authentication strategies are tested against a simulated API. The block pattern is reusable for any authenticated HTTP service.</p>"},{"location":"flow-reference/#cloud-storage","title":"Cloud Storage","text":""},{"location":"flow-reference/#s3-parquet-export","title":"S3 Parquet Export","text":"<p>What it demonstrates: Generate sample data as Pydantic models, transform with pandas, and write parquet to S3-compatible storage (RustFS/MinIO) using prefect-aws blocks.</p> <p>Airflow equivalent: PythonOperator + S3Hook.upload_file().</p> <pre><code>class SensorReading(BaseModel):\n    station: StationId\n    date: date\n    temperature_c: float = Field(ge=-50.0, le=60.0)\n    humidity_pct: float = Field(ge=0.0, le=100.0)\n    status: OperationalStatus\n\n    @computed_field\n    @property\n    def heat_index(self) -&gt; float:\n        return round(self.temperature_c + 0.05 * self.humidity_pct, 1)\n\n    @computed_field\n    @property\n    def temp_category(self) -&gt; TempCategory:\n        if self.temperature_c &lt;= 0.0:\n            return TempCategory.COLD\n        ...\n\n@task\ndef transform_to_dataframe(readings: list[SensorReading]) -&gt; TransformResult:\n    rows = [r.model_dump() for r in readings]\n    df = pd.DataFrame(rows)\n    buf = io.BytesIO()\n    df.to_parquet(buf, engine=\"pyarrow\", index=False)\n    return TransformResult(parquet_data=buf.getvalue(), ...)\n\n@task\ndef upload_to_s3(transform: TransformResult, key: str) -&gt; UploadResult:\n    minio_creds = MinIOCredentials(minio_root_user=\"admin\", ...)\n    bucket = S3Bucket(bucket_name=\"prefect-data\", credentials=minio_creds,\n                      aws_client_parameters=AwsClientParameters(endpoint_url=...))\n    bucket.upload_from_file_object(io.BytesIO(transform.parquet_data), key)\n    return UploadResult(key=key, backend=StorageBackend.S3, ...)\n</code></pre> <p>Sensor readings are validated with Pydantic <code>Field</code> constraints and <code>computed_field</code> for derived values (heat index, temperature category). The <code>model_dump()</code> method converts validated models to dicts for pandas. <code>MinIOCredentials</code> and <code>S3Bucket</code> from prefect-aws provide the S3 client -- the same blocks work with AWS S3 or any S3-compatible service (MinIO, RustFS). If S3 is unavailable, the flow falls back to a local temp file.</p>"},{"location":"flow-reference/#dhis2-geoparquet-export","title":"DHIS2 GeoParquet Export","text":"<p>What it demonstrates: Fetch org units with geometry from DHIS2, build a GeoDataFrame with shapely, and export to S3-compatible storage as GeoParquet (OGC standard for geospatial data in Parquet format).</p> <p>Airflow equivalent: PythonOperator + S3Hook.upload_file() with GeoPandas.</p> <pre><code>@task\ndef build_geodataframe(org_units: list[dict[str, Any]]) -&gt; gpd.GeoDataFrame:\n    rows = []\n    for ou in org_units:\n        geom = ou.get(\"geometry\")\n        if not geom:\n            continue\n        rows.append({\n            \"id\": ou[\"id\"],\n            \"name\": ou.get(\"name\", \"\"),\n            \"geometry\": shape(geom),\n        })\n    return gpd.GeoDataFrame(rows, geometry=\"geometry\", crs=\"EPSG:4326\")\n\n@task\ndef export_geoparquet(gdf: gpd.GeoDataFrame) -&gt; bytes:\n    buf = io.BytesIO()\n    gdf.to_parquet(buf, engine=\"pyarrow\", index=False)\n    return buf.getvalue()\n\n@flow(name=\"cloud_dhis2_geoparquet_export\", log_prints=True)\ndef dhis2_geoparquet_export_flow(instance: str = \"dhis2\") -&gt; ExportReport:\n    creds = get_dhis2_credentials(instance)\n    client = creds.get_client()\n    org_units = fetch_org_units_with_geometry(client)\n    gdf = build_geodataframe(org_units)\n    data = export_geoparquet(gdf)\n    key, backend = upload_to_s3(data, s3_key)\n    report = build_report(gdf, data, key, backend)\n    return report\n</code></pre> <p><code>shapely.geometry.shape()</code> converts DHIS2 GeoJSON geometry into Shapely objects. <code>GeoDataFrame.to_parquet()</code> writes OGC GeoParquet with embedded CRS metadata. The same S3 upload/fallback pattern from the S3 Parquet Export flow is reused. Multi-instance support via the <code>instance</code> parameter loads different DHIS2 credentials blocks.</p>"},{"location":"flow-reference/#worldpop-api","title":"WorldPop API","text":""},{"location":"flow-reference/#worldpop-dataset-catalog","title":"WorldPop Dataset Catalog","text":"<p>What it demonstrates: Hierarchical REST API navigation and dataset discovery using the WorldPop catalog API. Queries top-level datasets, drills into sub-datasets, and filters by country ISO3 code.</p> <p>Airflow equivalent: PythonOperator + HttpHook for REST API traversal.</p> <pre><code>@task(retries=2, retry_delay_seconds=[2, 5])\ndef list_datasets() -&gt; list[DatasetRecord]:\n    with httpx.Client(timeout=30) as client:\n        resp = client.get(WORLDPOP_CATALOG_URL)\n        resp.raise_for_status()\n    return [DatasetRecord(id=item.get(\"alias\", \"\"), ...) for item in resp.json()[\"data\"]]\n\n@task(retries=2, retry_delay_seconds=[2, 5])\ndef query_country_datasets(dataset: str, subdataset: str, iso3: str) -&gt; list[CountryDatasetRecord]:\n    url = f\"{WORLDPOP_CATALOG_URL}/{dataset}/{subdataset}\"\n    with httpx.Client(timeout=30) as client:\n        resp = client.get(url, params={\"iso3\": iso3})\n        resp.raise_for_status()\n    return [CountryDatasetRecord(...) for item in resp.json()[\"data\"]]\n\n@flow(name=\"cloud_worldpop_dataset_catalog\", log_prints=True)\ndef worldpop_dataset_catalog_flow(query: DatasetQuery | None = None) -&gt; CatalogReport:\n    datasets = list_datasets()\n    subdatasets = list_subdatasets(query.dataset)\n    country_records = query_country_datasets(query.dataset, query.subdataset, query.iso3)\n    report = build_catalog_report(datasets, subdatasets, country_records, query)\n    create_markdown_artifact(key=\"worldpop-dataset-catalog\", markdown=report.markdown)\n    return report\n</code></pre> <p>Multi-level REST API traversal with optional parameters at each level. Pydantic models validate each API response shape. The flow produces a markdown artifact with tables of available datasets, sub-datasets, and country records.</p>"},{"location":"flow-reference/#worldpop-population-stats","title":"WorldPop Population Stats","text":"<p>What it demonstrates: GeoJSON spatial queries against the WorldPop stats API, with support for both synchronous and asynchronous (polling) execution modes.</p> <p>Airflow equivalent: PythonOperator + HttpHook with polling sensor.</p> <pre><code>@task(retries=2, retry_delay_seconds=[2, 5])\ndef query_population(year: int, geojson: dict[str, Any], run_async: bool = False) -&gt; PopulationResult:\n    params = {\"dataset\": \"wpgppop\", \"year\": str(year), \"geojson\": json.dumps(geojson)}\n    if run_async:\n        params[\"runasync\"] = \"true\"\n    with httpx.Client(timeout=30) as client:\n        resp = client.get(WORLDPOP_STATS_URL, params=params)\n        resp.raise_for_status()\n        body = resp.json()\n    if run_async and body.get(\"status\") == \"created\":\n        body = _poll_async_result(body[\"taskid\"])\n    return PopulationResult(total_population=float(body[\"data\"][\"total_population\"]), year=year)\n</code></pre> <p>GeoJSON polygons are passed as flow parameters. The async mode demonstrates a polling pattern -- submit a job, then poll for results with timeout handling.</p>"},{"location":"flow-reference/#worldpop-age-sex-pyramid","title":"WorldPop Age-Sex Pyramid","text":"<p>What it demonstrates: Demographic data transformation from the WorldPop age-sex stats API. Parses age-sex brackets, computes dependency ratio, sex ratio, and median age bracket.</p> <p>Airflow equivalent: PythonOperator + HttpHook with data transformation.</p> <pre><code>@task\ndef compute_demographics(brackets: list[AgeSexBracket]) -&gt; DemographicStats:\n    total_male = sum(b.male for b in brackets)\n    total_female = sum(b.female for b in brackets)\n    sex_ratio = (total_male / total_female * 100) if total_female &gt; 0 else 0.0\n    young = sum(b.total for b in brackets[:3])\n    old = sum(b.total for b in brackets[13:])\n    working_age = sum(b.total for b in brackets[3:13])\n    dependency_ratio = ((young + old) / working_age * 100) if working_age &gt; 0 else 0.0\n    return DemographicStats(sex_ratio=round(sex_ratio, 1), dependency_ratio=round(dependency_ratio, 1), ...)\n</code></pre> <p>Complex API response parsing into typed Pydantic models. Computed statistics (dependency ratio, sex ratio) demonstrate data transformation patterns. The markdown artifact includes both a tabular pyramid and summary statistics.</p>"},{"location":"flow-reference/#worldpop-country-comparison","title":"WorldPop Country Comparison","text":"<p>What it demonstrates: Parallel multi-country queries using <code>.map()</code>, aggregation, and comparison table generation.</p> <p>Airflow equivalent: PythonOperator in a loop or dynamic task mapping.</p> <pre><code>@flow(name=\"cloud_worldpop_country_comparison\", log_prints=True)\ndef worldpop_country_comparison_flow(query: ComparisonQuery | None = None) -&gt; ComparisonReport:\n    futures = fetch_country_metadata.map(\n        query.iso3_codes,\n        dataset=query.dataset,\n        subdataset=query.subdataset,\n        year=query.year,\n    )\n    records = [f.result() for f in futures]\n    sorted_records = aggregate_comparison(records)\n    report = build_comparison_report(sorted_records, query)\n    create_markdown_artifact(key=\"worldpop-country-comparison\", markdown=report.markdown)\n    return report\n</code></pre> <p><code>.map()</code> fans out one API call per country in parallel. Results are collected and aggregated into a sorted comparison table. The flow demonstrates Prefect's dynamic mapping pattern with real API calls.</p>"},{"location":"flow-reference/#worldpop-country-report","title":"WorldPop Country Report","text":"<p>What it demonstrates: Multi-source API aggregation -- WorldPop catalog metadata enriched with population numbers from the World Bank API -- followed by Slack notification via webhook.</p> <p>External APIs: - WorldPop catalog API (<code>hub.worldpop.org/rest/data</code>) -- dataset metadata   and year-range availability per country. - World Bank API (<code>api.worldbank.org/v2</code>) -- country-level population by   ISO3 code and year (indicator <code>SP.POP.TOTL</code>).</p> <p>Airflow equivalent: PythonOperator + HttpHook + SlackWebhookOperator.</p> <pre><code>@flow(name=\"cloud_worldpop_country_report\", log_prints=True)\ndef worldpop_country_report_flow(query: CountryReportQuery | None = None) -&gt; PopulationReport:\n    futures = fetch_country_data.map(query.iso3_codes, dataset=query.dataset, subdataset=query.subdataset)\n    raw_countries = [f.result() for f in futures]\n    pop_data = fetch_population(query.iso3_codes, query.year)\n    for c in raw_countries:\n        c.population = pop_data.get(c.iso3, 0)\n    countries = transform_results(raw_countries)\n    markdown = build_report(countries)\n    create_markdown_artifact(key=\"worldpop-country-report\", markdown=markdown)\n    slack_sent = send_slack_notification(countries)\n    return PopulationReport(countries=countries, total_countries=len(countries), markdown=markdown, slack_sent=slack_sent)\n</code></pre> <p>The World Bank batch endpoint accepts semicolon-separated ISO3 codes in a single request, avoiding per-country fan-out for population data. Results are merged into the WorldPop metadata before sorting by population descending.</p>"},{"location":"flow-reference/#worldpop-population-time-series","title":"WorldPop Population Time-Series","text":"<p>What it demonstrates: Time-series construction from sequential API queries, year-over-year growth rate computation, and peak growth identification.</p> <p>Airflow equivalent: PythonOperator loop with HttpHook for paginated API.</p> <pre><code>@task\ndef compute_growth_rates(year_records: list[YearMetadata]) -&gt; list[GrowthRate]:\n    growth_rates = []\n    for i in range(1, len(year_records)):\n        prev_year = year_records[i - 1].year\n        curr_year = year_records[i].year\n        year_gap = curr_year - prev_year\n        compound_rate = ((1 + 2.5 / 100) ** year_gap - 1) * 100\n        growth_rates.append(GrowthRate(from_year=prev_year, to_year=curr_year, growth_rate_pct=round(compound_rate, 2)))\n    return growth_rates\n</code></pre> <p>Sequential API pagination over available years for a single country. Growth rates are computed from consecutive year metadata. The markdown artifact includes both the year listing and a growth rate table with peak identification.</p>"},{"location":"flow-reference/#world-bank-api","title":"World Bank API","text":""},{"location":"flow-reference/#world-bank-gdp-comparison","title":"World Bank GDP Comparison","text":"<p>What it demonstrates: Batch multi-country API call to the World Bank indicators endpoint, ranking and report generation.</p> <p>Airflow equivalent: PythonOperator + HttpHook for REST API with data ranking.</p> <pre><code>@task(retries=2, retry_delay_seconds=[2, 5])\ndef fetch_gdp_data(iso3_codes: list[str], year: int) -&gt; list[CountryGdp]:\n    codes = \";\".join(iso3_codes)\n    url = f\"{WORLDBANK_API_URL}/country/{codes}/indicator/NY.GDP.MKTP.CD\"\n    params = {\"date\": str(year), \"format\": \"json\", \"per_page\": str(len(iso3_codes))}\n    with httpx.Client(timeout=30) as client:\n        resp = client.get(url, params=params)\n        resp.raise_for_status()\n    ...\n\n@flow(name=\"cloud_worldbank_gdp_comparison\", log_prints=True)\ndef worldbank_gdp_comparison_flow(query: GdpComparisonQuery | None = None) -&gt; GdpComparisonReport:\n    countries = fetch_gdp_data(query.iso3_codes, query.year)\n    ranked = rank_by_gdp(countries)\n    markdown = build_gdp_report(ranked, query)\n    create_markdown_artifact(key=\"worldbank-gdp-comparison\", markdown=markdown)\n    return GdpComparisonReport(countries=ranked, total_countries=len(ranked), markdown=markdown)\n</code></pre> <p>The World Bank batch endpoint accepts semicolon-separated ISO3 codes, avoiding per-country fan-out. GDP values are ranked descending with summary statistics.</p>"},{"location":"flow-reference/#world-bank-indicator-time-series","title":"World Bank Indicator Time-Series","text":"<p>What it demonstrates: Time-series construction from a single World Bank indicator over a year range, with year-over-year growth rate computation and peak/trough identification.</p> <p>Airflow equivalent: PythonOperator loop with HttpHook for paginated API.</p> <pre><code>@task(retries=2, retry_delay_seconds=[2, 5])\ndef fetch_indicator_timeseries(iso3: str, indicator: str, start_year: int, end_year: int) -&gt; list[YearValue]:\n    url = f\"{WORLDBANK_API_URL}/country/{iso3}/indicator/{indicator}\"\n    params = {\"date\": f\"{start_year}:{end_year}\", \"format\": \"json\", \"per_page\": \"100\"}\n    ...\n\n@task\ndef compute_growth_rates(data: list[YearValue]) -&gt; list[GrowthRate]:\n    rates = []\n    for i in range(1, len(data)):\n        growth = (data[i].value - data[i-1].value) / data[i-1].value * 100\n        rates.append(GrowthRate(year=data[i].year, value=data[i].value, growth_pct=round(growth, 4)))\n    return rates\n</code></pre> <p>Year-range queries use <code>date=2000:2023</code> syntax. Null values from the API are filtered before growth rate computation. Peak and trough growth years are identified in the report summary.</p>"},{"location":"flow-reference/#world-bank-country-profile","title":"World Bank Country Profile","text":"<p>What it demonstrates: Multi-indicator aggregation using <code>.map()</code> to query several different World Bank indicators for a single country in parallel. Builds a dashboard-style country profile.</p> <p>Airflow equivalent: PythonOperator with multiple HttpHook calls.</p> <pre><code>DEFAULT_INDICATORS = [\n    \"SP.POP.TOTL\",      # Population\n    \"NY.GDP.MKTP.CD\",   # GDP (current US$)\n    \"NY.GDP.PCAP.CD\",   # GDP per capita\n    \"SP.DYN.LE00.IN\",   # Life expectancy\n    \"SE.ADT.LITR.ZS\",   # Literacy rate\n    \"SH.DYN.MORT\",      # Under-5 mortality rate\n]\n\n@flow(name=\"cloud_worldbank_country_profile\", log_prints=True)\ndef worldbank_country_profile_flow(query: ProfileQuery | None = None) -&gt; CountryProfile:\n    futures = fetch_indicator.map(query.iso3, query.indicators, year=query.year)\n    indicators = [f.result() for f in futures]\n    profile = build_country_profile(query.iso3, query.year, indicators)\n    create_markdown_artifact(key=\"worldbank-country-profile\", markdown=profile.markdown)\n    return profile\n</code></pre> <p><code>.map()</code> parallelises one API call per indicator. Value formatting adapts to indicator type: percentages for rate indicators (<code>.ZS</code> suffix), dollar formatting for large monetary values, and comma-separated integers for counts.</p>"},{"location":"flow-reference/#world-bank-poverty-analysis","title":"World Bank Poverty Analysis","text":"<p>What it demonstrates: Handling sparse/missing data common with development indicators. Uses <code>.map()</code> for parallel country queries, trend detection, and robust null-value handling.</p> <p>Airflow equivalent: PythonOperator in a loop with missing-data handling.</p> <pre><code>@task(retries=2, retry_delay_seconds=[2, 5])\ndef fetch_poverty_data(iso3: str, start_year: int, end_year: int) -&gt; CountryPoverty:\n    url = f\"{WORLDBANK_API_URL}/country/{iso3}/indicator/SI.POV.DDAY\"\n    ...\n    # Filter null values, find latest data point, determine trend\n    if len(data) &gt;= 2:\n        diff = latest_value - earliest_value\n        trend = \"improving\" if diff &lt; -1 else \"worsening\" if diff &gt; 1 else \"stable\"\n</code></pre> <p>Poverty data (<code>SI.POV.DDAY</code> -- headcount ratio at $2.15/day) is notoriously sparse, with many countries missing recent data points. The flow demonstrates robust handling of null API values, trend detection from available data, and sorting that separates countries with data from those without.</p>"},{"location":"flow-reference/#deployments-directory","title":"Deployments directory","text":"<p>Production deployment examples live in the <code>deployments/</code> directory. Each subdirectory is a self-contained deployment with its own <code>flow.py</code>, <code>prefect.yaml</code>, and <code>deploy.py</code>.</p>"},{"location":"flow-reference/#dhis2_connection-dhis2-connection-check","title":"dhis2_connection -- DHIS2 Connection Check","text":"<p>What it demonstrates: Verifies DHIS2 connectivity, fetches the org unit count, and produces a connection status artifact.</p> <pre><code>@flow(name=\"dhis2_connection\", log_prints=True)\ndef dhis2_connection_flow() -&gt; ConnectionReport:\n    creds = get_dhis2_credentials()\n    client = creds.get_client()\n    server_info = verify_connection(client)\n    count = fetch_org_unit_count(client)\n    report = build_report(creds, server_info, count)\n    return report\n</code></pre>"},{"location":"flow-reference/#dhis2_block_connection-dhis2-block-connection","title":"dhis2_block_connection -- DHIS2 Block Connection","text":"<p>What it demonstrates: Verifies DHIS2 connectivity using a named credentials block, fetches the org unit count, and produces a connection status artifact. Supports multi-instance deployment via the <code>instance</code> parameter.</p> <pre><code>@flow(name=\"dhis2_block_connection\", log_prints=True)\ndef dhis2_block_connection_flow(instance: str = \"dhis2\") -&gt; ConnectionReport:\n    creds = get_dhis2_credentials(instance)\n    client = creds.get_client()\n    server_info = verify_connection(client)\n    count = fetch_org_unit_count(client)\n    report = build_report(creds, instance, server_info, count)\n    return report\n</code></pre>"},{"location":"flow-reference/#dhis2_ou-dhis2-organisation-units","title":"dhis2_ou -- DHIS2 Organisation Units","text":"<p>What it demonstrates: A deployment-ready flow that fetches DHIS2 organisation units and produces a markdown artifact listing each unit's ID and display name.</p> <pre><code>@flow(name=\"dhis2_ou\", log_prints=True)\ndef dhis2_ou_flow() -&gt; OrgUnitReport:\n    client = get_dhis2_credentials().get_client()\n    org_units = fetch_org_units(client)\n    report = build_report(org_units)\n    return report\n</code></pre> <p>Key patterns demonstrated:</p> <ul> <li><code>prefect.runtime</code> -- <code>deployment.name</code> provides deployment-aware   context (falls back to \"local\" for direct runs)</li> <li>Block reference -- <code>get_dhis2_credentials()</code> loads credentials from a   saved block or falls back to inline defaults</li> <li>Markdown artifact -- <code>create_markdown_artifact</code> produces a table   visible in the Prefect UI artifacts tab</li> <li>Per-deployment <code>prefect.yaml</code> -- declarative config lives alongside   the flow code</li> </ul> <p>Deploy with <code>prefect.yaml</code>:</p> <pre><code>deployments:\n  - name: dhis2-ou\n    entrypoint: flow.py:dhis2_ou_flow\n    schedules:\n      - cron: \"0 6 * * *\"\n        timezone: \"UTC\"\n    work_pool:\n      name: default\n</code></pre> <p>Manage after deployment:</p> <pre><code>cd deployments/dhis2_ou &amp;&amp; prefect deploy --all   # register deployment\nprefect deployment run dhis2_ou/dhis2-ou           # trigger run\nprefect deployment set-schedule &lt;name&gt; --cron \"0 8 * * *\"     # change schedule\nprefect deployment pause &lt;name&gt;                   # pause scheduling\nprefect deployment resume &lt;name&gt;                  # resume scheduling\n</code></pre>"},{"location":"flow-reference/#s3_parquet_export-s3-parquet-export","title":"s3_parquet_export -- S3 Parquet Export","text":"<p>What it demonstrates: Deployment wrapper for the S3 Parquet Export flow that generates sample sensor data, transforms with pandas, and writes parquet to S3-compatible storage.</p> <pre><code>@flow(name=\"s3_parquet_export\", log_prints=True)\ndef s3_parquet_export_flow(n_records: int = 500) -&gt; ExportResult:\n    readings = generate_records(n_records)\n    transform = transform_to_dataframe(readings)\n    upload = upload_to_s3(transform, s3_key)\n    result = verify_upload(upload, transform)\n    return result\n</code></pre>"},{"location":"flow-reference/#dhis2_geoparquet_export-dhis2-geoparquet-export","title":"dhis2_geoparquet_export -- DHIS2 GeoParquet Export","text":"<p>What it demonstrates: A deployment-ready flow that fetches DHIS2 org units with geometry, builds a GeoDataFrame, and exports GeoParquet to S3. Supports multi-instance deployment via the <code>instance</code> parameter.</p> <pre><code>@flow(name=\"dhis2_geoparquet_export\", log_prints=True)\ndef dhis2_geoparquet_export_flow(instance: str = \"dhis2\") -&gt; ExportReport:\n    creds = get_dhis2_credentials(instance)\n    client = creds.get_client()\n    org_units = fetch_org_units_with_geometry(client)\n    gdf = build_geodataframe(org_units)\n    data = export_geoparquet(gdf)\n    key, backend = upload_to_s3(data, s3_key)\n    report = build_report(gdf, data, key, backend)\n    return report\n</code></pre>"},{"location":"flow-reference/#dhis2_worldpop_population_import-dhis2-worldpop-population-import","title":"dhis2_worldpop_population_import -- DHIS2 WorldPop Population Import","text":"<p>What it demonstrates: Deployment wrapper for the WorldPop population import flow that reads org unit polygon geometry from DHIS2, queries the WorldPop age-sex API, and writes sex-disaggregated population values back using category combinations.</p> <pre><code>@flow(name=\"dhis2_worldpop_population_import\", log_prints=True)\ndef dhis2_worldpop_population_import_flow(query: ImportQuery | None = None) -&gt; ImportResult:\n    client = get_dhis2_credentials().get_client()\n    org_units, coc_mapping = ensure_dhis2_metadata(client)\n    for ou in org_units:\n        results.append(fetch_worldpop_population(ou, query.year))\n    data_values = build_data_values(results, query.year, coc_mapping)\n    result = import_to_dhis2(client, dhis2_url, org_units, data_values)\n    return result\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>uv -- fast Python package manager</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/mortenoh/prefect-examples.git\ncd prefect-examples\nmake sync\n</code></pre> <p><code>make sync</code> runs <code>uv sync</code>, which installs all runtime and development dependencies (Prefect, pytest, ruff, mypy, mkdocs) into a project-local virtual environment.</p>"},{"location":"getting-started/#environment-variables","title":"Environment variables","text":"<p>Some flows require credentials or API URLs. Copy the example file and fill in any values you need:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Every flow file calls <code>load_dotenv()</code> in its <code>if __name__ == \"__main__\"</code> block, so variables from <code>.env</code> are loaded automatically when you run a flow with <code>uv run python</code>.</p>"},{"location":"getting-started/#run-your-first-flow","title":"Run your first flow","text":"<pre><code>uv run python flows/basics/basics_hello_world.py\n</code></pre> <p>You should see output like:</p> <pre><code>Hello from Prefect!\nTue Jan 15 10:30:00 UTC 2025\n</code></pre> <p>Every flow file is self-contained with an <code>if __name__ == \"__main__\"</code> block, so you can run any of them the same way:</p> <pre><code>uv run python flows/basics/basics_taskflow_etl.py\nuv run python flows/basics/basics_dynamic_tasks.py\n</code></pre>"},{"location":"getting-started/#run-the-tests","title":"Run the tests","text":"<pre><code>make test\n</code></pre> <p>This runs the full pytest suite. All 113 flows have corresponding tests in <code>tests/</code>.</p>"},{"location":"getting-started/#serve-the-documentation","title":"Serve the documentation","text":"<pre><code>make docs\n</code></pre> <p>Opens a local MkDocs server at <code>http://127.0.0.1:8000</code>.</p>"},{"location":"getting-started/#run-the-docker-stack","title":"Run the Docker stack","text":"<p>For a full Prefect environment with a server, worker, and database:</p> <pre><code>make start\n</code></pre> <p>This runs <code>docker compose up --build</code> and starts PostgreSQL, Prefect Server (UI at <code>http://localhost:4200</code>), a Prefect Worker, and RustFS for object storage. Once running, deploy the example flows:</p> <pre><code>make deploy\n</code></pre> <p>See Infrastructure for full details on the Docker stack.</p>"},{"location":"getting-started/#project-layout","title":"Project layout","text":"<pre><code>prefect-examples/\n    flows/              # 113 example flows in topic subdirectories\n    deployments/        # Production deployment examples\n        dhis2_connection/       # Connection check deployment\n        dhis2_ou/               # Org unit listing deployment\n        dhis2_block_connection/ # Block-based connection deployment\n        s3_parquet_export/      # S3 parquet export deployment\n        dhis2_geoparquet_export/ # DHIS2 GeoParquet export deployment\n    src/\n        prefect_examples/\n            config.py   # Shared configuration defaults\n            dhis2.py    # DHIS2 credentials block and API client\n            tasks.py    # Reusable task library\n    tests/              # pytest test suite\n    docs/               # MkDocs documentation source\n    compose.yml         # Docker Compose stack (Postgres, Server, Worker, RustFS)\n    Makefile            # Common commands (sync, test, lint, docs, start, deploy)\n    pyproject.toml      # Project metadata and tool config\n    mkdocs.yml          # MkDocs configuration\n</code></pre>"},{"location":"infrastructure/","title":"Infrastructure","text":"<p>The project includes a Docker Compose stack for running a complete Prefect environment locally. This page documents the services, configuration, and development workflow.</p>"},{"location":"infrastructure/#quick-start","title":"Quick start","text":"<pre><code>cp .env.example .env  # configure credentials (optional -- enables auth)\nmake start            # start all services\nmake deploy           # register deployments with the Prefect server\n</code></pre> <p>The Prefect UI is available at <code>http://localhost:4200</code>.</p> <p>The same <code>.env</code> file is used for local development -- every flow file calls <code>load_dotenv()</code> in its <code>__main__</code> block, so <code>uv run python flows/...</code> picks up the variables automatically.</p>"},{"location":"infrastructure/#services","title":"Services","text":"<p>The <code>compose.yml</code> defines four services:</p> <pre><code>graph LR\n    W[Prefect Worker] --&gt;|polls| S[Prefect Server :4200]\n    S --&gt;|reads/writes| P[(PostgreSQL)]\n    S --&gt;|stores results| R[RustFS :9000]</code></pre>"},{"location":"infrastructure/#postgresql","title":"PostgreSQL","text":"Setting Value Image <code>postgres:17-alpine</code> User / Password / DB <code>prefect</code> / <code>prefect</code> / <code>prefect</code> <p>Stores Prefect server state (flow runs, deployments, blocks, etc.). The database port is not exposed to the host (internal Docker network only). A healthcheck ensures the server waits for the database to be ready.</p>"},{"location":"infrastructure/#prefect-server","title":"Prefect Server","text":"Setting Value Image <code>prefecthq/prefect:3-python3.13</code> Port <code>4200</code> (UI + API) Depends on PostgreSQL (healthy) <p>Runs <code>prefect server start --host 0.0.0.0</code>. The server provides the UI, API, and orchestration layer. It connects to PostgreSQL via the <code>PREFECT_API_DATABASE_CONNECTION_URL</code> environment variable.</p>"},{"location":"infrastructure/#prefect-worker","title":"Prefect Worker","text":"Setting Value Image <code>prefecthq/prefect:3-python3.13</code> Depends on Prefect Server (healthy) Work pool <code>default</code> <p>Runs <code>prefect worker start -p default</code>. The worker polls the server for scheduled flow runs and executes them. It has access to the same volumes as the server (<code>flows/</code>, <code>src/</code>, <code>deployments/</code>).</p>"},{"location":"infrastructure/#rustfs","title":"RustFS","text":"Setting Value Image <code>rustfs/rustfs:latest</code> Ports <code>9000</code> (API), <code>9001</code> (console) Access Key / Secret Key <code>admin</code> / <code>admin</code> <p>S3-compatible object storage for flow result persistence and artifact storage.</p>"},{"location":"infrastructure/#volumes","title":"Volumes","text":"<p>All services share host-mounted volumes so code changes are immediately available without rebuilding containers:</p> Host path Container path Purpose <code>./flows</code> <code>/opt/prefect/flows</code> Flow source files <code>./src</code> <code>/opt/prefect/src</code> Shared library (<code>prefect_examples</code>) <code>./deployments</code> <code>/opt/prefect/deployments</code> Deployment configurations <p>The <code>PYTHONPATH</code> environment variable is set to <code>/opt/prefect/src</code> so that <code>from prefect_examples.dhis2 import ...</code> works inside containers.</p>"},{"location":"infrastructure/#environment-variables","title":"Environment variables","text":"<p>Shared across services via the <code>x-prefect-env</code> anchor:</p> Variable Value Purpose <code>PREFECT_API_DATABASE_CONNECTION_URL</code> <code>postgresql+asyncpg://prefect:prefect@postgres:5432/prefect</code> Database connection <code>PREFECT_SERVER_ANALYTICS_ENABLED</code> <code>false</code> Disable telemetry <code>PREFECT_SERVER_UI_SHOW_PROMOTIONAL_CONTENT</code> <code>false</code> Clean UI <code>PYTHONPATH</code> <code>/opt/prefect/src</code> Make <code>prefect_examples</code> importable <p>The worker additionally sets:</p> Variable Value Purpose <code>PREFECT_API_URL</code> <code>http://prefect-server:4200/api</code> Connect worker to server"},{"location":"infrastructure/#authentication","title":"Authentication","text":"<p>Prefect 3 self-hosted supports HTTP Basic Auth via environment variables. When enabled, the server requires a username and password for all API and UI access.</p>"},{"location":"infrastructure/#how-it-works","title":"How it works","text":"Variable Where Purpose <code>PREFECT_SERVER_API_AUTH_STRING</code> Server Enables auth and defines the <code>user:password</code> credential <code>PREFECT_API_AUTH_STRING</code> Clients (worker, CLI) Supplies credentials when calling the server API <p>Both variables use the format <code>username:password</code>. When <code>PREFECT_SERVER_API_AUTH_STRING</code> is unset or empty, authentication is disabled (the default).</p>"},{"location":"infrastructure/#setting-credentials","title":"Setting credentials","text":"<p>Copy the template and edit to taste:</p> <pre><code>cp .env.example .env\n# uncomment the PREFECT_SERVER_API_AUTH_STRING and PREFECT_API_AUTH_STRING lines\n</code></pre> <p>Docker Compose loads <code>.env</code> into the server and worker containers via <code>env_file</code>. When the auth variables are commented out (the default), auth is disabled. Uncomment both to enable it.</p>"},{"location":"infrastructure/#ui-login","title":"UI login","text":"<p>When auth is enabled, the Prefect UI shows a browser-native Basic Auth login prompt. Enter the username and password from your <code>.env</code> file (the value of <code>PREFECT_SERVER_API_AUTH_STRING</code>, e.g. <code>admin:admin</code>).</p>"},{"location":"infrastructure/#cli-usage","title":"CLI usage","text":"<p>For local CLI commands that target the Docker server, export the auth variable:</p> <pre><code>export PREFECT_API_URL=http://localhost:4200/api\nexport PREFECT_API_AUTH_STRING=admin:changeme\n\nprefect deployment ls\nprefect flow-run ls\n</code></pre> <p>Or inline:</p> <pre><code>PREFECT_API_AUTH_STRING=admin:changeme prefect deployment ls\n</code></pre> <p>The Makefile targets (<code>deploy</code>, <code>register-blocks</code>, <code>create-blocks</code>) forward <code>PREFECT_API_AUTH_STRING</code> from the shell environment automatically.</p>"},{"location":"infrastructure/#csrf-protection","title":"CSRF protection","text":"<p>Prefect 3 also supports CSRF token protection. Enable it with:</p> <pre><code>PREFECT_SERVER_CSRF_PROTECTION_ENABLED=true\n</code></pre> <p>This adds a CSRF token requirement to state-changing API requests. The Prefect client handles token exchange automatically.</p>"},{"location":"infrastructure/#standalone-server-no-docker","title":"Standalone server (no Docker)","text":"<p>When running the server outside Docker via <code>make server</code>, pass the auth string directly:</p> <pre><code>PREFECT_SERVER_API_AUTH_STRING=admin:changeme make server\n</code></pre>"},{"location":"infrastructure/#limitations","title":"Limitations","text":"<p>Basic Auth protects the API with a single shared credential. For more advanced requirements:</p> Feature Availability HTTP Basic Auth Prefect OSS (this setup) RBAC (role-based access) Prefect Cloud only SSO / OIDC Prefect Cloud only JWT / OAuth2 Reverse proxy (nginx/Caddy/Traefik with oauth2-proxy) <p>For comparison, Airflow ships with built-in RBAC and Flask-Login for multi-user access control. Prefect OSS provides only basic auth; teams needing per-user permissions should use Prefect Cloud or add a reverse proxy with an identity provider.</p>"},{"location":"infrastructure/#makefile-targets","title":"Makefile targets","text":"Target Command Description <code>make start</code> <code>docker compose up --build</code> Start all services (foreground) <code>make restart</code> <code>docker compose down -v &amp;&amp; docker compose build --no-cache &amp;&amp; docker compose up</code> Full teardown and rebuild <code>make deploy</code> register-blocks + create-blocks + <code>prefect deploy --all</code> Register blocks, create instances, and deploy all flows <code>make register-blocks</code> <code>prefect block register -m prefect_dhis2</code> Register custom block types with the server <code>make create-blocks</code> <code>uv run python scripts/create_blocks.py</code> Create DHIS2 credentials block instances for all known servers <code>make server</code> Server + worker Start Prefect server and a process worker (no Docker) <code>make deploy-local</code> <code>prefect deploy --all</code> Deploy all flows from root <code>prefect.yaml</code> to local server <p>To run services in the background, add <code>-d</code>:</p> <pre><code>docker compose up --build -d\n</code></pre> <p>View logs:</p> <pre><code>docker compose logs -f prefect-server\ndocker compose logs -f prefect-worker\n</code></pre> <p>Stop and remove everything (including database data):</p> <pre><code>docker compose down -v\n</code></pre>"},{"location":"infrastructure/#deployment-workflow","title":"Deployment workflow","text":"<p>There are two ways to run deployments locally: Docker Compose (production-like) and standalone (lightweight, no Docker).</p>"},{"location":"infrastructure/#option-a-docker-compose-production-like","title":"Option A: Docker Compose (production-like)","text":""},{"location":"infrastructure/#1-start-the-stack","title":"1. Start the stack","text":"<pre><code>make start\n</code></pre> <p>This brings up PostgreSQL, the Prefect server, a worker, and object storage. Wait for the healthchecks to pass. The UI will be at <code>http://localhost:4200</code>.</p>"},{"location":"infrastructure/#2-deploy-flows","title":"2. Deploy flows","text":"<pre><code>make deploy\n</code></pre> <p>This registers custom block types, creates block instances, and deploys flows from the <code>deployments/</code> directories. Each deployment has its own <code>prefect.yaml</code>.</p>"},{"location":"infrastructure/#option-b-standalone-no-docker","title":"Option B: Standalone (no Docker)","text":""},{"location":"infrastructure/#1-start-server-and-worker","title":"1. Start server and worker","text":"<pre><code>make server\n</code></pre> <p>This starts both the Prefect server and a process worker in a single terminal. The UI will be at <code>http://localhost:4200</code>. Press Ctrl+C to stop both.</p>"},{"location":"infrastructure/#2-deploy-all-flows","title":"2. Deploy all flows","text":"<pre><code>make deploy-local\n</code></pre> <p>This creates the <code>default</code> work pool (if needed) and deploys all flows from the root <code>prefect.yaml</code>. The root <code>prefect.yaml</code> is auto-generated from the <code>flows/</code> directory and covers all example flows.</p>"},{"location":"infrastructure/#verify-in-the-ui","title":"Verify in the UI","text":"<p>Open <code>http://localhost:4200</code> and navigate to the Deployments page.</p>"},{"location":"infrastructure/#trigger-a-manual-run","title":"Trigger a manual run","text":"<p>CLI:</p> <pre><code>prefect deployment run dhis2_org_units/dhis2-org-units\n</code></pre> <p>REST API (curl):</p> <pre><code># Look up deployment by flow name and deployment name\ncurl -s http://localhost:4200/api/deployments/name/dhis2_org_units/dhis2-org-units\n\n# Trigger a run (requires deployment ID)\nDEPLOY_ID=$(curl -s http://localhost:4200/api/deployments/name/dhis2_org_units/dhis2-org-units | python3 -c \"import sys,json;print(json.load(sys.stdin)['id'])\")\n\ncurl -s -X POST \"http://localhost:4200/api/deployments/$DEPLOY_ID/create_flow_run\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n\n# With parameter overrides\ncurl -s -X POST \"http://localhost:4200/api/deployments/$DEPLOY_ID/create_flow_run\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"parameters\": {\"output_dir\": \"/tmp/org-units\"}}'\n</code></pre> <p>Or use the \"Run\" button in the UI.</p>"},{"location":"infrastructure/#monitor","title":"Monitor","text":"<p>The Flow Runs page in the UI shows run status, logs, artifacts, and task-level details. Use the CLI for a quick check:</p> <pre><code>prefect flow-run ls\n</code></pre>"},{"location":"infrastructure/#deployment-configuration","title":"Deployment configuration","text":"<p>Each deployment directory contains three files:</p> <pre><code>deployments/dhis2_ou/\n    flow.py           # The flow entrypoint\n    prefect.yaml      # Deployment configuration (schedule, work pool)\n    deploy.py         # Optional programmatic deployment script\n</code></pre>"},{"location":"infrastructure/#prefectyaml-format","title":"<code>prefect.yaml</code> format","text":"<pre><code>pull:\n  - prefect.deployments.steps.set_working_directory:\n      directory: /opt/prefect/deployments/dhis2_ou\n\ndeployments:\n  - name: dhis2-ou\n    entrypoint: flow.py:dhis2_ou_flow\n    schedules:\n      - cron: \"*/15 * * * *\"\n        timezone: \"UTC\"\n    work_pool:\n      name: default\n</code></pre> <p>Key fields:</p> Field Description <code>pull</code> Steps to prepare the execution environment (e.g. set working directory) <code>name</code> Deployment name (used in <code>prefect deployment run &lt;flow&gt;/&lt;name&gt;</code>) <code>entrypoint</code> <code>&lt;file&gt;:&lt;function&gt;</code> pointing to the flow <code>schedules</code> List of schedule objects (cron, interval, or rrule) <code>schedules[].cron</code> Cron expression (e.g. <code>\"*/15 * * * *\"</code> for every 15 minutes) <code>schedules[].timezone</code> Timezone for the schedule (e.g. <code>\"UTC\"</code>, <code>\"Europe/Oslo\"</code>) <code>work_pool.name</code> Which work pool executes the runs"},{"location":"infrastructure/#schedule-types-in-prefectyaml","title":"Schedule types in <code>prefect.yaml</code>","text":"<pre><code># Cron schedule\nschedules:\n  - cron: \"0 6 * * *\"\n    timezone: \"UTC\"\n\n# Interval schedule (seconds)\nschedules:\n  - interval: 900\n    timezone: \"UTC\"\n\n# RRule schedule\nschedules:\n  - rrule: \"FREQ=WEEKLY;BYDAY=MO,WE,FR\"\n    timezone: \"UTC\"\n</code></pre> <p>Multiple schedules can be combined on a single deployment. Each schedule creates independent runs.</p>"},{"location":"infrastructure/#updating-schedules","title":"Updating schedules","text":"<p>After a deployment is registered, schedules can be changed without redeploying:</p> <pre><code># Change to daily at 08:00\nprefect deployment set-schedule dhis2_ou/dhis2-ou --cron \"0 8 * * *\"\n\n# Change to every 30 minutes\nprefect deployment set-schedule dhis2_ou/dhis2-ou --interval 1800\n\n# Remove all schedules\nprefect deployment clear-schedule dhis2_ou/dhis2-ou\n\n# Pause without removing\nprefect deployment pause dhis2_ou/dhis2-ou\n</code></pre> <p>See CLI Reference for the full list of schedule commands.</p>"},{"location":"patterns/","title":"Patterns","text":"<p>Common patterns used throughout the examples.</p>"},{"location":"patterns/#subflows-for-composition","title":"Subflows for composition","text":"<p>Break large pipelines into smaller, independently testable flows. A parent flow calls child flows like regular functions:</p> <pre><code>@flow\ndef extract() -&gt; list[dict]:\n    return fetch_records()\n\n@flow\ndef transform(raw: list[dict]) -&gt; list[dict]:\n    return [clean(r) for r in raw]\n\n@flow\ndef pipeline():\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n</code></pre> <p>Each subflow gets its own flow run in the Prefect UI, with independent state tracking and retry behaviour.</p> <p>See: Subflows, Flow of Flows, Complex Pipeline</p>"},{"location":"patterns/#dynamic-mapping-with-map","title":"Dynamic mapping with <code>.map()</code>","text":"<p>Fan out work over a list of items at runtime. The number of items does not need to be known at definition time:</p> <pre><code>@task\ndef process_item(item: str) -&gt; str:\n    return f\"processed({item})\"\n\n@flow\ndef pipeline():\n    items = generate_items()          # returns [\"a\", \"b\", \"c\", ...]\n    processed = process_item.map(items)\n    summarize(processed)\n</code></pre> <p><code>.map()</code> creates one task run per item. Results are collected as a list of futures.</p> <p>You can chain <code>.map()</code> calls to build multi-step fan-out pipelines:</p> <pre><code>validated = validate_record.map(raw)\nenriched = enrich_record.map(validated)\n</code></pre> <p>See: Dynamic Tasks, Complex Pipeline, WorldPop Country Comparison</p>"},{"location":"patterns/#error-handling-with-allow_failure-and-retries","title":"Error handling with <code>allow_failure</code> and retries","text":""},{"location":"patterns/#retries","title":"Retries","text":"<p>Add <code>retries</code> and <code>retry_delay_seconds</code> to any task:</p> <pre><code>@task(retries=3, retry_delay_seconds=10)\ndef flaky_api_call() -&gt; dict:\n    ...\n</code></pre> <p>Prefect automatically retries the task up to the specified number of times, with a delay between attempts.</p>"},{"location":"patterns/#allow_failure","title":"allow_failure","text":"<p>When a task may fail but downstream work should still run, wrap its future with <code>allow_failure</code>:</p> <pre><code>from prefect import allow_failure\n\n@flow\ndef pipeline():\n    risky = risky_task.submit()\n    cleanup_task(wait_for=[allow_failure(risky)])\n</code></pre> <p>See: State Handlers, Retries and Hooks, WorldPop Dataset Catalog</p>"},{"location":"patterns/#polling-with-while-loops","title":"Polling with while loops","text":"<p>Replace Airflow sensors with a simple polling loop inside a task:</p> <pre><code>@task\ndef wait_for_file(path: str, interval: float = 5.0, timeout: float = 300.0) -&gt; str:\n    start = time.monotonic()\n    while True:\n        if Path(path).exists():\n            return path\n        if time.monotonic() - start &gt; timeout:\n            raise TimeoutError(f\"Timed out waiting for {path}\")\n        time.sleep(interval)\n</code></pre> <p>Use <code>.submit()</code> to run multiple polls in parallel:</p> <pre><code>future_a = poll_condition.submit(name=\"sensor-A\")\nfuture_b = poll_condition.submit(name=\"sensor-B\")\nprocess([future_a.result(), future_b.result()])\n</code></pre> <p>See: Polling Tasks, WorldPop Population Stats</p>"},{"location":"patterns/#concurrency-limits","title":"Concurrency limits","text":"<p>Throttle how many tasks run simultaneously using the <code>concurrency()</code> context manager:</p> <pre><code>from prefect.concurrency.sync import concurrency\n\n@task\ndef rate_limited_call(item: str) -&gt; str:\n    with concurrency(\"api-limit\", occupy=1):\n        return call_external_api(item)\n</code></pre> <p>The limit name is global -- all tasks sharing the same name compete for the same slots.</p> <p>See: Concurrency Limits</p>"},{"location":"patterns/#variables-and-configuration","title":"Variables and configuration","text":"<p>Store runtime configuration in Prefect Variables:</p> <pre><code>from prefect.variables import Variable\n\n# Write\nVariable.set(\"my_config\", '{\"batch_size\": 100}', overwrite=True)\n\n# Read\nraw = Variable.get(\"my_config\", default=\"{}\")\nconfig = json.loads(raw)\n</code></pre> <p>Combine with typed flow parameters for flexible, environment-specific behaviour:</p> <pre><code>@flow\ndef pipeline(environment: str = \"dev\"):\n    config = read_config()\n    process(config, environment)\n</code></pre> <p>See: Variables and Params</p>"},{"location":"patterns/#state-hooks-for-observability","title":"State hooks for observability","text":"<p>Attach hook functions to tasks or flows to react to state transitions:</p> <pre><code>def on_failure(task, task_run, state):\n    send_alert(f\"Task {task_run.name} failed: {state}\")\n\n@task(on_failure=[on_failure])\ndef important_task():\n    ...\n\ndef on_completion(flow, flow_run, state):\n    log_metric(f\"Flow {flow_run.name} finished: {state.name}\")\n\n@flow(on_completion=[on_completion])\ndef pipeline():\n    ...\n</code></pre> <p>Hooks are plain functions, not tasks. They receive the task/flow object, the run metadata, and the final state.</p> <p>See: State Handlers, Retries and Hooks</p>"},{"location":"patterns/#task-caching","title":"Task caching","text":"<p>Cache task results to avoid redundant computation. Prefect offers three caching strategies:</p> <pre><code>from prefect.cache_policies import INPUTS, TASK_SOURCE\n\n# Cache by inputs \u2014 same arguments return cached result\n@task(cache_policy=INPUTS, cache_expiration=300)\ndef expensive_computation(x: int, y: int) -&gt; int:\n    return x * y\n\n# Cache by source + inputs \u2014 invalidate when code changes\n@task(cache_policy=TASK_SOURCE + INPUTS)\ndef source_aware_task(data: str) -&gt; str:\n    return data.upper()\n\n# Custom cache key function\ndef my_cache_key(context, parameters):\n    return f\"{parameters['category']}:{parameters['item_id']}\"\n\n@task(cache_key_fn=my_cache_key, cache_expiration=600)\ndef cached_lookup(category: str, item_id: int) -&gt; dict: ...\n</code></pre> <p>Cache hits are only visible during Prefect runtime. Calling <code>.fn()</code> always executes the underlying function.</p> <p>See: Task Caching</p>"},{"location":"patterns/#async-patterns","title":"Async patterns","text":"<p>Use <code>async def</code> for tasks and flows when working with I/O-bound operations. Combine with <code>asyncio.gather()</code> for concurrent execution:</p> <pre><code>@task\nasync def fetch_endpoint(name: str) -&gt; dict:\n    await asyncio.sleep(0.5)\n    return {\"endpoint\": name, \"records\": 100}\n\n@flow\nasync def concurrent_flow() -&gt; None:\n    # Concurrent \u2014 total time \u2248 max(delays)\n    results = await asyncio.gather(\n        fetch_endpoint(\"users\"),\n        fetch_endpoint(\"orders\"),\n        fetch_endpoint(\"products\"),\n    )\n</code></pre> <p>Sync and async tasks can be mixed in an async flow. Sync tasks are called normally; async tasks are awaited.</p> <p>See: Async Tasks, Concurrent Async, Async Flow Patterns, Async Map and Submit</p>"},{"location":"patterns/#artifacts","title":"Artifacts","text":"<p>Publish rich content to the Prefect UI with artifact functions:</p> <pre><code>from prefect.artifacts import create_markdown_artifact, create_table_artifact, create_link_artifact\n\n# Markdown reports\ncreate_markdown_artifact(key=\"report\", markdown=\"# Report\\n...\", description=\"Daily report\")\n\n# Structured tables\ncreate_table_artifact(key=\"inventory\", table=[{\"item\": \"A\", \"qty\": 10}])\n\n# Reference links\ncreate_link_artifact(key=\"dashboard\", link=\"https://example.com/dashboard\")\n</code></pre> <p>Without a Prefect server, artifact functions silently no-op \u2014 tests pass locally.</p> <p>See: Markdown Artifacts, Table and Link Artifacts</p>"},{"location":"patterns/#blocks-and-secrets","title":"Blocks and secrets","text":"<p>Blocks provide typed, reusable configuration. The built-in <code>Secret</code> block handles encrypted credentials:</p> <pre><code>from prefect.blocks.system import Secret\n\n# Load with graceful fallback\ntry:\n    api_key = Secret.load(\"my-api-key\").get()\nexcept ValueError:\n    api_key = \"dev-fallback\"\n</code></pre> <p>Define custom blocks for typed configuration with <code>SecretStr</code> for credentials. The block stores connection details; <code>get_client()</code> returns a dedicated API client class:</p> <pre><code>from prefect.blocks.core import Block\nfrom pydantic import Field, SecretStr\n\nclass Dhis2Client:\n    \"\"\"Authenticated DHIS2 API client.\"\"\"\n\n    def __init__(self, base_url: str, username: str, password: str) -&gt; None:\n        self._http = httpx.Client(\n            base_url=f\"{base_url}/api\",\n            auth=(username, password),\n            timeout=60,\n        )\n\n    def fetch_metadata(self, endpoint: str) -&gt; list[dict]:\n        resp = self._http.get(f\"/{endpoint}\", params={\"paging\": \"false\"})\n        resp.raise_for_status()\n        return resp.json()[endpoint]\n\nclass Dhis2Credentials(Block):\n    base_url: str = \"https://play.im.dhis2.org/dev\"\n    username: str = \"admin\"\n    password: SecretStr = Field(default=SecretStr(\"district\"))\n\n    def get_client(self) -&gt; Dhis2Client:\n        return Dhis2Client(\n            self.base_url,\n            self.username,\n            self.password.get_secret_value(),\n        )\n</code></pre> <p>The <code>get_client()</code> pattern is used by official Prefect integrations (prefect-aws, prefect-gcp). It returns an authenticated client, keeping credential storage (Block) separate from API logic (Client).</p> <p>Blocks can be saved (<code>block.save(\"name\")</code>) and loaded (<code>Block.load(\"name\")</code>) from the Prefect server. <code>SecretStr</code> fields are encrypted at rest.</p>"},{"location":"patterns/#graceful-fallback-for-development","title":"Graceful fallback for development","text":"<p>Load from the server when available, fall back to inline defaults when developing without a server:</p> <pre><code>def get_dhis2_credentials() -&gt; Dhis2Credentials:\n    try:\n        return Dhis2Credentials.load(\"dhis2\")\n    except Exception:\n        return Dhis2Credentials()  # uses inline defaults\n</code></pre>"},{"location":"patterns/#testing-blocks-mock-patterns","title":"Testing blocks (mock patterns)","text":"<p>Mock at the <code>Dhis2Client</code> method level with <code>@patch.object</code> to avoid interfering with Prefect's internal httpx usage:</p> <pre><code>from unittest.mock import MagicMock, patch\n\n@patch.object(Dhis2Client, \"fetch_metadata\")\ndef test_fetch(mock_fetch):\n    mock_fetch.return_value = [{\"id\": \"OU1\"}, {\"id\": \"OU2\"}]\n    client = MagicMock(spec=Dhis2Client)\n    client.fetch_metadata = mock_fetch\n    result = some_task.fn(client)\n</code></pre> <p>For full flow integration tests, patch <code>Dhis2Credentials.get_client</code> to return a mock <code>Dhis2Client</code>:</p> <pre><code>@patch.object(Dhis2Credentials, \"get_client\")\ndef test_flow(mock_get_client):\n    mock_client = MagicMock(spec=Dhis2Client)\n    mock_client.fetch_metadata.return_value = [{\"id\": \"OU1\"}]\n    mock_get_client.return_value = mock_client\n    state = my_flow(return_state=True)\n    assert state.is_completed()\n</code></pre> <p>See: Secret Block, Custom Blocks</p>"},{"location":"patterns/#notification-blocks","title":"Notification blocks","text":"<p>Prefect ships with <code>NotificationBlock</code> subclasses that provide a unified <code>notify(body, subject)</code> interface for pipeline alerting. The two most common are <code>SlackWebhook</code> and <code>CustomWebhookNotificationBlock</code>:</p> <pre><code>import os\nfrom pydantic import SecretStr\nfrom prefect.blocks.notifications import SlackWebhook, CustomWebhookNotificationBlock\n\n# SlackWebhook -- read URL from environment, never hardcode\nslack = SlackWebhook(url=SecretStr(os.environ[\"SLACK_WEBHOOK_URL\"]))\nslack.notify(body=\"Pipeline completed: 150 records processed\", subject=\"Pipeline Alert\")\n\n# CustomWebhookNotificationBlock -- flexible for any HTTP endpoint\ncustom = CustomWebhookNotificationBlock(\n    name=\"ops-webhook\",\n    url=\"https://monitoring.example.com/alerts\",\n    method=\"POST\",\n    json_data={\"text\": \"{{subject}}: {{body}}\"},\n    secrets={\"api_token\": \"my-secret-token\"},\n)\ncustom.notify(body=\"All checks passed\", subject=\"Quality Report\")\n</code></pre> <p>Both blocks call <code>notify(body, subject)</code> -- the only difference is how they build the HTTP request.</p>"},{"location":"patterns/#template-resolution","title":"Template resolution","text":"<p><code>CustomWebhookNotificationBlock</code> resolves <code>{{subject}}</code>, <code>{{body}}</code>, <code>{{name}}</code>, and any key from <code>secrets</code> in both the URL and JSON payload:</p> <pre><code>block = CustomWebhookNotificationBlock(\n    name=\"template-demo\",\n    url=\"https://api.example.com/notify?token={{api_token}}\",\n    method=\"POST\",\n    json_data={\n        \"title\": \"{{subject}}\",\n        \"message\": \"{{body}}\",\n        \"source\": \"{{name}}\",\n        \"auth\": \"Bearer {{api_token}}\",\n    },\n    secrets={\"api_token\": \"secret-xyz-789\"},\n)\n</code></pre> <p>When <code>notify()</code> is called, every <code>{{placeholder}}</code> is replaced with its value before the HTTP request is sent.</p>"},{"location":"patterns/#flow-hooks-for-production-notifications","title":"Flow hooks for production notifications","text":"<p>Attach notification blocks to flow hooks for automatic alerting:</p> <pre><code>def on_completion_notify(flow, flow_run, state):\n    SlackWebhook.load(\"prod-slack\").notify(\n        body=f\"Flow {flow_run.name!r} completed.\",\n        subject=\"Flow Completed\",\n    )\n\ndef on_failure_notify(flow, flow_run, state):\n    SlackWebhook.load(\"prod-slack\").notify(\n        body=f\"CRITICAL: Flow {flow_run.name!r} failed: {state.message}\",\n        subject=\"Flow Failed\",\n    )\n\n@flow(on_completion=[on_completion_notify], on_failure=[on_failure_notify])\ndef my_pipeline():\n    ...\n</code></pre> <p>Save a block once with <code>slack.save(\"prod-slack\", overwrite=True)</code>, then load it in any hook or task with <code>SlackWebhook.load(\"prod-slack\")</code>.</p> <p>See: Notification Blocks</p>"},{"location":"patterns/#deployment-basics","title":"Deployment basics","text":""},{"location":"patterns/#serve-vs-deploy-decision-guide","title":"Serve vs deploy decision guide","text":"<p>Use <code>flow.serve()</code> when:</p> <ul> <li>You want the simplest possible deployment</li> <li>The flow runs on a single machine (dev laptop, cron server, VM)</li> <li>No Docker or Kubernetes infrastructure is available</li> </ul> <p>Use <code>flow.deploy()</code> when:</p> <ul> <li>You need infrastructure-level isolation (Docker, K8s)</li> <li>Multiple team members trigger runs from the UI</li> <li>You want auto-scaling via work pool configuration</li> </ul> <p>Use <code>prefect.yaml</code> when:</p> <ul> <li>You manage multiple deployments declaratively</li> <li>You want deployment config in version control</li> <li>You follow a GitOps workflow</li> </ul>"},{"location":"patterns/#flowserve","title":"flow.serve()","text":"<p>The simplest deployment -- runs in-process with optional scheduling:</p> <pre><code>my_flow.serve(name=\"my-deployment\", cron=\"0 6 * * *\")\n</code></pre> <p><code>flow.serve()</code> blocks the process and polls for scheduled runs. It is the Prefect equivalent of placing a DAG file in Airflow's <code>dags/</code> folder.</p>"},{"location":"patterns/#flowdeploy","title":"flow.deploy()","text":"<p>Production deployment -- sends runs to a work pool:</p> <pre><code>my_flow.deploy(\n    name=\"my-deployment\",\n    work_pool_name=\"my-pool\",\n    cron=\"0 6 * * *\",\n)\n</code></pre> <p>Work pools define where work runs (process, docker, kubernetes). Workers poll pools for scheduled runs.</p>"},{"location":"patterns/#parameterized-deployments","title":"Parameterized deployments","text":"<p>Pass default parameters at deployment time. These can be overridden per run from the UI, API, or CLI:</p> <pre><code># In code\nmy_flow.serve(\n    name=\"dhis2-sync\",\n    cron=\"0 6 * * *\",\n    parameters={\"endpoints\": [\"organisationUnits\", \"dataElements\"]},\n)\n\n# In prefect.yaml\n# deployments:\n#   - name: dhis2-ou\n#     entrypoint: deployments/dhis2_ou/flow.py:dhis2_ou_flow\n</code></pre> <p>Override at run time:</p> <pre><code>prefect deployment run dhis2_ou/dhis2-ou\n</code></pre>"},{"location":"patterns/#prefectyaml-pattern","title":"<code>prefect.yaml</code> pattern","text":"<p>Define multiple deployments in a single YAML file at the project root:</p> <pre><code>deployments:\n  - name: dhis2-ou\n    entrypoint: flow.py:dhis2_ou_flow\n    schedules:\n      - cron: \"0 6 * * *\"\n        timezone: \"UTC\"\n    work_pool:\n      name: default\n\n  - name: etl-every-5m\n    entrypoint: flows/core/core_flow_serve.py:flow_serve_flow\n    schedules:\n      - cron: \"*/5 * * * *\"\n    work_pool:\n      name: default\n</code></pre> <p>Deploy all at once with <code>prefect deploy --all</code> or individually with <code>prefect deploy -n dhis2-ou</code>.</p>"},{"location":"patterns/#deployment-aware-flows-with-prefectruntime","title":"Deployment-aware flows with <code>prefect.runtime</code>","text":"<p>Use <code>prefect.runtime</code> to access deployment context inside a running flow:</p> <pre><code>from prefect.runtime import deployment, flow_run\n\ndeployment_name = deployment.name            # None for local runs\nflow_run_name = flow_run.name                # auto-generated name\nscheduled_start = flow_run.scheduled_start_time\n</code></pre> <p>This lets a single flow produce different output depending on whether it is running locally or as a scheduled deployment.</p>"},{"location":"patterns/#work-pool-setup-walkthrough","title":"Work pool setup walkthrough","text":"<pre><code># 1. Create a process-based work pool\nprefect work-pool create my-pool --type process\n\n# 2. Start a worker that polls the pool\nprefect worker start --pool my-pool\n\n# 3. Deploy a flow to the pool\nprefect deploy -n dhis2-ou\n\n# 4. Trigger a run manually\nprefect deployment run dhis2_ou/dhis2-ou\n</code></pre>"},{"location":"patterns/#managing-deployments-with-the-cli","title":"Managing deployments with the CLI","text":"<pre><code># List and inspect\nprefect deployment ls\nprefect deployment inspect &lt;flow/deployment&gt;\n\n# Schedule management\nprefect deployment set-schedule &lt;name&gt; --cron \"0 8 * * *\"\nprefect deployment set-schedule &lt;name&gt; --interval 3600\nprefect deployment set-schedule &lt;name&gt; --rrule \"FREQ=WEEKLY;BYDAY=MO,WE,FR\"\nprefect deployment clear-schedule &lt;name&gt;\n\n# Pause and resume\nprefect deployment pause &lt;name&gt;\nprefect deployment resume &lt;name&gt;\n\n# Trigger runs with parameter overrides\nprefect deployment run &lt;name&gt; -p key=value\n\n# Cleanup\nprefect deployment delete &lt;name&gt;\n</code></pre>"},{"location":"patterns/#devprod-deployment-pattern","title":"Dev/prod deployment pattern","text":"<p>Use <code>flow.serve()</code> during development for fast iteration, then switch to <code>flow.deploy()</code> (or <code>prefect.yaml</code>) for production:</p> <pre><code>if __name__ == \"__main__\":\n    # Dev: run directly\n    # dhis2_ou_flow()\n\n    # Dev: serve with schedule\n    # dhis2_ou_flow.serve(name=\"dev-sync\", cron=\"*/5 * * * *\")\n\n    # Prod: deploy to work pool (or use prefect.yaml)\n    # dhis2_ou_flow.deploy(\n    #     name=\"dhis2-ou\",\n    #     work_pool_name=\"my-pool\",\n    #     cron=\"0 6 * * *\",\n    # )\n    dhis2_ou_flow()\n</code></pre>"},{"location":"patterns/#deploying-dhis2-flows","title":"Deploying DHIS2 flows","text":"<p>The <code>deployments/</code> directory contains production deployment examples, each with its own <code>flow.py</code>, <code>prefect.yaml</code>, and <code>deploy.py</code>:</p> <ul> <li><code>dhis2_connection/</code> -- connection check and server status artifact</li> <li><code>dhis2_ou/</code> -- org unit listing with markdown artifact</li> </ul> <p>See: Flow Serve, Schedules, Work Pools, Deployments directory</p>"},{"location":"patterns/#pydantic-models-for-type-safe-pipelines","title":"Pydantic models for type-safe pipelines","text":"<p>Use Pydantic <code>BaseModel</code> as task parameters and return types for automatic validation and serialisation:</p> <pre><code>from pydantic import BaseModel\n\nclass UserRecord(BaseModel):\n    name: str\n    email: str\n    age: int\n\nclass ProcessingResult(BaseModel):\n    records: list[dict]\n    errors: list[str]\n    summary: str\n\n@task\ndef validate_users(users: list[UserRecord]) -&gt; ProcessingResult:\n    valid, errors = [], []\n    for user in users:\n        if user.age &lt; 0:\n            errors.append(f\"Invalid age for {user.name}\")\n        else:\n            valid.append(user.model_dump())\n    return ProcessingResult(records=valid, errors=errors, summary=f\"{len(valid)} valid\")\n</code></pre> <p>Pydantic replaces XCom serialisation pain with automatic validation, type safety, and clean <code>.model_dump()</code> for dict conversion.</p> <p>Using Pydantic models as flow parameters (not just task parameters) lets Prefect auto-generate rich parameter schemas. The deployment UI renders typed form fields instead of a freeform JSON editor, and <code>enforce_parameter_schema</code> validates inputs at runtime:</p> <pre><code>@flow(name=\"config_driven_pipeline\", log_prints=True)\ndef config_driven_pipeline_flow(config: PipelineConfig | None = None) -&gt; PipelineResult:\n    ...\n</code></pre> <p>See: Pydantic Models, Pydantic Validation</p>"},{"location":"patterns/#shell-and-http-tasks","title":"Shell and HTTP tasks","text":"<p>Replace Airflow's BashOperator and HttpOperator with plain Python in <code>@task</code> functions:</p> <pre><code># Shell: subprocess.run() replaces BashOperator\n@task\ndef run_command(cmd: str) -&gt; str:\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)\n    return result.stdout.strip()\n\n# HTTP: httpx replaces HttpOperator\n@task\ndef http_get(url: str) -&gt; dict:\n    response = httpx.get(url, timeout=10.0)\n    response.raise_for_status()\n    return response.json()\n</code></pre> <p>No special operators needed. Standard Python libraries inside tasks are the Prefect way.</p> <p>See: Shell Tasks, HTTP Tasks</p>"},{"location":"patterns/#error-handling-with-quarantine-pattern","title":"Error handling with quarantine pattern","text":"<p>Separate good and bad records during processing, capturing error reasons:</p> <pre><code>from pydantic import BaseModel\n\nclass QuarantineResult(BaseModel):\n    good_records: list[dict]\n    bad_records: list[dict]\n    errors: list[str]\n\n@task\ndef process_with_quarantine(records: list[dict]) -&gt; QuarantineResult:\n    good, bad, errors = [], [], []\n    for record in records:\n        try:\n            validate(record)\n            good.append(record)\n        except ValueError as e:\n            bad.append(record)\n            errors.append(str(e))\n    return QuarantineResult(good_records=good, bad_records=bad, errors=errors)\n</code></pre> <p>The quarantine pattern prevents a few bad records from failing the entire pipeline.</p> <p>See: Error Handling ETL</p>"},{"location":"patterns/#transactions-for-atomic-operations","title":"Transactions for atomic operations","text":"<p>Group tasks atomically with <code>transaction()</code> -- if any task fails, the group is treated as a unit:</p> <pre><code>from prefect.transactions import transaction\n\n@flow\ndef atomic_pipeline():\n    with transaction():\n        step_a()\n        step_b()\n        step_c()\n</code></pre> <p>Transactions are a Prefect-specific feature with no direct Airflow equivalent.</p> <p>See: Transactions</p>"},{"location":"patterns/#task-runners-for-concurrent-execution","title":"Task runners for concurrent execution","text":"<p>Choose the right task runner for your workload:</p> <pre><code>from prefect.task_runners import ThreadPoolTaskRunner\n\n# I/O-bound: ThreadPoolTaskRunner for concurrent execution\n@flow(task_runner=ThreadPoolTaskRunner(max_workers=3))\ndef io_flow():\n    futures = fetch_data.map(urls)\n    return [f.result() for f in futures]\n\n# CPU-bound: default runner (or ConcurrentTaskRunner)\n@flow\ndef cpu_flow():\n    futures = compute.map(inputs)\n    return [f.result() for f in futures]\n</code></pre> <p><code>ThreadPoolTaskRunner</code> provides concurrent execution for I/O-bound tasks like API calls and file reads.</p> <p>See: Task Runners</p>"},{"location":"patterns/#file-io-patterns","title":"File I/O patterns","text":"<p>Use stdlib <code>csv</code> and <code>json</code> for file-based pipelines. <code>tempfile.mkdtemp()</code> provides isolated working directories, and <code>pathlib.Path</code> keeps paths clean:</p> <pre><code>@task\ndef read_csv(path: Path) -&gt; list[dict]:\n    with open(path, newline=\"\") as f:\n        return list(csv.DictReader(f))\n\n@task\ndef write_csv(directory: str, filename: str, rows: list[dict]) -&gt; Path:\n    path = Path(directory) / filename\n    with open(path, \"w\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n        writer.writeheader()\n        writer.writerows(rows)\n    return path\n</code></pre> <p>For mixed file types, dispatch on suffix:</p> <pre><code>@task\ndef read_file(path: Path) -&gt; list[dict]:\n    if path.suffix == \".csv\":\n        return list(csv.DictReader(open(path, newline=\"\")))\n    elif path.suffix == \".json\":\n        return json.loads(path.read_text())\n</code></pre> <p>See: CSV File Processing, JSON Event Ingestion, Multi-File Batch Processing</p>"},{"location":"patterns/#data-quality-rules-engine","title":"Data quality rules engine","text":"<p>Define quality rules as configuration and dispatch them to check functions:</p> <pre><code>class QualityRule(BaseModel):\n    name: str\n    rule_type: str\n    column: str = \"\"\n    params: dict = {}\n\n@task\ndef execute_rule(data: list[dict], rule: QualityRule) -&gt; RuleResult:\n    if rule.rule_type == \"not_null\":\n        return run_not_null_check.fn(data, rule.column)\n    elif rule.rule_type == \"range\":\n        return run_range_check.fn(data, rule.column, ...)\n</code></pre> <p>Compute an overall score and classify as green (&gt;= 0.9), amber (&gt;= 0.7), or red (&lt; 0.7).</p> <p>See: Quality Rules Engine, Cross-Dataset Validation</p>"},{"location":"patterns/#configuration-driven-pipelines","title":"Configuration-driven pipelines","text":"<p>Control pipeline behaviour entirely through configuration dicts:</p> <pre><code>class PipelineConfig(BaseModel):\n    name: str\n    stages: list[StageConfig]\n    fail_fast: bool = True\n\n@flow\ndef pipeline(raw_config: dict):\n    config = parse_config(raw_config)\n    for stage in config.stages:\n        if stage.enabled:\n            dispatch_stage(stage, context)\n</code></pre> <p>Different configs produce different pipeline runs through the same flow code. Disabled stages are skipped automatically.</p> <p>See: Config-Driven Pipeline</p>"},{"location":"patterns/#producer-consumer-pattern","title":"Producer-consumer pattern","text":"<p>Decouple data production from consumption using file-based data contracts:</p> <pre><code>@flow\ndef producer_consumer_flow():\n    producer_flow(data_dir, producer_id=\"alpha\")\n    producer_flow(data_dir, producer_id=\"beta\")\n    results = consumer_flow(data_dir, consumer_id=\"main\")\n</code></pre> <p>Producers write JSON data + metadata files. Consumers discover packages by scanning for metadata files. Each flow is independently testable.</p> <p>See: Producer-Consumer</p>"},{"location":"patterns/#circuit-breaker-pattern","title":"Circuit breaker pattern","text":"<p>Prevent cascading failures by opening a circuit after consecutive failures:</p> <pre><code>@task\ndef call_with_circuit(circuit: CircuitState, should_succeed: bool):\n    if circuit.state == \"open\":\n        circuit = circuit.model_copy(update={\"state\": \"half_open\"})\n    # Execute call, track failures, trip if threshold reached\n</code></pre> <p>States: closed (normal) -&gt; open (fail-fast) -&gt; half_open (probe) -&gt; closed (recovery). Deterministic boolean outcomes make testing straightforward.</p> <p>See: Circuit Breaker</p>"},{"location":"patterns/#incremental-processing-with-manifests","title":"Incremental processing with manifests","text":"<p>Track processed files in a JSON manifest to avoid reprocessing:</p> <pre><code>@task\ndef identify_new_files(all_files: list[Path], manifest: ProcessingManifest) -&gt; list[Path]:\n    return [f for f in all_files if f.name not in manifest.processed_files]\n</code></pre> <p>Run the flow twice: the second run processes zero files. This is the foundation for idempotent file pipelines.</p> <p>See: Incremental Processing, Idempotent Operations</p>"},{"location":"patterns/#checkpoint-based-recovery","title":"Checkpoint-based recovery","text":"<p>Save progress after each stage. On re-run, skip completed stages:</p> <pre><code>for stage in stages:\n    if not should_run_stage.fn(store, stage):\n        recovered += 1\n        continue\n    result = execute_stage.fn(stage, context)\n    save_checkpoint.fn(store, stage, \"completed\", result, path)\n</code></pre> <p>Fail at stage X, fix the issue, re-run, and stages before X are automatically skipped.</p> <p>See: Error Recovery</p>"},{"location":"patterns/#application-level-api-caching","title":"Application-level API caching","text":"<p>Cache API responses in a dict with hashlib-based keys and TTL expiry:</p> <pre><code>@task\ndef fetch_with_cache(endpoint, params, cache, ttl_seconds=300):\n    key = make_cache_key.fn(endpoint, params)\n    entry = check_cache.fn(cache, key, ttl_seconds)\n    if entry is not None:\n        return entry.value, True  # cache hit\n    data = simulate_api_call.fn(endpoint, params)\n    cache[key] = {\"value\": data, \"cached_at\": time.time()}\n    return data, False  # cache miss\n</code></pre> <p>Track hit/miss rates to measure cache effectiveness.</p> <p>See: Response Caching</p>"},{"location":"patterns/#threshold-classification-and-advisories-flow-081","title":"Threshold classification and advisories (flow 081)","text":"<p>Classify values against an ordered list of thresholds and generate advisories:</p> <pre><code>AQI_THRESHOLDS: list[tuple[float, str, str]] = [\n    (50.0, \"Good\", \"green\"),\n    (100.0, \"Moderate\", \"yellow\"),\n    (150.0, \"Unhealthy for Sensitive Groups\", \"orange\"),\n    (200.0, \"Unhealthy\", \"red\"),\n    (300.0, \"Very Unhealthy\", \"purple\"),\n    (float(\"inf\"), \"Hazardous\", \"maroon\"),\n]\n\nfor threshold, cat, col in AQI_THRESHOLDS:\n    if mean_val &lt;= threshold:\n        category = cat\n        color = col\n        break\n</code></pre> <p>Walk the thresholds in order and stop at the first match. Use a separate severity ordering list to rank worst outcomes.</p> <p>See: Air Quality Index</p>"},{"location":"patterns/#composite-risk-scoring-flow-082","title":"Composite risk scoring (flow 082)","text":"<p>Normalize risk factors to a common scale, then compute a weighted composite:</p> <pre><code>marine_avg = sum(f.normalized_score for f in marine_factors) / len(marine_factors)\nflood_avg = sum(f.normalized_score for f in flood_factors) / len(flood_factors)\nweighted = marine_avg * marine_weight + flood_avg * flood_weight\n</code></pre> <p>Configurable weights allow tuning the relative importance of each risk source.</p> <p>See: Composite Risk Assessment</p>"},{"location":"patterns/#pearson-correlation-flows-086-088","title":"Pearson correlation (flows 086, 088)","text":"<p>Manual Pearson correlation using only <code>math</code> and <code>statistics</code>:</p> <pre><code>def _pearson(x: list[float], y: list[float]) -&gt; float:\n    mean_x = statistics.mean(x)\n    mean_y = statistics.mean(y)\n    num = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y, strict=True))\n    den_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n    den_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n    return num / (den_x * den_y)\n</code></pre> <p>This pattern appears in flows 083, 086, 087, and 088. No numpy or scipy required.</p> <p>See: Multi-Indicator Correlation, Hypothesis Testing</p>"},{"location":"patterns/#log-linear-regression-flow-089","title":"Log-linear regression (flow 089)","text":"<p>Manual OLS regression with R-squared and residual-based ranking:</p> <pre><code>cov_xy = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y, strict=True)) / n\nvar_x = sum((xi - mean_x) ** 2 for xi in x) / n\nslope = cov_xy / var_x\nintercept = mean_y - slope * mean_x\nss_res = sum((yi - (slope * xi + intercept)) ** 2 for xi, yi in zip(x, y, strict=True))\nss_tot = sum((yi - mean_y) ** 2 for yi in y)\nr_squared = 1.0 - (ss_res / ss_tot)\n</code></pre> <p>Log-transform skewed data before regression. Rank entities by residual: negative residual means better-than-predicted performance.</p> <p>See: Regression Analysis</p>"},{"location":"patterns/#dimensional-modeling-star-schema-flow-090","title":"Dimensional modeling (Star schema) (flow 090)","text":"<p>Build fact and dimension tables with surrogate keys and composite index:</p> <pre><code>@task\ndef build_country_dimension(data: list[dict]) -&gt; list[DimCountry]:\n    for i, d in enumerate(data, 1):\n        dims.append(DimCountry(key=i, name=d[\"name\"], region=d[\"region\"], ...))\n\n@task\ndef min_max_normalize(values: list[float], higher_is_better: bool) -&gt; list[float]:\n    normalized = [(v - min_v) / (max_v - min_v) for v in values]\n    if not higher_is_better:\n        normalized = [1.0 - n for n in normalized]\n    return normalized\n</code></pre> <p>Surrogate keys are assigned sequentially. Min-max normalization respects the <code>higher_is_better</code> flag. Weighted normalized indicators produce a composite ranking.</p> <p>See: Star Schema</p>"},{"location":"patterns/#simulated-sql-etl-flow-091","title":"Simulated SQL ETL (flow 091)","text":"<p>Three-layer ETL: staging, production, summary:</p> <pre><code>raw = generate_raw_data()\nstaging = load_staging(raw)                   # raw + timestamp\nproduction = validate_and_transform(staging)  # parsed + is_valid flag\nvalid = filter_valid(production)\nsummary = compute_summary(valid, \"category\")  # grouped stats\n</code></pre> <p>Each layer adds metadata. Invalid records carry <code>is_valid=False</code> through the production layer without being dropped.</p> <p>See: Staged ETL Pipeline</p>"},{"location":"patterns/#regex-expression-parsing-flow-094","title":"Regex expression parsing (flow 094)","text":"<p>Count operands and operators to score expression complexity:</p> <pre><code>OPERAND_PATTERN = re.compile(r\"#\\{[^}]+\\}\")\n\nnum_operands = len(OPERAND_PATTERN.findall(expression))\nnum_operators = sum(1 for c in expression if c in \"+-*/\")\ntotal = num_operands + num_operators\n</code></pre> <p>Bin scores into trivial/simple/moderate/complex categories for reporting.</p> <p>See: Expression Complexity Scoring</p>"},{"location":"patterns/#data-lineage-tracking-flow-097","title":"Data lineage tracking (flow 097)","text":"<p>Hash-based provenance through pipeline stages:</p> <pre><code>@task\ndef compute_data_hash(records: list[DataRecord]) -&gt; str:\n    raw = str(sorted(str(r.model_dump()) for r in records))\n    return hashlib.sha256(raw.encode()).hexdigest()[:16]\n</code></pre> <p>Each transformation records input_hash and output_hash. The lineage graph shows which stages modified data (input_hash != output_hash) and which were passthrough.</p> <p>See: Data Lineage Tracking</p>"},{"location":"patterns/#pipeline-templates-flow-098","title":"Pipeline templates (flow 098)","text":"<p>Factory pattern for reusable pipeline configurations:</p> <pre><code>etl_basic = create_template(\"etl_basic\", [\n    StageTemplate(name=\"extract\", stage_type=\"extract\", default_params={\"batch_size\": 100}),\n    StageTemplate(name=\"load\", stage_type=\"load\", default_params={\"batch_size\": 100}),\n])\n\nbasic_small = instantiate_template(etl_basic, {\"batch_size\": 50})\nbasic_large = instantiate_template(etl_basic, {\"batch_size\": 500})\n</code></pre> <p>Define a template once, instantiate with different overrides. Stage execution merges overrides into default parameters.</p> <p>See: Pipeline Template Factory</p>"},{"location":"patterns/#custom-blocks-for-api-integration","title":"Custom blocks for API integration","text":"<p>Subclass <code>Block</code> to create typed connection objects for external APIs. This is the Prefect equivalent of Airflow's <code>BaseHook.get_connection()</code>. The credentials block stores connection details; <code>get_client()</code> returns a dedicated API client:</p> <pre><code>from prefect.blocks.core import Block\nfrom pydantic import Field, SecretStr\n\nclass Dhis2Client:\n    \"\"\"Authenticated DHIS2 API client.\"\"\"\n\n    def __init__(self, base_url: str, username: str, password: str) -&gt; None:\n        self._http = httpx.Client(\n            base_url=f\"{base_url}/api\",\n            auth=(username, password),\n            timeout=60,\n        )\n\n    def get_server_info(self) -&gt; dict:\n        resp = self._http.get(\"/system/info\")\n        resp.raise_for_status()\n        return resp.json()\n\n    def fetch_metadata(self, endpoint: str, fields: str = \":owner\") -&gt; list[dict]:\n        resp = self._http.get(f\"/{endpoint}\", params={\"paging\": \"false\", \"fields\": fields})\n        resp.raise_for_status()\n        return resp.json()[endpoint]\n\nclass Dhis2Credentials(Block):\n    _block_type_name = \"dhis2-credentials\"\n    base_url: str = Field(default=\"https://play.im.dhis2.org/dev\")\n    username: str = Field(default=\"admin\")\n    password: SecretStr = Field(default=SecretStr(\"district\"))\n\n    def get_client(self) -&gt; Dhis2Client:\n        return Dhis2Client(\n            self.base_url,\n            self.username,\n            self.password.get_secret_value(),\n        )\n\n# Load from server with fallback to inline defaults\ndef get_dhis2_credentials() -&gt; Dhis2Credentials:\n    try:\n        return Dhis2Credentials.load(\"dhis2\")\n    except Exception:\n        return Dhis2Credentials()\n</code></pre> <p>Usage in flows is clean -- the block provides credentials, the client provides API methods:</p> <pre><code>creds = get_dhis2_credentials()\nclient = creds.get_client()\ninfo = client.get_server_info()            # authenticated API call\nunits = client.fetch_metadata(\"organisationUnits\")  # returns list[dict]\n</code></pre> <p>Register a block once via the Prefect UI or <code>Dhis2Credentials(...).save(\"dhis2\")</code>. All flows that need the connection call <code>Dhis2Credentials.load(\"dhis2\")</code>.</p> <p>For generic authenticated APIs, use a pluggable auth block:</p> <pre><code>class ApiAuthConfig(Block):\n    auth_type: str  # \"api_key\", \"bearer\", \"basic\"\n    base_url: str\n\ndef build_auth_header(config: ApiAuthConfig, credentials: str) -&gt; AuthHeader:\n    if config.auth_type == \"api_key\":\n        return AuthHeader(header_name=\"X-API-Key\", header_value=credentials)\n    elif config.auth_type == \"bearer\":\n        return AuthHeader(header_name=\"Authorization\",\n                          header_value=f\"Bearer {credentials}\")\n    ...\n</code></pre> <p>See: DHIS2 Connection Block, Authenticated API Pipeline</p>"},{"location":"patterns/#secret-management-strategies","title":"Secret management strategies","text":"<p>Prefect provides multiple ways to manage secrets. Choose based on your environment:</p> <pre><code># 1. SecretStr on a Block (recommended -- keeps credentials with config)\nfrom pydantic import SecretStr\nclass MyConnection(Block):\n    password: SecretStr = Field(default=SecretStr(\"dev-password\"))\n\n# 2. Secret block (standalone credential)\nfrom prefect.blocks.system import Secret\npassword = Secret.load(\"dhis2-password\").get()\n\n# 3. Environment variable (simple, works everywhere)\nimport os\npassword = os.environ.get(\"DHIS2_PASSWORD\", \"fallback\")\n\n# 4. JSON block (for structured config)\nfrom prefect.blocks.system import JSON\nconfig = JSON.load(\"dhis2-config\").value\n</code></pre> <p>The preferred pattern is <code>SecretStr</code> directly on the connection block. This keeps credentials co-located with the connection config they belong to:</p> <pre><code>creds = Dhis2Credentials.load(\"dhis2\")  # password included, encrypted at rest\nclient = creds.get_client()\nclient.fetch_metadata(\"organisationUnits\")  # no separate password argument\n</code></pre> <p>Always use graceful fallbacks so flows work in development (no server) and production (with server):</p> <pre><code>def get_dhis2_credentials() -&gt; Dhis2Credentials:\n    try:\n        return Dhis2Credentials.load(\"dhis2\")\n    except Exception:\n        return Dhis2Credentials()  # uses inline defaults\n</code></pre> <p>See: Environment-Based Configuration, DHIS2 Connection Block</p>"},{"location":"testing/","title":"Testing","text":"<p>How to test Prefect flows and tasks. All examples in this project have corresponding tests in the <code>tests/</code> directory.</p>"},{"location":"testing/#running-tests","title":"Running tests","text":"<pre><code>make test\n# or directly:\nuv run pytest\n</code></pre>"},{"location":"testing/#testing-tasks-with-fn","title":"Testing tasks with <code>.fn()</code>","text":"<p>Call <code>.fn()</code> on a task to execute the underlying Python function without the Prefect runtime. This makes unit tests fast and free of side effects:</p> <pre><code>from my_flow import greet, compute_sum\n\ndef test_greet():\n    result = greet.fn(\"World\")\n    assert result == \"Hello, World!\"\n\ndef test_compute_sum():\n    result = compute_sum.fn(3, 7)\n    assert result == 10\n</code></pre> <p><code>.fn()</code> bypasses retries, state tracking, and logging -- you are testing pure business logic.</p>"},{"location":"testing/#testing-flows-with-return_statetrue","title":"Testing flows with <code>return_state=True</code>","text":"<p>Pass <code>return_state=True</code> to a flow call to get the final <code>State</code> object instead of the return value. This lets you assert on completion status:</p> <pre><code>def test_flow_completes():\n    state = my_flow(return_state=True)\n    assert state.is_completed()\n</code></pre> <p>You can also check for failure:</p> <pre><code>def test_flow_fails_on_bad_input():\n    state = my_flow(bad_param=True, return_state=True)\n    assert state.is_failed()\n</code></pre>"},{"location":"testing/#importing-digit-prefixed-modules-with-importlib","title":"Importing digit-prefixed modules with importlib","text":"<p>Flow files like <code>001_hello_world.py</code> start with a digit, which makes them invalid as Python module names. Use <code>importlib</code> to import them:</p> <pre><code>import importlib.util\nimport sys\nfrom pathlib import Path\n\n_spec = importlib.util.spec_from_file_location(\n    \"flow_001\",\n    Path(__file__).resolve().parent.parent / \"flows\" / \"001_hello_world.py\",\n)\nassert _spec and _spec.loader\n_mod = importlib.util.module_from_spec(_spec)\nsys.modules[\"flow_001\"] = _mod\n_spec.loader.exec_module(_mod)\n\nsay_hello = _mod.say_hello\nhello_world = _mod.hello_world\n</code></pre> <p>This pattern is used in every test file in the project.</p>"},{"location":"testing/#the-flow_module-fixture","title":"The <code>flow_module</code> fixture","text":"<p>The shared <code>conftest.py</code> provides a <code>flow_module</code> fixture that wraps the importlib boilerplate:</p> <pre><code># tests/conftest.py\n@pytest.fixture\ndef flow_module() -&gt; type:\n    class _Loader:\n        @staticmethod\n        def __call__(name: str) -&gt; ModuleType:\n            path = Path(__file__).resolve().parent.parent / \"flows\" / f\"{name}.py\"\n            spec = importlib.util.spec_from_file_location(name, path)\n            assert spec and spec.loader\n            mod = importlib.util.module_from_spec(spec)\n            sys.modules[name] = mod\n            spec.loader.exec_module(mod)\n            return mod\n    return _Loader\n</code></pre> <p>Use it in tests:</p> <pre><code>def test_etl(flow_module):\n    mod = flow_module(\"004_taskflow_etl\")\n    result = mod.extract.fn()\n    assert isinstance(result, dict)\n</code></pre>"},{"location":"testing/#typical-test-structure","title":"Typical test structure","text":"<p>A complete test file for a flow:</p> <pre><code>\"\"\"Tests for flow 004 \u2014 Taskflow / ETL.\"\"\"\n\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\n_spec = importlib.util.spec_from_file_location(\n    \"flow_004\",\n    Path(__file__).resolve().parent.parent / \"flows\" / \"004_taskflow_etl.py\",\n)\nassert _spec and _spec.loader\n_mod = importlib.util.module_from_spec(_spec)\nsys.modules[\"flow_004\"] = _mod\n_spec.loader.exec_module(_mod)\n\nextract = _mod.extract\ntransform = _mod.transform\nload = _mod.load\ntaskflow_etl_flow = _mod.taskflow_etl_flow\n\n\ndef test_extract_returns_dict() -&gt; None:\n    result = extract.fn()\n    assert isinstance(result, dict)\n\n\ndef test_transform() -&gt; None:\n    raw = extract.fn()\n    result = transform.fn(raw)\n    assert \"users\" in result\n\n\ndef test_flow_runs() -&gt; None:\n    state = taskflow_etl_flow(return_state=True)\n    assert state.is_completed()\n</code></pre> <p>Key takeaways:</p> <ol> <li><code>.fn()</code> for unit-testing individual tasks (fast, no Prefect overhead).</li> <li><code>return_state=True</code> for integration-testing the full flow.</li> <li><code>importlib</code> to handle digit-prefixed filenames.</li> <li><code>conftest.py</code> fixture to reduce import boilerplate.</li> </ol>"},{"location":"testing/#testing-with-dhis2client-mocks","title":"Testing with <code>Dhis2Client</code> mocks","text":"<p>DHIS2 flows depend on an external API. Mock at the <code>Dhis2Client</code> method level using <code>MagicMock(spec=Dhis2Client)</code> to avoid hitting real endpoints while keeping Prefect's internal httpx usage untouched.</p>"},{"location":"testing/#mocking-individual-tasks","title":"Mocking individual tasks","text":"<p>Use <code>@patch.object</code> on the client method and pass a <code>MagicMock(spec=Dhis2Client)</code> to the task's <code>.fn()</code>:</p> <pre><code>from unittest.mock import MagicMock, patch\nfrom prefect_examples.dhis2 import Dhis2Client\n\n@patch.object(Dhis2Client, \"fetch_metadata\")\ndef test_export_org_units(mock_fetch, tmp_path):\n    mock_fetch.return_value = [{\"id\": \"OU1\", \"name\": \"District A\", \"level\": 1}]\n    client = MagicMock(spec=Dhis2Client)\n    client.fetch_metadata = mock_fetch\n\n    result = export_org_units.fn(client, str(tmp_path))\n    assert result.endpoint == \"organisationUnits\"\n    assert result.record_count == 1\n</code></pre> <p>The <code>spec=Dhis2Client</code> parameter ensures the mock only allows methods that exist on the real client, catching typos early.</p>"},{"location":"testing/#mocking-for-full-flow-integration-tests","title":"Mocking for full flow integration tests","text":"<p>Patch <code>Dhis2Credentials.get_client</code> to return a mock client. This lets the flow run end-to-end without any real HTTP calls:</p> <pre><code>from prefect_examples.dhis2 import Dhis2Client, Dhis2Credentials\n\n@patch.object(Dhis2Credentials, \"get_client\")\ndef test_flow_runs(mock_get_client, tmp_path):\n    mock_client = MagicMock(spec=Dhis2Client)\n    mock_client.fetch_metadata.return_value = [{\"id\": \"OU1\", \"name\": \"A\"}]\n    mock_get_client.return_value = mock_client\n\n    state = my_dhis2_flow(return_state=True)\n    assert state.is_completed()\n</code></pre>"},{"location":"testing/#multi-endpoint-dispatch-with-side_effect","title":"Multi-endpoint dispatch with <code>side_effect</code>","text":"<p>When a flow calls <code>fetch_metadata</code> with different endpoints, use <code>side_effect</code> to return different data based on the argument:</p> <pre><code>def _mock_client_with_side_effect():\n    mock_client = MagicMock(spec=Dhis2Client)\n\n    def _fetch(endpoint, **kwargs):\n        if \"organisationUnits\" in endpoint:\n            return [{\"id\": \"OU1\", \"name\": \"A\", \"level\": 1}]\n        elif \"dataElements\" in endpoint:\n            return [{\"id\": \"DE1\", \"name\": \"B\", \"valueType\": \"NUMBER\"}]\n        return []\n\n    mock_client.fetch_metadata.side_effect = _fetch\n    return mock_client\n</code></pre> <p>This pattern is used in <code>test_106_dhis2_combined_export.py</code> where three endpoints are fetched in parallel.</p>"},{"location":"testing/#testing-deployment-flows","title":"Testing deployment flows","text":"<p>Deployment flows (in <code>deployments/</code>) follow the same patterns as regular flows. The test imports the flow module from the deployment directory and patches the credentials block:</p> <pre><code>from unittest.mock import MagicMock, patch\nfrom prefect_examples.dhis2 import Dhis2Client, Dhis2Credentials\n\n@patch.object(Dhis2Credentials, \"get_client\")\ndef test_deployment_flow(mock_get_client):\n    mock_client = MagicMock(spec=Dhis2Client)\n    mock_client.get_server_info.return_value = {\"version\": \"2.41\"}\n    mock_client.fetch_metadata.return_value = [{\"id\": \"OU1\"}]\n    mock_get_client.return_value = mock_client\n\n    state = dhis2_ou_flow(return_state=True)\n    assert state.is_completed()\n</code></pre> <p>Deployment tests live alongside the regular test files: <code>tests/test_deploy_dhis2_connection.py</code> and <code>tests/test_deploy_dhis2_ou.py</code>.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Step-by-step walkthroughs for common Prefect tasks using this project.</p>"},{"location":"tutorials/#tutorial-1-your-first-flow","title":"Tutorial 1: Your first flow","text":"<p>Write a flow with tasks, run it locally, and view the results.</p>"},{"location":"tutorials/#step-1-create-the-flow","title":"Step 1 -- Create the flow","text":"<p>Create a file <code>flows/my_first_flow.py</code>:</p> <pre><code>from prefect import flow, task\n\n\n@task\ndef extract() -&gt; list[dict]:\n    \"\"\"Simulate data extraction.\"\"\"\n    return [\n        {\"name\": \"Alice\", \"score\": 85},\n        {\"name\": \"Bob\", \"score\": 92},\n    ]\n\n\n@task\ndef transform(records: list[dict]) -&gt; list[dict]:\n    \"\"\"Add a pass/fail field.\"\"\"\n    for r in records:\n        r[\"status\"] = \"pass\" if r[\"score\"] &gt;= 70 else \"fail\"\n    return records\n\n\n@task\ndef load(records: list[dict]) -&gt; None:\n    \"\"\"Print the results.\"\"\"\n    for r in records:\n        print(f\"{r['name']}: {r['score']} ({r['status']})\")\n\n\n@flow(name=\"my_first_flow\", log_prints=True)\ndef my_first_flow() -&gt; None:\n    raw = extract()\n    processed = transform(raw)\n    load(processed)\n\n\nif __name__ == \"__main__\":\n    my_first_flow()\n</code></pre>"},{"location":"tutorials/#step-2-run-it","title":"Step 2 -- Run it","text":"<pre><code>uv run python flows/my_first_flow.py\n</code></pre> <p>You should see Prefect log output showing the flow and each task completing, followed by the printed results.</p>"},{"location":"tutorials/#step-3-view-in-the-prefect-ui","title":"Step 3 -- View in the Prefect UI","text":"<p>Start a Prefect server and run the flow again:</p> <pre><code># Terminal 1: start the server\nuv run prefect server start\n\n# Terminal 2: run the flow\nuv run python flows/my_first_flow.py\n</code></pre> <p>Open <code>http://localhost:4200</code> to see the flow run, task runs, and their states.</p>"},{"location":"tutorials/#what-you-learned","title":"What you learned","text":"<ul> <li><code>@task</code> turns a function into a tracked unit of work</li> <li><code>@flow</code> is the top-level orchestration container</li> <li><code>log_prints=True</code> captures <code>print()</code> output in Prefect logs</li> <li>Return values flow between tasks naturally -- no XCom needed</li> </ul> <p>Related flows: Hello World, Python Tasks, Taskflow ETL</p>"},{"location":"tutorials/#tutorial-2-working-with-blocks","title":"Tutorial 2: Working with blocks","text":"<p>Create a <code>Dhis2Credentials</code> block, save it, and use it in a flow.</p>"},{"location":"tutorials/#step-1-create-and-save-a-block","title":"Step 1 -- Create and save a block","text":"<pre><code>from prefect_examples.dhis2 import Dhis2Credentials\n\ncreds = Dhis2Credentials(\n    base_url=\"https://play.im.dhis2.org/dev\",\n    username=\"admin\",\n    # password defaults to \"district\"\n)\n\n# Save to the Prefect server (requires a running server)\ncreds.save(\"dhis2\", overwrite=True)\nprint(\"Block saved!\")\n</code></pre> <p>Run this once to register the block:</p> <pre><code>uv run python -c \"\nfrom prefect_examples.dhis2 import Dhis2Credentials\nDhis2Credentials().save('dhis2', overwrite=True)\nprint('Block saved!')\n\"\n</code></pre>"},{"location":"tutorials/#step-2-load-and-use-the-block","title":"Step 2 -- Load and use the block","text":"<pre><code>from prefect import flow, task\nfrom prefect_examples.dhis2 import Dhis2Credentials, Dhis2Client\n\n\n@task\ndef fetch_org_units(client: Dhis2Client) -&gt; list[dict]:\n    return client.fetch_metadata(\"organisationUnits\", fields=\"id,name,level\")\n\n\n@flow(name=\"block_demo\", log_prints=True)\ndef block_demo() -&gt; None:\n    creds = Dhis2Credentials.load(\"dhis2\")\n    client = creds.get_client()\n    units = fetch_org_units(client)\n    print(f\"Fetched {len(units)} org units\")\n\n\nif __name__ == \"__main__\":\n    block_demo()\n</code></pre>"},{"location":"tutorials/#step-3-graceful-fallback","title":"Step 3 -- Graceful fallback","text":"<p>Use <code>get_dhis2_credentials()</code> so the flow works with or without a server:</p> <pre><code>from prefect_examples.dhis2 import get_dhis2_credentials\n\ncreds = get_dhis2_credentials()  # loads from server or uses inline defaults\nclient = creds.get_client()\n</code></pre>"},{"location":"tutorials/#what-you-learned_1","title":"What you learned","text":"<ul> <li>Blocks are typed configuration objects that can be saved/loaded from the server</li> <li><code>SecretStr</code> fields are encrypted at rest when saved</li> <li><code>get_client()</code> returns an authenticated API client</li> <li>The fallback pattern keeps flows runnable without a server</li> </ul> <p>Related flows: DHIS2 Connection Block, DHIS2 Org Units API</p>"},{"location":"tutorials/#tutorial-3-variables-and-configuration","title":"Tutorial 3: Variables and configuration","text":"<p>Use <code>Variable.set()</code>/<code>Variable.get()</code> for runtime configuration.</p>"},{"location":"tutorials/#step-1-set-a-variable","title":"Step 1 -- Set a variable","text":"<pre><code>from prefect.variables import Variable\n\nVariable.set(\"batch_config\", '{\"batch_size\": 100, \"retries\": 3}', overwrite=True)\n</code></pre>"},{"location":"tutorials/#step-2-use-it-in-a-flow","title":"Step 2 -- Use it in a flow","text":"<pre><code>import json\nfrom prefect import flow, task\nfrom prefect.variables import Variable\n\n\n@task\ndef get_config() -&gt; dict:\n    raw = Variable.get(\"batch_config\", default='{\"batch_size\": 50}')\n    return json.loads(raw)\n\n\n@task\ndef process_batch(config: dict) -&gt; None:\n    print(f\"Processing with batch_size={config['batch_size']}\")\n\n\n@flow(name=\"variable_demo\", log_prints=True)\ndef variable_demo() -&gt; None:\n    config = get_config()\n    process_batch(config)\n\n\nif __name__ == \"__main__\":\n    variable_demo()\n</code></pre>"},{"location":"tutorials/#when-to-use-what","title":"When to use what","text":"Mechanism Best for Example <code>Variable</code> Simple key-value runtime config Batch sizes, feature flags Custom <code>Block</code> Typed connection config with methods <code>Dhis2Credentials</code> <code>Secret</code> block Single credential values API keys, tokens <code>JSON</code> block Structured configuration Threshold mappings Environment variables CI/CD, container config <code>DHIS2_PASSWORD</code> Flow parameters Per-run overrides <code>--param batch_size=200</code>"},{"location":"tutorials/#what-you-learned_2","title":"What you learned","text":"<ul> <li>Variables are simple string key-value pairs stored on the Prefect server</li> <li><code>Variable.get()</code> accepts a default for offline development</li> <li>Variables complement blocks and parameters -- they are the simplest config   mechanism</li> </ul> <p>Related flows: Variables and Params</p>"},{"location":"tutorials/#tutorial-4-deploying-a-flow","title":"Tutorial 4: Deploying a flow","text":"<p>Create a deployment, register it, trigger a run, and manage schedules.</p>"},{"location":"tutorials/#step-1-create-the-deployment-directory","title":"Step 1 -- Create the deployment directory","text":"<pre><code>mkdir -p deployments/my_flow\n</code></pre>"},{"location":"tutorials/#step-2-write-the-flow","title":"Step 2 -- Write the flow","text":"<p>Create <code>deployments/my_flow/flow.py</code>:</p> <pre><code>from prefect import flow, task\nfrom prefect.artifacts import create_markdown_artifact\n\n\n@task\ndef do_work() -&gt; str:\n    return \"Hello from a deployment!\"\n\n\n@flow(name=\"my_deployed_flow\", log_prints=True)\ndef my_deployed_flow() -&gt; None:\n    result = do_work()\n    print(result)\n    create_markdown_artifact(\n        key=\"deployment-result\",\n        markdown=f\"## Result\\n\\n{result}\",\n    )\n\n\nif __name__ == \"__main__\":\n    my_deployed_flow()\n</code></pre>"},{"location":"tutorials/#step-3-write-prefectyaml","title":"Step 3 -- Write <code>prefect.yaml</code>","text":"<p>Create <code>deployments/my_flow/prefect.yaml</code>:</p> <pre><code>pull:\n  - prefect.deployments.steps.set_working_directory:\n      directory: /opt/prefect/deployments/my_flow\n\ndeployments:\n  - name: my-flow\n    entrypoint: flow.py:my_deployed_flow\n    schedules:\n      - cron: \"0 6 * * *\"\n        timezone: \"UTC\"\n    work_pool:\n      name: default\n</code></pre> <p>The <code>pull</code> step tells the worker where to find the flow source inside the Docker container.</p>"},{"location":"tutorials/#step-4-deploy","title":"Step 4 -- Deploy","text":"<pre><code>cd deployments/my_flow\nPREFECT_API_URL=http://localhost:4200/api uv run prefect deploy --all\n</code></pre> <p>Or add it to the <code>Makefile</code>'s <code>deploy</code> target.</p>"},{"location":"tutorials/#step-5-trigger-a-run","title":"Step 5 -- Trigger a run","text":"<pre><code>PREFECT_API_URL=http://localhost:4200/api \\\n  uv run prefect deployment run my_deployed_flow/my-flow\n</code></pre>"},{"location":"tutorials/#step-6-manage-the-schedule","title":"Step 6 -- Manage the schedule","text":"<pre><code># Change to every 30 minutes\nprefect deployment set-schedule my_deployed_flow/my-flow --interval 1800\n\n# Pause scheduling\nprefect deployment pause my_deployed_flow/my-flow\n\n# Resume scheduling\nprefect deployment resume my_deployed_flow/my-flow\n\n# Remove all schedules\nprefect deployment clear-schedule my_deployed_flow/my-flow\n</code></pre>"},{"location":"tutorials/#schedule-types","title":"Schedule types","text":"<p>You can use three types of schedules when deploying:</p> <p>Cron -- standard cron expressions:</p> <pre><code>schedules:\n  - cron: \"*/15 * * * *\"    # every 15 minutes\n    timezone: \"UTC\"\n</code></pre> <p>Interval -- fixed number of seconds:</p> <pre><code>schedules:\n  - interval: 900           # every 15 minutes\n</code></pre> <p>RRule -- RFC 5545 recurrence rules:</p> <pre><code>schedules:\n  - rrule: \"FREQ=WEEKLY;BYDAY=MO,WE,FR\"\n    timezone: \"UTC\"\n</code></pre> <p>Multiple schedules can be combined on a single deployment:</p> <pre><code>schedules:\n  - cron: \"0 6 * * *\"\n    timezone: \"UTC\"\n  - cron: \"0 18 * * *\"\n    timezone: \"UTC\"\n</code></pre>"},{"location":"tutorials/#what-you-learned_3","title":"What you learned","text":"<ul> <li>Deployments package a flow for scheduled or on-demand execution</li> <li><code>prefect.yaml</code> defines the entrypoint, schedule, and work pool</li> <li>Schedules support cron, interval, and RRule formats</li> <li>Schedules can be updated after deployment via CLI or UI</li> <li>The <code>pull</code> step configures where the worker finds the flow source</li> </ul> <p>Related flows: Flow Serve, Schedules</p>"},{"location":"tutorials/#tutorial-5-running-the-docker-stack","title":"Tutorial 5: Running the Docker stack","text":"<p>Start the full Prefect environment, deploy flows, and monitor runs.</p>"},{"location":"tutorials/#step-1-start-the-stack","title":"Step 1 -- Start the stack","text":"<pre><code>make start\n</code></pre> <p>This runs <code>docker compose up --build</code> and starts four services:</p> Service Port Purpose PostgreSQL 5432 Database backend Prefect Server 4200 UI + API Prefect Worker -- Executes flow runs RustFS 9000, 9001 S3-compatible object storage <p>Wait for all services to be healthy (watch the healthcheck logs).</p>"},{"location":"tutorials/#step-2-verify-services","title":"Step 2 -- Verify services","text":"<p>Open <code>http://localhost:4200</code> in your browser. You should see the Prefect UI with an empty dashboard.</p> <p>Check the worker is connected:</p> <pre><code>docker compose logs prefect-worker | tail -5\n</code></pre> <p>You should see the worker polling the <code>default</code> work pool.</p>"},{"location":"tutorials/#step-3-deploy-flows","title":"Step 3 -- Deploy flows","text":"<p>In a separate terminal:</p> <pre><code>make deploy\n</code></pre> <p>This registers the <code>dhis2-connection</code> and <code>dhis2-ou</code> deployments. Refresh the UI and navigate to the Deployments page to see them.</p>"},{"location":"tutorials/#step-4-trigger-a-run","title":"Step 4 -- Trigger a run","text":"<pre><code>PREFECT_API_URL=http://localhost:4200/api \\\n  uv run prefect deployment run dhis2_connection/dhis2-connection\n</code></pre> <p>Or click the \"Run\" button in the UI.</p>"},{"location":"tutorials/#step-5-view-results","title":"Step 5 -- View results","text":"<p>Navigate to the Flow Runs page in the UI. Click on the run to see:</p> <ul> <li>Timeline -- task execution order and duration</li> <li>Logs -- captured print output and Prefect log messages</li> <li>Artifacts -- markdown reports and tables created by the flow</li> </ul>"},{"location":"tutorials/#step-6-manage-schedules","title":"Step 6 -- Manage schedules","text":"<p>Both deployments are configured to run every 15 minutes. To change the schedule:</p> <pre><code># Switch to hourly\nPREFECT_API_URL=http://localhost:4200/api \\\n  uv run prefect deployment set-schedule dhis2_ou/dhis2-ou --interval 3600\n\n# Pause the schedule\nPREFECT_API_URL=http://localhost:4200/api \\\n  uv run prefect deployment pause dhis2_ou/dhis2-ou\n</code></pre>"},{"location":"tutorials/#step-7-shut-down","title":"Step 7 -- Shut down","text":"<p>Press <code>Ctrl+C</code> in the terminal running <code>make start</code>, or:</p> <pre><code>docker compose down       # stop services, keep data\ndocker compose down -v    # stop services and delete all data\n</code></pre>"},{"location":"tutorials/#what-you-learned_4","title":"What you learned","text":"<ul> <li><code>make start</code> brings up the complete Prefect environment</li> <li><code>make deploy</code> registers deployments with the server</li> <li>The worker automatically picks up scheduled and manually triggered runs</li> <li>Deployment schedules can be viewed and managed from the UI or CLI</li> <li><code>docker compose down -v</code> provides a clean reset</li> </ul> <p>Related pages: Infrastructure, CLI Reference</p>"},{"location":"tutorials/#tutorial-6-testing-with-slack-webhooks","title":"Tutorial 6: Testing with Slack webhooks","text":"<p>Send real Slack notifications from a Prefect flow using notification blocks.</p>"},{"location":"tutorials/#step-1-create-a-slack-app","title":"Step 1 -- Create a Slack app","text":"<ol> <li>Go to api.slack.com/apps and click    Create New App &gt; From scratch.</li> <li>Name it (e.g. \"Prefect Alerts\") and pick a workspace.</li> <li>In the left sidebar, click Incoming Webhooks and toggle it On.</li> <li>Click Add New Webhook to Workspace, choose a channel, and click    Allow.</li> <li>Copy the webhook URL -- it looks like    <code>https://hooks.slack.com/services/T00/B00/xxxx</code>.</li> </ol>"},{"location":"tutorials/#step-2-test-with-curl","title":"Step 2 -- Test with curl","text":"<p>Verify the URL works before writing any Python:</p> <pre><code>curl -X POST -H 'Content-Type: application/json' \\\n  -d '{\"text\": \"Hello from curl!\"}' \\\n  https://hooks.slack.com/services/T00/B00/xxxx\n</code></pre> <p>You should see a message appear in the channel you selected.</p>"},{"location":"tutorials/#step-3-add-the-url-to-env","title":"Step 3 -- Add the URL to <code>.env</code>","text":"<p>Never hardcode webhook URLs in source code. Add it to your <code>.env</code> file (already gitignored):</p> <pre><code># .env\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/T00/B00/xxxx\n</code></pre> <p>The variable is listed in <code>.env.example</code> as a reference.</p>"},{"location":"tutorials/#step-4-use-in-a-flow","title":"Step 4 -- Use in a flow","text":"<pre><code>import os\n\nfrom dotenv import load_dotenv\nfrom prefect import flow\nfrom prefect.blocks.notifications import SlackWebhook\nfrom pydantic import SecretStr\n\n@flow(log_prints=True)\ndef slack_test() -&gt; None:\n    url = os.environ[\"SLACK_WEBHOOK_URL\"]\n    slack = SlackWebhook(url=SecretStr(url))\n    slack.notify(body=\"Hello from Prefect!\", subject=\"Test Notification\")\n    print(\"Notification sent\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n    slack_test()\n</code></pre> <p>Run it:</p> <pre><code>uv run python my_slack_test.py\n</code></pre>"},{"location":"tutorials/#step-5-save-as-a-block-for-reuse","title":"Step 5 -- Save as a block for reuse","text":"<p>Saving the block to the Prefect server means you never hardcode the URL again:</p> <pre><code>import os\nfrom pydantic import SecretStr\nfrom prefect.blocks.notifications import SlackWebhook\n\nslack = SlackWebhook(url=SecretStr(os.environ[\"SLACK_WEBHOOK_URL\"]))\nslack.save(\"prod-slack\", overwrite=True)\nprint(\"Block saved!\")\n</code></pre> <p>From now on, any flow can load it:</p> <pre><code>slack = SlackWebhook.load(\"prod-slack\")\nslack.notify(body=\"Pipeline finished\", subject=\"ETL Complete\")\n</code></pre>"},{"location":"tutorials/#step-6-use-in-flow-hooks","title":"Step 6 -- Use in flow hooks","text":"<p>Wire the saved block into lifecycle hooks for automatic alerting:</p> <pre><code>from prefect import flow\nfrom prefect.blocks.notifications import SlackWebhook\n\ndef on_completion(flow, flow_run, state):\n    SlackWebhook.load(\"prod-slack\").notify(\n        body=f\"Flow {flow_run.name!r} completed successfully.\",\n        subject=\"Flow Completed\",\n    )\n\ndef on_failure(flow, flow_run, state):\n    SlackWebhook.load(\"prod-slack\").notify(\n        body=f\"Flow {flow_run.name!r} failed: {state.message}\",\n        subject=\"Flow Failed\",\n    )\n\n@flow(on_completion=[on_completion], on_failure=[on_failure], log_prints=True)\ndef my_pipeline() -&gt; None:\n    print(\"Doing work...\")\n\nif __name__ == \"__main__\":\n    my_pipeline()\n</code></pre>"},{"location":"tutorials/#step-7-customwebhooknotificationblock-for-generic-webhooks","title":"Step 7 -- CustomWebhookNotificationBlock for generic webhooks","text":"<p><code>CustomWebhookNotificationBlock</code> works with any HTTP endpoint -- Discord, Teams, or a custom monitoring service:</p> <pre><code>from prefect.blocks.notifications import CustomWebhookNotificationBlock\n\nwebhook = CustomWebhookNotificationBlock(\n    name=\"discord-alerts\",\n    url=\"https://discord.com/api/webhooks/1234/abcd\",\n    method=\"POST\",\n    json_data={\"content\": \"**{{subject}}**\\n{{body}}\"},\n)\nwebhook.notify(body=\"All checks passed\", subject=\"Quality Report\")\n\n# Save for reuse\nwebhook.save(\"discord-alerts\", overwrite=True)\n</code></pre> <p>Template placeholders (<code>{{subject}}</code>, <code>{{body}}</code>, <code>{{name}}</code>) and custom <code>secrets</code> keys are resolved automatically before the HTTP request is sent.</p>"},{"location":"tutorials/#what-you-learned_5","title":"What you learned","text":"<ul> <li>Slack Incoming Webhooks provide a simple URL for posting messages</li> <li><code>SlackWebhook</code> wraps the URL in a <code>SecretStr</code> and sends via <code>notify()</code></li> <li>Saving blocks with <code>.save()</code> avoids hardcoding URLs in source code</li> <li>Flow hooks (<code>on_completion</code>, <code>on_failure</code>) automate notifications</li> <li><code>CustomWebhookNotificationBlock</code> extends the same pattern to any HTTP   endpoint with template resolution</li> </ul> <p>Related flows: Notification Blocks, Webhook Notifications</p>"}]}